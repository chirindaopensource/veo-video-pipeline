{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "FPGweqo7i_5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Essential Modules\n",
        "\n",
        "\"\"\"\n",
        "Module imports organized according to PEP-8 standards.\n",
        "Includes error handling for optional Google Cloud dependencies.\n",
        "\"\"\"\n",
        "\n",
        "# Standard library imports\n",
        "import asyncio\n",
        "import functools\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import threading\n",
        "import time\n",
        "import unicodedata\n",
        "import uuid\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from enum import Enum\n",
        "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
        "\n",
        "# Third-party imports (Google Cloud)\n",
        "try:\n",
        "    # Google API Core\n",
        "    from google.api_core import exceptions as gapi_exceptions\n",
        "    from google.api_core import retry as google_retry\n",
        "\n",
        "    # Google Auth\n",
        "    from google.auth import default as get_default_credentials\n",
        "    from google.auth.exceptions import DefaultCredentialsError\n",
        "\n",
        "    # Google Cloud Services\n",
        "    from google.cloud import storage\n",
        "    from google.cloud import texttospeech_v1 as texttospeech\n",
        "    from google.cloud.exceptions import (\n",
        "        Forbidden,\n",
        "        GoogleCloudError,\n",
        "        NotFound,\n",
        "        ServiceUnavailable\n",
        "    )\n",
        "\n",
        "    # Vertex AI\n",
        "    import vertexai\n",
        "\n",
        "except ImportError as e:\n",
        "    logging.critical(\n",
        "        f\"Required Google Cloud libraries not installed: {e}. \"\n",
        "        f\"Please install: pip install google-cloud-texttospeech \"\n",
        "        f\"google-cloud-storage google-cloud-aiplatform\"\n",
        "    )\n",
        "    raise ImportError(\n",
        "        \"Google Cloud dependencies required. Install with: \"\n",
        "        \"pip install google-cloud-texttospeech google-cloud-storage \"\n",
        "        \"google-cloud-aiplatform\"\n",
        "    ) from e\n"
      ],
      "metadata": {
        "id": "uh453rW0jCqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "# Vertex AI TTS Orchestrator: Cost-Efficient Text-to-Speech on Google Cloud\n",
        "\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n",
        "[![PEP-8 Compliant](https://img.shields.io/badge/code%20style-pep8-orange.svg)](https://www.python.org/dev/peps/pep-0008/)\n",
        "\n",
        "## Overview\n",
        "\n",
        "The Vertex AI TTS Orchestrator is a Python-based system designed to provide a highly reliable, scalable, and cost-effective solution for generating high-quality speech from text using Google Cloud's Text-to-Speech API, with deep integration considerations for Vertex AI environments. It intelligently manages API calls, incorporates caching, employs reliability patterns, and offers detailed cost tracking to minimize operational expenses while maximizing performance and audio quality.\n",
        "\n",
        "This system is engineered for production environments where both audio fidelity and budget adherence are paramount. All components are contained within the `draft_standalone_audio_layer.ipynb` Jupyter Notebook, which includes the core orchestrator logic, helper classes, and a usage example.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "*   **Cost-Efficient Synthesis:** Intelligent voice selection (Standard, WaveNet, Neural2) based on configurable cost thresholds and dynamic quality adjustments.\n",
        "*   **Advanced Caching:** In-memory caching with Time-To-Live (TTL) and Least Recently Used (LRU) eviction policy to reduce redundant API calls and associated costs.\n",
        "*   **Dynamic Voice Discovery:** Fetches and validates available voices directly from the Google Cloud TTS API at initialization, ensuring up-to-date voice selection.\n",
        "*   **Robust Reliability Patterns:**\n",
        "    *   **Circuit Breaker:** Prevents repeated calls to a failing API, allowing services to recover.\n",
        "    *   **Adaptive Rate Limiter:** Dynamically adjusts request rates to avoid API quotas and adapt to server load.\n",
        "    *   **Client-Side Retries:** Leverages Google Cloud client library built-in retries for transient network issues.\n",
        "*   **Asynchronous Operations:** True asynchronous implementation for I/O-bound tasks (TTS synthesis, batch processing) using `asyncio` for enhanced performance and scalability.\n",
        "*   **Comprehensive Input Validation:** Rigorous validation of all `TTSRequest` parameters to ensure data integrity and prevent erroneous API calls.\n",
        "*   **Granular Error Handling:** Custom exception hierarchy for precise error identification and handling.\n",
        "*   **Detailed Metrics & Cost Analysis:** Thread-safe tracking of character usage, costs, cache performance, and API call statistics, with methods to retrieve comprehensive cost analysis reports and optimization recommendations.\n",
        "*   **SSML Support:** Full support for Speech Synthesis Markup Language (SSML) for fine-grained control over speech output.\n",
        "*   **Batch Processing:** Concurrent processing of multiple TTS requests with individual error handling and adherence to rate limits and circuit breaker states.\n",
        "*   **PEP-8 Compliant Code:** Written to the highest Python coding standards for readability and maintainability.\n",
        "*   **Professional Grade Logging:** Structured and detailed logging throughout the system for observability and debugging.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "The orchestrator is built around the `VertexTTSOrchestrator` class, which coordinates several key components:\n",
        "\n",
        "1.  **`TTSRequest` Dataclass:** A structured and validated representation of a text-to-speech request.\n",
        "2.  **`VoiceQuality` Enum:** Manages voice quality tiers and associated cost/free-tier information.\n",
        "3.  **Intelligent Voice Selection:** Dynamically chooses the optimal voice based on request parameters, available voices, and cost-saving strategies.\n",
        "4.  **Caching Layer:** (`_get_from_cache`, `_put_in_cache`): In-memory cache for storing and retrieving synthesized audio, reducing API calls.\n",
        "5.  **API Interaction Layer:** Handles communication with the Google Cloud Text-to-Speech API.\n",
        "    *   **`CircuitBreaker`:** Wraps API calls to manage service unavailability.\n",
        "    *   **`AdaptiveRateLimiter`:** Controls the rate of API calls.\n",
        "6.  **`CostMetrics` Dataclass:** Tracks usage and costs in a thread-safe manner.\n",
        "7.  **Asynchronous Core (`synthesize_speech`, `batch_synthesize`):** Manages concurrent operations efficiently.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "*   **Python:** Version 3.8 or higher.\n",
        "*   **Google Cloud Platform (GCP) Account:**\n",
        "    *   A GCP project with billing enabled.\n",
        "    *   The **Text-to-Speech API** and **Vertex AI API** must be enabled for your project.\n",
        "*   **Google Cloud SDK (`gcloud`):** Installed and configured with credentials authorized to access your GCP project (e.g., via `gcloud auth application-default login`). Alternatively, a service account key JSON file can be used.\n",
        "*   **Jupyter Notebook Environment:** To run `draft_standalone_audio_layer.ipynb`.\n",
        "\n",
        "## Setup and Installation\n",
        "\n",
        "1.  **Clone/Download the Notebook:**\n",
        "    Obtain the `draft_standalone_audio_layer.ipynb` file.\n",
        "\n",
        "2.  **Install Dependencies:**\n",
        "    The orchestrator relies on several Google Cloud client libraries and standard Python packages. Install them using pip:\n",
        "    ```bash\n",
        "    pip install google-cloud-texttospeech google-cloud-storage google-cloud-aiplatform asyncio logging\n",
        "    ```\n",
        "    *(Note: `google-cloud-storage` is listed due to its import, though the GCS caching feature is currently commented out in favor of in-memory caching in the provided code. Other standard libraries like `json`, `hashlib`, etc., are part of Python's standard distribution.)*\n",
        "\n",
        "3.  **Google Cloud Authentication:**\n",
        "    Ensure your environment is authenticated to Google Cloud. The recommended methods are:\n",
        "    *   **Application Default Credentials (ADC):** Run `gcloud auth application-default login` in your terminal.\n",
        "    *   **Service Account Key:** Download a service account JSON key file with appropriate permissions (e.g., \"Text-to-Speech API User\", \"Vertex AI User\"). Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of this JSON file, or pass the path directly to the `VertexTTSOrchestrator` during initialization.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "The orchestrator primarily requires the following configurations, ideally set via environment variables for production deployments:\n",
        "\n",
        "*   `GOOGLE_CLOUD_PROJECT`: Your Google Cloud Project ID. (Required)\n",
        "*   `GOOGLE_CLOUD_REGION`: The Google Cloud region for Vertex AI operations (e.g., `us-central1`). Defaults to `us-central1` if not set. (Optional, but recommended)\n",
        "*   `GOOGLE_APPLICATION_CREDENTIALS`: Path to your service account key JSON file. (Optional if using ADC via `gcloud`)\n",
        "\n",
        "These are used by the `main_usage_example()` function within the notebook. When instantiating `VertexTTSOrchestrator` directly, these (and other parameters like `cost_threshold_usd`) can be passed as arguments.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `draft_standalone_audio_layer.ipynb` notebook contains all class definitions and the `main_usage_example()` function. To use the orchestrator:\n",
        "\n",
        "1.  **Open the Notebook:** Launch the `.ipynb` file in a Jupyter Notebook environment (e.g., JupyterLab, VS Code with Python extension).\n",
        "2.  **Run All Cells (or relevant cells):** Execute the cells containing the class definitions (`TTSOrchestratorError` and its descendants, `VoiceQuality`, `TTSRequest`, `CostMetrics`, `CircuitBreaker`, `AdaptiveRateLimiter`, `VertexTTSOrchestrator`).\n",
        "3.  **Instantiate and Use the Orchestrator:**\n",
        "\n",
        "    ```python\n",
        "    # Ensure necessary imports and class definitions from the notebook are executed first.\n",
        "\n",
        "    # Example: Initialize the orchestrator (ensure project_id is set)\n",
        "    # project_id = \"your-gcp-project-id\" # Or retrieved from os.getenv\n",
        "    # orchestrator = VertexTTSOrchestrator(project_id=project_id)\n",
        "\n",
        "    # --- Single TTS Request ---\n",
        "    # request_data = TTSRequest(\n",
        "    #     text=\"Hello, world! This is a test of the TTS orchestrator.\",\n",
        "    #     language_code=\"en-US\",\n",
        "    #     quality_tier=VoiceQuality.NEURAL2\n",
        "    # )\n",
        "    # audio_bytes, metadata = await orchestrator.synthesize_speech(request_data)\n",
        "    # if audio_bytes:\n",
        "    #     with open(\"output_single.mp3\", \"wb\") as f:\n",
        "    #         f.write(audio_bytes)\n",
        "    #     print(f\"Single synthesis successful. Metadata: {metadata}\")\n",
        "    # else:\n",
        "    #     print(f\"Single synthesis failed. Error: {metadata.get('error')}\")\n",
        "\n",
        "    # --- Batch TTS Requests ---\n",
        "    # batch_requests = [\n",
        "    #     TTSRequest(text=\"First batch item.\", language_code=\"en-US\", priority=1),\n",
        "    #     TTSRequest(text=\"Second batch item, in French.\", language_code=\"fr-FR\", quality_tier=VoiceQuality.STANDARD, priority=2)\n",
        "    # ]\n",
        "    # results = await orchestrator.batch_synthesize(batch_requests)\n",
        "    # for i, (audio_bytes, metadata) in enumerate(results):\n",
        "    #     if audio_bytes:\n",
        "    #         with open(f\"output_batch_{i}.mp3\", \"wb\") as f:\n",
        "    #             f.write(audio_bytes)\n",
        "    #         print(f\"Batch item {i} successful. Metadata: {metadata}\")\n",
        "    #     else:\n",
        "    #         print(f\"Batch item {i} failed. Error: {metadata.get('error')}\")\n",
        "\n",
        "    # --- Get Cost Analysis ---\n",
        "    # cost_report = orchestrator.get_cost_analysis()\n",
        "    # import json\n",
        "    # print(json.dumps(cost_report, indent=2))\n",
        "    ```\n",
        "\n",
        "    Refer to the `main_usage_example()` function in the notebook for a comprehensive demonstration.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "*   **`VertexTTSOrchestrator`:** The main class orchestrating all TTS operations.\n",
        "*   **`TTSRequest`:** Dataclass for defining and validating individual TTS requests.\n",
        "*   **`VoiceQuality` (Enum):** Defines available voice quality tiers and their cost implications.\n",
        "*   **`CostMetrics`:** Thread-safe dataclass for tracking usage, costs, and performance metrics.\n",
        "*   **`CircuitBreaker`:** Implements the circuit breaker pattern for API call resilience.\n",
        "*   **`AdaptiveRateLimiter`:** Manages API request rates dynamically.\n",
        "*   **Custom Exceptions:** A hierarchy of custom exceptions (`TTSOrchestratorError`, `ValidationError`, etc.) for granular error reporting.\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The system employs a robust error handling strategy:\n",
        "*   **Input Validation:** `TTSRequest` validates all inputs upon instantiation.\n",
        "*   **Custom Exceptions:** Specific exceptions are raised for different error conditions (e.g., `ConfigurationError`, `SynthesisFailedError`, `APICallError`).\n",
        "*   **Circuit Breaker & Rate Limiter:** These components inherently handle and mitigate errors related to service unavailability or rate limiting.\n",
        "*   **Detailed Logging:** Errors are logged with contextual information for easier debugging.\n",
        "\n",
        "## Cost Optimization Strategies\n",
        "\n",
        "The orchestrator implements several strategies to minimize TTS costs:\n",
        "1.  **Intelligent Voice Selection:** Prioritizes standard voices if cost thresholds are approached, even if premium voices are requested.\n",
        "2.  **Caching:** Stores frequently requested audio, significantly reducing API calls for repeated content.\n",
        "3.  **Free Tier Awareness:** The `_calculate_estimated_cost` method considers Google Cloud's free tier limits when estimating costs for individual requests.\n",
        "4.  **Efficient Batching:** Processes multiple requests concurrently, optimizing resource utilization (though cost per character remains the primary driver).\n",
        "\n",
        "## Logging and Monitoring\n",
        "\n",
        "*   The system uses Python's standard `logging` module.\n",
        "*   Logs are structured to include timestamps, logger names, levels, thread names, and messages.\n",
        "*   The `CostMetrics` class provides data that can be periodically snapshot and sent to monitoring systems for tracking TTS usage, costs, and performance over time.\n",
        "\n",
        "## Running the Example\n",
        "\n",
        "The `draft_standalone_audio_layer.ipynb` notebook includes a `main_usage_example()` asynchronous function. To run it:\n",
        "1.  Ensure all class definitions in the notebook are executed.\n",
        "2.  Set the `GOOGLE_CLOUD_PROJECT` environment variable (and optionally `GOOGLE_CLOUD_REGION` and `GOOGLE_APPLICATION_CREDENTIALS`).\n",
        "3.  Execute the cell containing the `if __name__ == \"__main__\":` block, which calls `asyncio.run(main_usage_example())`.\n",
        "\n",
        "Output audio files (`output_single_*.mp3`, `output_batch_*.mp3`) will be saved in the same directory as the notebook, and a cost analysis report will be printed to the console and logs.\n",
        "\n",
        "## Limitations and Future Work\n",
        "\n",
        "*   **In-Memory Cache:** The current caching mechanism is in-memory. For distributed or persistent caching in larger production systems, integration with external cache stores like Redis or Memcached would be beneficial.\n",
        "*   **Single Notebook Structure:** While convenient for demonstration, for use as a library in larger projects, the classes should ideally be refactored into separate Python modules (`.py` files) for better organization and importability.\n",
        "*   **Dynamic Region/Endpoint Configuration:** While the region is configurable, more advanced endpoint management for Vertex AI or TTS could be added if required.\n",
        "*   **Advanced Analytics:** The current cost analysis is rule-based. More sophisticated trend analysis or predictive cost forecasting could be built on top of the collected `CostMetrics`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome to enhance the capabilities and robustness of the Vertex AI TTS Orchestrator. Please follow these guidelines:\n",
        "\n",
        "1.  **Fork the Repository** (if this were a Git repository).\n",
        "2.  **Create a Feature Branch:** (`git checkout -b feature/YourFeatureName`)\n",
        "3.  **Adhere to Coding Standards:**\n",
        "    *   Ensure all code is compliant with PEP-8.\n",
        "    *   Write comprehensive unit tests for new features or bug fixes.\n",
        "    *   Update documentation (docstrings, README) as necessary.\n",
        "    *   Maintain the high level of in-text commenting established in the project.\n",
        "4.  **Commit Your Changes:** (`git commit -m 'Add some YourFeatureName'`)\n",
        "5.  **Push to the Branch:** (`git push origin feature/YourFeatureName`)\n",
        "6.  **Open a Pull Request.**\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file (conceptual, as no separate file is provided for a single notebook) for details.\n",
        "\n",
        "*(The MIT License is a permissive free software license originating at the Massachusetts Institute of Technology (MIT). As a permissive license, it puts only very limited restriction on reuse and has, therefore, high license compatibility.)*\n",
        "\n",
        "--\n",
        "\n",
        "This README provides a comprehensive guide to understanding, setting up, and utilizing the Vertex AI TTS Orchestrator. It is designed to meet the rigorous standards of professional, implementation-grade software documentation."
      ],
      "metadata": {
        "id": "2R4ZLdSFjGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Exception Types for better error differentiation\n",
        "# Defines the base custom exception for all orchestrator-specific errors.\n",
        "class TTSOrchestratorError(Exception):\n",
        "    \"\"\"Base exception for the TTS Orchestrator.\"\"\"\n",
        "    # Indicates that this class definition is complete and no further methods are added here.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception for errors occurring during orchestrator initialization.\n",
        "class InitializationError(TTSOrchestratorError):\n",
        "    \"\"\"Error during orchestrator initialization.\"\"\"\n",
        "    # Indicates that this class definition is complete and no further methods are added here.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception for errors related to configuration values.\n",
        "class ConfigurationError(TTSOrchestratorError):\n",
        "    \"\"\"Error related to configuration values.\"\"\"\n",
        "    # Indicates that this class definition is complete and no further methods are added here.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception, 'ValidationError', inheriting from both 'TTSOrchestratorError' and the built-in 'ValueError',\n",
        "# specifically for errors related to invalid input data provided to the system.\n",
        "class ValidationError(TTSOrchestratorError, ValueError):\n",
        "    \"\"\"Error for invalid input data.\"\"\"\n",
        "    # This 'pass' statement signifies that the class body is intentionally empty,\n",
        "    # as the class derives its primary functionality and identity from its parent classes and its specific semantic role.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception, 'VoiceSelectionError', inheriting from 'TTSOrchestratorError',\n",
        "# designated for errors encountered during the intelligent voice selection process.\n",
        "class VoiceSelectionError(TTSOrchestratorError):\n",
        "    \"\"\"Error during intelligent voice selection.\"\"\"\n",
        "    # This 'pass' statement signifies that the class body is intentionally empty,\n",
        "    # as the class derives its primary functionality and identity from its parent class and its specific semantic role.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception, 'SynthesisFailedError', inheriting from 'TTSOrchestratorError',\n",
        "# specifically for errors that occur during the speech synthesis process itself.\n",
        "class SynthesisFailedError(TTSOrchestratorError):\n",
        "    \"\"\"Error during speech synthesis.\"\"\"\n",
        "    # This 'pass' statement signifies that the class body is intentionally empty,\n",
        "    # as the class derives its primary functionality and identity from its parent class and its specific semantic role.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception, 'CacheError', inheriting from 'TTSOrchestratorError',\n",
        "# designated for errors specifically related to caching operations within the system.\n",
        "class CacheError(TTSOrchestratorError):\n",
        "    \"\"\"Error related to caching operations.\"\"\"\n",
        "    # This 'pass' statement signifies that the class body is intentionally empty,\n",
        "    # as the class derives its primary functionality and identity from its parent class and its specific semantic role.\n",
        "    pass\n",
        "\n",
        "# Defines a custom exception, 'APICallError', inheriting from 'TTSOrchestratorError',\n",
        "# intended for errors encountered during an API call to an external Google Cloud service.\n",
        "class APICallError(TTSOrchestratorError):\n",
        "    \"\"\"Error during an API call to a Google Cloud service.\"\"\"\n",
        "    # This 'pass' statement signifies that the class body is intentionally empty,\n",
        "    # as the class derives its primary functionality and identity from its parent class and its specific semantic role.\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Joc6wO71a-1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `VoiceQuality(Enum)` (including its methods)**\n",
        "# Configure root logger early for comprehensive logging\n",
        "# This line configures the basic settings for the root logger.\n",
        "logging.basicConfig(\n",
        "    # Set the minimum logging level to INFO.\n",
        "    level=logging.INFO,\n",
        "    # Define the format string for log messages.\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(message)s',\n",
        "    # Define the date format for the 'asctime' field in log messages.\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "# Get a logger instance specific to the current module (__name__).\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Defines an enumeration 'VoiceQuality' inheriting from 'Enum', representing different voice quality tiers\n",
        "# with associated cost implications and pricing details for Google Cloud Text-to-Speech.\n",
        "class VoiceQuality(Enum):\n",
        "    \"\"\"\n",
        "    Enumerates voice quality tiers with associated cost implications and pricing details.\n",
        "\n",
        "    Each enum value represents a different pricing tier in Google Cloud Text-to-Speech.\n",
        "    Accurate pricing is crucial for cost-efficient operations. As of June 2025 (illustrative):\n",
        "    - STANDARD: Typically offers a larger free tier (e.g., 4M characters/month), then a lower price (e.g., $4/1M characters).\n",
        "    - WAVENET: A premium tier, often with a smaller free tier (e.g., 1M characters/month), then a higher price (e.g., $16/1M characters).\n",
        "    - NEURAL2: Another premium tier, similar in pricing to WaveNet (e.g., 1M free characters/month, then $16/1M characters).\n",
        "    These values should be verified against the latest Google Cloud TTS pricing documentation.\n",
        "    \"\"\"\n",
        "    # Defines the STANDARD enum member, representing cost-efficient standard voices,\n",
        "    # with an associated string value \"standard\". E.g., $4/1M chars after 4M free monthly.\n",
        "    STANDARD = \"standard\"\n",
        "    # Defines the WAVENET enum member, representing high-quality WaveNet voices,\n",
        "    # with an associated string value \"wavenet\". E.g., $16/1M chars after 1M free monthly.\n",
        "    WAVENET = \"wavenet\"\n",
        "    # Defines the NEURAL2 enum member, representing the latest Neural2 voices,\n",
        "    # with an associated string value \"neural2\". E.g., $16/1M chars after 1M free monthly.\n",
        "    NEURAL2 = \"neural2\"\n",
        "\n",
        "    # Defines a method to retrieve the cost per million characters for the voice quality tier.\n",
        "    def get_cost_per_million_characters(self) -> float:\n",
        "        \"\"\"\n",
        "        Returns the cost in USD per million characters for this voice quality tier,\n",
        "        applicable after the free tier is exhausted.\n",
        "\n",
        "        Returns:\n",
        "            float: Cost in USD per million characters.\n",
        "        \"\"\"\n",
        "        # Check if the current enum instance (self) is VoiceQuality.STANDARD.\n",
        "        if self == VoiceQuality.STANDARD:\n",
        "            # Return the cost (4.0 USD) for standard voices per million characters.\n",
        "            return 4.0\n",
        "        # Handle cases where the voice quality is not STANDARD (i.e., WAVENET or NEURAL2).\n",
        "        else: # WAVENET or NEURAL2\n",
        "            # Return the cost (16.0 USD) for premium voices (WaveNet/Neural2) per million characters.\n",
        "            return 16.0\n",
        "\n",
        "    # Defines a method to retrieve the monthly free tier character limit for the voice quality.\n",
        "    def get_free_tier_character_limit(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the monthly free tier character limit for this voice quality.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of free characters allocated per month for this tier.\n",
        "        \"\"\"\n",
        "        # Check if the current enum instance (self) is VoiceQuality.STANDARD.\n",
        "        if self == VoiceQuality.STANDARD:\n",
        "            # Return the monthly free character limit (4,000,000) for standard voices.\n",
        "            return 4_000_000\n",
        "        # Handle cases where the voice quality is not STANDARD (i.e., WAVENET or NEURAL2).\n",
        "        else: # WAVENET or NEURAL2\n",
        "            # Return the monthly free character limit (1,000,000) for premium voices.\n",
        "            return 1_000_000\n",
        "\n",
        "    # Defines a static method to determine VoiceQuality from a Google Cloud TTS voice name string.\n",
        "    @staticmethod\n",
        "    # Specifies the method signature: takes a 'voice_name' string and returns a 'VoiceQuality' enum member.\n",
        "    def from_voice_name(voice_name: str) -> 'VoiceQuality':\n",
        "        \"\"\"\n",
        "        Determines the VoiceQuality tier from a Google Cloud TTS voice name.\n",
        "\n",
        "        Args:\n",
        "            voice_name: The name of the voice (e.g., \"en-US-Standard-C\", \"en-US-Wavenet-A\", \"en-US-Neural2-F\").\n",
        "\n",
        "        Returns:\n",
        "            VoiceQuality: The corresponding quality tier.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the voice name does not match a known quality pattern.\n",
        "        \"\"\"\n",
        "        # Perform a substring check to determine if \"Neural2\" is present in the 'voice_name' string.\n",
        "        if \"Neural2\" in voice_name:\n",
        "            # Return the VoiceQuality.NEURAL2 enum member if \"Neural2\" is found.\n",
        "            return VoiceQuality.NEURAL2\n",
        "        # Else, perform a substring check to determine if \"Wavenet\" is present in the 'voice_name' string.\n",
        "        elif \"Wavenet\" in voice_name: # Note: Google often uses \"Wavenet\" (capital W) in names.\n",
        "            # Return the VoiceQuality.WAVENET enum member if \"Wavenet\" is found.\n",
        "            return VoiceQuality.WAVENET\n",
        "        # Else, perform a substring check to determine if \"Standard\" is present in the 'voice_name' string.\n",
        "        elif \"Standard\" in voice_name:\n",
        "            # Return the VoiceQuality.STANDARD enum member if \"Standard\" is found.\n",
        "            return VoiceQuality.STANDARD\n",
        "        # Handle cases where the voice name does not match any of the known quality patterns.\n",
        "        else:\n",
        "            # This indicates an unknown or new voice type not covered by the enum logic.\n",
        "            # Log a warning message indicating inability to determine quality and the defaulting behavior.\n",
        "            logger.warning(f\"Could not determine voice quality from voice name: {voice_name}. Defaulting to STANDARD.\")\n",
        "            # Default to and return VoiceQuality.STANDARD for unrecognized voice name patterns.\n",
        "            return VoiceQuality.STANDARD\n"
      ],
      "metadata": {
        "id": "-Qzhu64eche9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the 'TTSRequest' dataclass, a structured representation for Text-to-Speech (TTS) requests,\n",
        "# incorporating comprehensive validation, cost optimization metadata, and tracking identifiers.\n",
        "@dataclass\n",
        "class TTSRequest:\n",
        "    \"\"\"\n",
        "    Structured Text-to-Speech (TTS) request with comprehensive validation,\n",
        "    cost optimization metadata, and tracking identifiers.\n",
        "\n",
        "    This dataclass encapsulates all parameters required for TTS synthesis.\n",
        "    It includes built-in validation in `__post_init__` to ensure data integrity\n",
        "    before processing.\n",
        "    \"\"\"\n",
        "    # Defines the 'text' field: The primary text content to be synthesized into speech. Must be a non-empty string.\n",
        "    text: str\n",
        "    # Defines the 'voice_name' field: Optional specific voice name (e.g., \"en-US-Neural2-A\"). Defaults to None.\n",
        "    voice_name: Optional[str] = None\n",
        "    # Defines the 'language_code' field: Language code in BCP-47 format (e.g., 'en-US'). Defaults to 'en-US'.\n",
        "    language_code: str = \"en-US\"\n",
        "    # Defines the 'quality_tier' field: Desired voice quality tier, affecting audio and cost. Defaults to VoiceQuality.STANDARD.\n",
        "    quality_tier: VoiceQuality = VoiceQuality.STANDARD\n",
        "    # Defines the 'ssml_enabled' field: Flag indicating if 'text' contains SSML tags. Defaults to False.\n",
        "    ssml_enabled: bool = False\n",
        "    # Defines the 'speaking_rate' field: Speaking rate/speed (1.0 is normal). Defaults to 1.0.\n",
        "    speaking_rate: float = 1.0\n",
        "    # Defines the 'pitch' field: Pitch adjustment (0.0 is normal). Defaults to 0.0.\n",
        "    pitch: float = 0.0\n",
        "    # Defines the 'audio_encoding' field: Audio encoding format. Defaults to MP3.\n",
        "    audio_encoding: texttospeech.AudioEncoding = texttospeech.AudioEncoding.MP3\n",
        "\n",
        "    # --- Internal fields, typically set by the orchestrator ---\n",
        "    # Defines 'cache_key': Generated cache key for this request. Not part of public representation. Defaults to None.\n",
        "    cache_key: Optional[str] = field(default=None, repr=False)\n",
        "    # Defines 'estimated_cost': Estimated USD cost for this request. Not part of public representation. Defaults to 0.0.\n",
        "    estimated_cost: float = field(default=0.0, repr=False)\n",
        "    # Defines 'priority': Priority for batch processing (1=highest). Defaults to 5.\n",
        "    priority: int = field(default=5)\n",
        "    # Defines 'request_id': Unique identifier for this request, generated via UUID.\n",
        "    request_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
        "    # Defines 'created_at': Timestamp (UTC) of request object creation.\n",
        "    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "\n",
        "    # Defines the post-initialization hook method, automatically called after the dataclass instance is created.\n",
        "    def __post_init__(self):\n",
        "        \"\"\"\n",
        "        Performs validation on all fields after the dataclass instance is initialized.\n",
        "        Ensures that the request data is sane and meets predefined constraints.\n",
        "\n",
        "        Raises:\n",
        "            ValidationError: If any field contains invalid data according to the defined rules.\n",
        "            TypeError: If any field has an incorrect data type.\n",
        "        \"\"\"\n",
        "        # Validate 'text': ensure it is a string and not empty or only whitespace.\n",
        "        if not isinstance(self.text, str) or not self.text.strip():\n",
        "            # Construct an error message for invalid 'text'.\n",
        "            error_msg = f\"Request {self.request_id}: Text must be a non-empty string.\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "        # Define the maximum allowed length for the 'text' field in bytes (UTF-8 encoded).\n",
        "        max_length = 5000 # General limit, can be more specific if API docs state so.\n",
        "        # Check if the UTF-8 byte length of 'text' exceeds the defined 'max_length'.\n",
        "        if len(self.text.encode('utf-8')) > max_length: # Check byte length as per Google's quota\n",
        "            # Construct an error message for 'text' length exceeding the maximum.\n",
        "            error_msg = f\"Request {self.request_id}: Text length ({len(self.text.encode('utf-8'))} bytes) exceeds maximum allowed ({max_length} bytes).\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "\n",
        "        # Validate 'language_code': ensure it conforms to the BCP-47 format by calling the internal validation method.\n",
        "        if not self._is_valid_language_code(self.language_code):\n",
        "            # Construct an error message for an invalid 'language_code' format.\n",
        "            error_msg = f\"Request {self.request_id}: Invalid language code format: '{self.language_code}'. Must be BCP-47.\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "\n",
        "        # Validate 'quality_tier': ensure it is an instance of the VoiceQuality enum.\n",
        "        if not isinstance(self.quality_tier, VoiceQuality):\n",
        "            # This check is for runtime safety, though type hinting should ideally catch this.\n",
        "            # Construct an error message for an invalid 'quality_tier' type.\n",
        "            error_msg = f\"Request {self.request_id}: quality_tier must be a VoiceQuality enum value.\"\n",
        "            # Raise a TypeError with the constructed message (as it's a type mismatch).\n",
        "            raise TypeError(error_msg)\n",
        "\n",
        "        # Validate 'priority': ensure it is an integer and within the defined range [1, 10].\n",
        "        if not isinstance(self.priority, int) or not (1 <= self.priority <= 10):\n",
        "            # Construct an error message for an invalid 'priority' value.\n",
        "            error_msg = f\"Request {self.request_id}: Priority must be an integer between 1 and 10, got {self.priority}.\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "\n",
        "        # Validate 'text' content if 'ssml_enabled' is True, using the internal SSML validation method.\n",
        "        if self.ssml_enabled and not self._is_valid_ssml(self.text):\n",
        "            # Construct an error message for invalid or malformed SSML in 'text'.\n",
        "            error_msg = f\"Request {self.request_id}: Invalid or malformed SSML markup detected in text when ssml_enabled is True.\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "\n",
        "        # Validate 'speaking_rate': ensure it is a number (int or float) and within Google's typical accepted range [0.25, 4.0].\n",
        "        if not isinstance(self.speaking_rate, (int, float)) or not (0.25 <= self.speaking_rate <= 4.0):\n",
        "            # Construct an error message for an invalid 'speaking_rate' value.\n",
        "            error_msg = f\"Request {self.request_id}: Speaking rate must be a float between 0.25 and 4.0, got {self.speaking_rate}.\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "\n",
        "        # Validate 'pitch': ensure it is a number (int or float) and within Google's typical accepted range [-20.0, 20.0].\n",
        "        if not isinstance(self.pitch, (int, float)) or not (-20.0 <= self.pitch <= 20.0):\n",
        "            # Construct an error message for an invalid 'pitch' value.\n",
        "            error_msg = f\"Request {self.request_id}: Pitch must be a float between -20.0 and 20.0, got {self.pitch}.\"\n",
        "            # Raise a ValidationError with the constructed message.\n",
        "            raise ValidationError(error_msg)\n",
        "\n",
        "        # Validate 'audio_encoding': ensure it is an instance of the texttospeech.AudioEncoding enum.\n",
        "        if not isinstance(self.audio_encoding, texttospeech.AudioEncoding):\n",
        "            # Construct an error message for an invalid 'audio_encoding' type.\n",
        "            error_msg = f\"Request {self.request_id}: audio_encoding must be a texttospeech.AudioEncoding enum value.\"\n",
        "            # Raise a TypeError with the constructed message (as it's a type mismatch).\n",
        "            raise TypeError(error_msg)\n",
        "\n",
        "    # Defines an internal helper method to validate the BCP-47 format of a language code string.\n",
        "    def _is_valid_language_code(self, code: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validates if the given language code string conforms to the BCP-47 standard format.\n",
        "        Example: 'en-US', 'fr-FR', 'es', 'zh-CN'.\n",
        "\n",
        "        Args:\n",
        "            code: The language code string to validate.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the code is a valid BCP-47 format, False otherwise.\n",
        "        \"\"\"\n",
        "        # Define the regular expression pattern for BCP-47 language code validation.\n",
        "        # This pattern is a common approximation for language[-script][-region] formats.\n",
        "        pattern = r'^[a-zA-Z]{2,3}(-[a-zA-Z]{2,4})?(-[a-zA-Z0-9]{2,8})*$'\n",
        "        # Check if 'code' is a string and if it fully matches the defined 'pattern'.\n",
        "        # The 'bool()' conversion ensures a True/False return value from 're.fullmatch' result.\n",
        "        return isinstance(code, str) and bool(re.fullmatch(pattern, code))\n",
        "\n",
        "    # Defines an internal helper method to perform basic validation on SSML text.\n",
        "    def _is_valid_ssml(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Performs basic validation on SSML text to catch common malformations.\n",
        "        Checks for the presence and basic balance of <speak> tags.\n",
        "        Note: This is not a full XML/SSML parser. For production, a more robust\n",
        "        SSML validation library might be considered if complex SSML is used.\n",
        "\n",
        "        Args:\n",
        "            text: The SSML string to validate.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the SSML appears to be minimally valid, False otherwise.\n",
        "        \"\"\"\n",
        "        # Check if the trimmed 'text' starts with '<speak>' and ends with '</speak>'.\n",
        "        if not (text.strip().startswith('<speak>') and text.strip().endswith('</speak>')):\n",
        "            # Log a debug message indicating the SSML validation failure due to missing/misplaced <speak> tags.\n",
        "            logger.debug(f\"Request {self.request_id}: SSML validation failed: Missing or misplaced <speak> tags.\")\n",
        "            # Return False as the SSML is invalid.\n",
        "            return False\n",
        "\n",
        "        # Perform a basic count of common opening SSML tags for a simplified balance check.\n",
        "        open_tags = text.count('<speak>') + text.count('<voice') + text.count('<prosody') + text.count('<say-as') + text.count('<break')\n",
        "        # Perform a basic count of common closing SSML tags for the simplified balance check.\n",
        "        # Note: Self-closing tags like <break /> are not explicitly handled differently in this simplified count.\n",
        "        close_tags = text.count('</speak>') + text.count('</voice>') + text.count('</prosody>') + text.count('</say-as>')\n",
        "\n",
        "        # Check if the count of closing tags is greater than opening tags, which indicates a definite malformation.\n",
        "        if open_tags < close_tags : # More closing than opening is definitely wrong\n",
        "             # Log a debug message indicating the SSML validation failure due to mismatched tag counts.\n",
        "             logger.debug(f\"Request {self.request_id}: SSML validation failed: Mismatched tag counts (open: {open_tags}, close: {close_tags}).\")\n",
        "             # Return False as the SSML is invalid.\n",
        "             return False\n",
        "        # Note: open_tags > close_tags can occur with unclosed tags or self-closing tags not perfectly accounted for by this heuristic.\n",
        "\n",
        "        # Return True if the SSML passes these basic validation checks.\n",
        "        return True # Passes basic checks.\n",
        "\n",
        "    # Defines a method to calculate the billable character count for the TTS request.\n",
        "    def get_billable_character_count(self) -> int:\n",
        "        \"\"\"\n",
        "        Calculates the number of characters that will be billed by Google Cloud TTS.\n",
        "        If SSML is enabled, tags are typically not counted towards the billable characters.\n",
        "        The exact rules should be confirmed with Google Cloud documentation.\n",
        "\n",
        "        Returns:\n",
        "            int: The count of billable characters.\n",
        "        \"\"\"\n",
        "        # Check if SSML processing is disabled for this request.\n",
        "        if not self.ssml_enabled:\n",
        "            # If SSML is not enabled, the entire length of the 'text' is billable.\n",
        "            return len(self.text)\n",
        "\n",
        "        # If SSML is enabled, remove all SSML tags from the 'text' to count only the speakable content.\n",
        "        # This regular expression aims to remove any content enclosed in <...>.\n",
        "        plain_text = re.sub(r'<[^>]+>', '', self.text)\n",
        "        # Return the length of the SSML-stripped plain text, which represents the billable characters.\n",
        "        return len(plain_text)\n"
      ],
      "metadata": {
        "id": "Caa6kszUdTRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the 'CostMetrics' dataclass, responsible for managing comprehensive, thread-safe\n",
        "# cost tracking and analysis for Text-to-Speech (TTS) usage.\n",
        "@dataclass\n",
        "class CostMetrics:\n",
        "    \"\"\"\n",
        "    Manages comprehensive, thread-safe cost tracking and analysis for TTS usage.\n",
        "\n",
        "    This class maintains detailed metrics related to character processing,\n",
        "    cost accumulation, cache performance, and request statistics. All update\n",
        "    operations are thread-safe using an internal lock.\n",
        "    \"\"\"\n",
        "    # Defines 'characters_processed': Total characters from successful synthesis requests. Initialized to 0.\n",
        "    characters_processed: int = 0\n",
        "    # Defines 'standard_voice_usage': Characters processed using STANDARD quality voices. Initialized to 0.\n",
        "    standard_voice_usage: int = 0\n",
        "    # Defines 'premium_voice_usage': Characters processed using premium (WaveNet, Neural2) voices. Initialized to 0.\n",
        "    premium_voice_usage: int = 0\n",
        "    # Defines 'estimated_monthly_cost': Running total of estimated costs in USD. Initialized to 0.0.\n",
        "    estimated_monthly_cost: float = 0.0\n",
        "    # Defines 'cache_hit_rate': Cache hit rate as a decimal (e.g., 0.75 for 75%). Initialized to 0.0.\n",
        "    cache_hit_rate: float = 0.0\n",
        "    # Defines 'successful_requests': Count of successful synthesis requests. Initialized to 0.\n",
        "    successful_requests: int = 0\n",
        "    # Defines 'failed_requests': Count of failed synthesis requests. Initialized to 0.\n",
        "    failed_requests: int = 0\n",
        "    # Defines 'cache_hits': Total times a result was served from cache. Initialized to 0.\n",
        "    cache_hits: int = 0\n",
        "    # Defines 'cache_misses': Total times a result was not found in cache. Initialized to 0.\n",
        "    cache_misses: int = 0\n",
        "    # Defines 'avg_response_time_ms': Rolling average response time for successful synthesis (ms). Initialized to 0.0.\n",
        "    avg_response_time_ms: float = 0.0\n",
        "    # Defines 'last_reset_utc': Timestamp (UTC) of the last monthly metrics reset. Defaults to current UTC time.\n",
        "    last_reset_utc: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    # Defines '_lock': Internal threading.Lock for thread-safe updates. Not part of public API, not initialized by user.\n",
        "    _lock: threading.Lock = field(default_factory=threading.Lock, init=False, repr=False)\n",
        "\n",
        "    # Defines a method to thread-safely update metrics after a successful synthesis operation.\n",
        "    def update_on_synthesis_success(self, character_count: int, quality_tier: VoiceQuality, cost: float, response_time_ms: float) -> None:\n",
        "        \"\"\"\n",
        "        Thread-safely updates metrics after a successful synthesis operation.\n",
        "\n",
        "        Args:\n",
        "            character_count: Number of billable characters processed in the request.\n",
        "            quality_tier: The VoiceQuality tier used for the synthesis.\n",
        "            cost: Estimated cost incurred for this synthesis request.\n",
        "            response_time_ms: Time taken for the synthesis operation in milliseconds.\n",
        "        \"\"\"\n",
        "        # Assert that the provided 'character_count' is non-negative.\n",
        "        assert character_count >= 0, \"Character count for metric update cannot be negative.\"\n",
        "        # Assert that the provided 'cost' is non-negative.\n",
        "        assert cost >= 0.0, \"Cost for metric update cannot be negative.\"\n",
        "        # Assert that the provided 'response_time_ms' is non-negative.\n",
        "        assert response_time_ms >= 0.0, \"Response time for metric update cannot be negative.\"\n",
        "\n",
        "        # Acquire the internal thread lock to ensure atomic updates to shared metric fields.\n",
        "        with self._lock:\n",
        "            # Increment the total 'characters_processed' by the 'character_count' of the current request.\n",
        "            self.characters_processed += character_count\n",
        "            # Check if the 'quality_tier' used for the synthesis was STANDARD.\n",
        "            if quality_tier == VoiceQuality.STANDARD:\n",
        "                # Increment 'standard_voice_usage' by the 'character_count'.\n",
        "                self.standard_voice_usage += character_count\n",
        "            # Handle cases where a premium quality tier (WaveNet or Neural2) was used.\n",
        "            else:\n",
        "                # Increment 'premium_voice_usage' by the 'character_count'.\n",
        "                self.premium_voice_usage += character_count\n",
        "            # Add the 'cost' of the current synthesis to the 'estimated_monthly_cost'.\n",
        "            self.estimated_monthly_cost += cost\n",
        "            # Increment the count of 'successful_requests'.\n",
        "            self.successful_requests += 1\n",
        "\n",
        "            # Calculate the total number of completed requests for averaging response time.\n",
        "            total_completed_requests = self.successful_requests # Only average successful ones for API perf\n",
        "            # Check if this is the first successful request.\n",
        "            if total_completed_requests == 1: # First successful request\n",
        "                 # Set the 'avg_response_time_ms' directly to the 'response_time_ms' of this first request.\n",
        "                 self.avg_response_time_ms = response_time_ms\n",
        "            # Handle subsequent successful requests to update the rolling average.\n",
        "            else:\n",
        "                 # Update 'avg_response_time_ms' using the rolling average formula: NewAvg = ((OldAvg * (N-1)) + NewValue) / N.\n",
        "                 self.avg_response_time_ms = ((self.avg_response_time_ms * (total_completed_requests - 1)) + response_time_ms) / total_completed_requests\n",
        "\n",
        "    # Defines a method to thread-safely update cache performance metrics.\n",
        "    def update_cache_metrics(self, cache_hit: bool) -> None:\n",
        "        \"\"\"\n",
        "        Thread-safely updates cache performance metrics (hits, misses, hit rate).\n",
        "\n",
        "        Args:\n",
        "            cache_hit: True if the request was served from cache, False otherwise.\n",
        "        \"\"\"\n",
        "        # Acquire the internal thread lock to ensure atomic updates to shared cache metric fields.\n",
        "        with self._lock:\n",
        "            # Check if the cache lookup resulted in a hit.\n",
        "            if cache_hit:\n",
        "                # Increment the 'cache_hits' counter.\n",
        "                self.cache_hits += 1\n",
        "            # Handle cases where the cache lookup resulted in a miss.\n",
        "            else:\n",
        "                # Increment the 'cache_misses' counter.\n",
        "                self.cache_misses += 1\n",
        "\n",
        "            # Calculate the total number of cache lookups (hits + misses).\n",
        "            total_cache_lookups = self.cache_hits + self.cache_misses\n",
        "            # Check if there have been any cache lookups to avoid division by zero.\n",
        "            if total_cache_lookups > 0:\n",
        "                # Recalculate 'cache_hit_rate' as the ratio of hits to total lookups.\n",
        "                self.cache_hit_rate = self.cache_hits / total_cache_lookups\n",
        "            # Handle the case where there have been no cache lookups yet.\n",
        "            else:\n",
        "                # Set 'cache_hit_rate' to 0.0 to avoid division by zero.\n",
        "                self.cache_hit_rate = 0.0\n",
        "\n",
        "    # Defines a method to thread-safely record a failed synthesis attempt.\n",
        "    def record_synthesis_failure(self) -> None:\n",
        "        \"\"\"\n",
        "        Thread-safely records a failed synthesis attempt.\n",
        "        \"\"\"\n",
        "        # Acquire the internal thread lock to ensure atomic update to the 'failed_requests' counter.\n",
        "        with self._lock:\n",
        "            # Increment the 'failed_requests' counter.\n",
        "            self.failed_requests += 1\n",
        "\n",
        "    # Defines a method to return a thread-safe snapshot of all current metric values.\n",
        "    def get_current_metrics_snapshot(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns a thread-safe snapshot of all current metric values.\n",
        "        This is useful for reporting or analysis without direct mutable access.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary containing the current state of all metrics.\n",
        "        \"\"\"\n",
        "        # Acquire the internal thread lock to ensure a consistent and atomic read of all metric fields.\n",
        "        with self._lock:\n",
        "            # Create and return a dictionary containing copies of all current metric values.\n",
        "            # This prevents external modification of the internal metric state.\n",
        "            return {\n",
        "                'characters_processed': self.characters_processed,\n",
        "                'standard_voice_usage': self.standard_voice_usage,\n",
        "                'premium_voice_usage': self.premium_voice_usage,\n",
        "                'estimated_monthly_cost': self.estimated_monthly_cost,\n",
        "                'cache_hit_rate': self.cache_hit_rate,\n",
        "                'successful_requests': self.successful_requests,\n",
        "                'failed_requests': self.failed_requests,\n",
        "                'cache_hits': self.cache_hits,\n",
        "                'cache_misses': self.cache_misses,\n",
        "                'avg_response_time_ms': self.avg_response_time_ms,\n",
        "                # Convert the 'last_reset_utc' datetime object to an ISO 8601 string format for serialization.\n",
        "                'last_reset_utc': self.last_reset_utc.isoformat()\n",
        "            }\n",
        "\n",
        "    # Defines a method to thread-safely reset metrics typically tracked on a monthly basis.\n",
        "    def reset_monthly_metrics(self) -> None:\n",
        "        \"\"\"\n",
        "        Thread-safely resets metrics that are typically tracked on a monthly basis\n",
        "        (e.g., usage counts, estimated cost). Cache performance and overall\n",
        "        request counts might be kept longer or reset differently.\n",
        "        \"\"\"\n",
        "        # Acquire the internal thread lock to ensure an atomic reset of monthly metric fields.\n",
        "        with self._lock:\n",
        "            # Reset 'standard_voice_usage' to 0.\n",
        "            self.standard_voice_usage = 0\n",
        "            # Reset 'premium_voice_usage' to 0.\n",
        "            self.premium_voice_usage = 0\n",
        "            # Reset 'estimated_monthly_cost' to 0.0.\n",
        "            self.estimated_monthly_cost = 0.0\n",
        "            # Update 'last_reset_utc' to the current UTC date and time.\n",
        "            self.last_reset_utc = datetime.now(timezone.utc)\n",
        "            # Log an informational message indicating that monthly metrics have been reset.\n",
        "            logger.info(\"Monthly cost metrics have been reset.\")\n",
        "            # Note: Other metrics like 'characters_processed', cache stats, 'avg_response_time_ms'\n",
        "            # are not reset here as they might be tracked over different periods.\n"
      ],
      "metadata": {
        "id": "mHC0O8TdeFPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the 'CircuitBreaker' class, implementing the Circuit Breaker reliability pattern\n",
        "# to enhance resilience against failing external API calls.\n",
        "class CircuitBreaker:\n",
        "    \"\"\"\n",
        "    Implements the Circuit Breaker pattern to enhance resilience against failing external API calls.\n",
        "\n",
        "    When an external service (like Google Cloud TTS API) experiences issues,\n",
        "    the circuit breaker prevents an application from repeatedly trying an operation\n",
        "    that is likely to fail. This saves resources and avoids cascading failures.\n",
        "\n",
        "    States:\n",
        "    - CLOSED: Normal operation. Calls pass through. Failures are counted.\n",
        "    - OPEN: After a threshold of failures, the circuit opens. Calls are rejected immediately\n",
        "            without attempting the operation. After a timeout, transitions to HALF_OPEN.\n",
        "    - HALF_OPEN: Allows a limited number of test calls. If successful, transitions to CLOSED.\n",
        "                 If it fails, transitions back to OPEN.\n",
        "    \"\"\"\n",
        "    # Defines a class constant representing the 'CLOSED' state of the circuit breaker.\n",
        "    STATE_CLOSED = \"CLOSED\"\n",
        "    # Defines a class constant representing the 'OPEN' state of the circuit breaker.\n",
        "    STATE_OPEN = \"OPEN\"\n",
        "    # Defines a class constant representing the 'HALF_OPEN' state of the circuit breaker.\n",
        "    STATE_HALF_OPEN = \"HALF_OPEN\"\n",
        "\n",
        "    # Defines the constructor for the CircuitBreaker class.\n",
        "    def __init__(self, failure_threshold: int = 5, recovery_timeout_seconds: float = 60.0, half_open_attempt_limit: int = 1):\n",
        "        \"\"\"\n",
        "        Initializes the CircuitBreaker.\n",
        "\n",
        "        Args:\n",
        "            failure_threshold: Number of consecutive failures to trigger opening the circuit.\n",
        "            recovery_timeout_seconds: Time in seconds the circuit stays OPEN before transitioning to HALF_OPEN.\n",
        "            half_open_attempt_limit: Number of allowed attempts in HALF_OPEN state.\n",
        "        \"\"\"\n",
        "        # Assign the 'failure_threshold' argument to the instance variable 'self.failure_threshold'.\n",
        "        self.failure_threshold = failure_threshold\n",
        "        # Assign the 'recovery_timeout_seconds' argument to the instance variable 'self.recovery_timeout_seconds'.\n",
        "        self.recovery_timeout_seconds = recovery_timeout_seconds\n",
        "        # Assign the 'half_open_attempt_limit' argument to the instance variable 'self.half_open_attempt_limit'.\n",
        "        self.half_open_attempt_limit = half_open_attempt_limit\n",
        "\n",
        "        # Initialize the internal counter for consecutive failures to 0.\n",
        "        self._failure_count = 0\n",
        "        # Initialize the timestamp of the last recorded failure to None, as no failures have occurred yet.\n",
        "        self._last_failure_time_utc: Optional[datetime] = None\n",
        "        # Initialize the current state of the circuit breaker to 'STATE_CLOSED'.\n",
        "        self._state = CircuitBreaker.STATE_CLOSED\n",
        "        # Initialize the counter for attempts made in the 'HALF_OPEN' state to 0.\n",
        "        self._half_open_attempts = 0\n",
        "        # Initialize a threading.Lock object for ensuring thread-safe modifications to the circuit breaker's state.\n",
        "        self._lock = threading.Lock()\n",
        "        # Obtain a logger instance specific to this CircuitBreaker class instance.\n",
        "        self._logger = logging.getLogger(f\"{__name__}.CircuitBreaker\")\n",
        "\n",
        "    # Defines a property 'state' to get the current state of the circuit breaker in a thread-safe manner.\n",
        "    @property\n",
        "    # Specifies the method signature for the state getter.\n",
        "    def state(self) -> str:\n",
        "        \"\"\"Returns the current state of the circuit breaker.\"\"\"\n",
        "        # Acquire the internal thread lock to ensure thread-safe access to the '_state' attribute.\n",
        "        with self._lock:\n",
        "            # Return the current value of the '_state' attribute.\n",
        "            return self._state\n",
        "\n",
        "    # Defines an internal method to determine if a call can be attempted based on the current circuit breaker state.\n",
        "    def _can_attempt_call(self) -> bool:\n",
        "        \"\"\"Determines if a call can be attempted based on the current state.\"\"\"\n",
        "        # Check if the current state of the circuit breaker is 'STATE_CLOSED'.\n",
        "        if self._state == CircuitBreaker.STATE_CLOSED:\n",
        "            # If CLOSED, always allow the call attempt.\n",
        "            return True\n",
        "        # Check if the current state of the circuit breaker is 'STATE_OPEN'.\n",
        "        if self._state == CircuitBreaker.STATE_OPEN:\n",
        "            # Check if a last failure time is recorded and if the recovery timeout has elapsed.\n",
        "            if self._last_failure_time_utc and \\\n",
        "               (datetime.now(timezone.utc) - self._last_failure_time_utc).total_seconds() >= self.recovery_timeout_seconds:\n",
        "                # If timeout elapsed, transition the state to 'STATE_HALF_OPEN'.\n",
        "                self._state = CircuitBreaker.STATE_HALF_OPEN\n",
        "                # Reset the half-open attempt counter for the new HALF_OPEN phase.\n",
        "                self._half_open_attempts = 0\n",
        "                # Log the state transition to HALF_OPEN.\n",
        "                self._logger.info(\"Circuit breaker transitioned to HALF_OPEN state.\")\n",
        "                # Allow the first attempt in the new HALF_OPEN state.\n",
        "                return True\n",
        "            # If still in OPEN state and timeout has not elapsed, do not allow the call attempt.\n",
        "            return False\n",
        "        # Check if the current state of the circuit breaker is 'STATE_HALF_OPEN'.\n",
        "        if self._state == CircuitBreaker.STATE_HALF_OPEN:\n",
        "            # Allow the call attempt if the number of half-open attempts is less than the defined limit.\n",
        "            return self._half_open_attempts < self.half_open_attempt_limit\n",
        "        # This path should ideally not be reached if states are managed correctly; return False as a safeguard.\n",
        "        return False\n",
        "\n",
        "    # Defines an asynchronous method to execute a function under circuit breaker protection.\n",
        "    async def execute_async(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:\n",
        "        \"\"\"\n",
        "        Executes an asynchronous function under circuit breaker protection.\n",
        "\n",
        "        Args:\n",
        "            func: The asynchronous function to execute.\n",
        "            *args: Positional arguments for the function.\n",
        "            **kwargs: Keyword arguments for the function.\n",
        "\n",
        "        Returns:\n",
        "            The result of the function if successful.\n",
        "\n",
        "        Raises:\n",
        "            APICallError: If the circuit is OPEN or if the function fails while in HALF_OPEN\n",
        "                               (causing a transition back to OPEN), or if the underlying call fails.\n",
        "            Exception: Any exception raised by the wrapped function if it's not an APICallError.\n",
        "        \"\"\"\n",
        "        # Acquire the internal thread lock to check and potentially update the circuit breaker's state atomically.\n",
        "        with self._lock:\n",
        "            # Determine if the call can be attempted based on the current state.\n",
        "            if not self._can_attempt_call():\n",
        "                # If the call cannot be attempted (e.g., circuit is OPEN or HALF_OPEN attempts exhausted), log a warning.\n",
        "                self._logger.warning(f\"Circuit breaker is {self._state}. Call to {func.__name__} rejected.\")\n",
        "                # Raise an APICallError indicating that the call was rejected by the circuit breaker.\n",
        "                raise APICallError(f\"Circuit breaker is {self._state}. Call rejected.\")\n",
        "\n",
        "            # Check if the circuit breaker is currently in the 'STATE_HALF_OPEN'.\n",
        "            if self._state == CircuitBreaker.STATE_HALF_OPEN:\n",
        "                # If in HALF_OPEN state, increment the counter for half-open attempts.\n",
        "                self._half_open_attempts += 1\n",
        "\n",
        "        # The lock is released before executing the (potentially long-running) wrapped function 'func'.\n",
        "        # Begin a try block to handle potential exceptions during the execution of 'func'.\n",
        "        try:\n",
        "            # Await the execution of the provided asynchronous function 'func' with its arguments.\n",
        "            result = await func(*args, **kwargs)\n",
        "            # If 'func' executes successfully, record this success with the circuit breaker.\n",
        "            self.record_success()\n",
        "            # Return the result obtained from the successful execution of 'func'.\n",
        "            return result\n",
        "        # Catch any exception 'e' that occurs during the execution of 'func'.\n",
        "        except Exception as e:\n",
        "            # If 'func' fails, record this failure with the circuit breaker.\n",
        "            self.record_failure()\n",
        "            # Log an error message detailing the failure recorded by the circuit breaker.\n",
        "            self._logger.error(f\"Circuit breaker recorded failure for {func.__name__}: {e}\", exc_info=True)\n",
        "            # Check if the caught exception 'e' is already an instance of APICallError.\n",
        "            if isinstance(e, APICallError):\n",
        "                # If it is, re-raise the original APICallError.\n",
        "                raise\n",
        "            # If 'e' is not an APICallError, wrap it in a new APICallError and raise it, preserving the original exception.\n",
        "            raise APICallError(f\"Call to {func.__name__} failed: {e}\") from e\n",
        "\n",
        "    # Defines a method to record a successful call, potentially closing or resetting the circuit.\n",
        "    def record_success(self) -> None:\n",
        "        \"\"\"Records a successful call, potentially closing or resetting the circuit.\"\"\"\n",
        "        # Acquire the internal thread lock to ensure atomic updates to the circuit breaker's state.\n",
        "        with self._lock:\n",
        "            # Check if the circuit breaker is currently in the 'STATE_HALF_OPEN'.\n",
        "            if self._state == CircuitBreaker.STATE_HALF_OPEN:\n",
        "                # If in HALF_OPEN and the call was successful, transition the state to 'STATE_CLOSED'.\n",
        "                self._state = CircuitBreaker.STATE_CLOSED\n",
        "                # Reset the consecutive failure count to 0.\n",
        "                self._failure_count = 0\n",
        "                # Reset the half-open attempt counter to 0.\n",
        "                self._half_open_attempts = 0\n",
        "                # Log the state transition to CLOSED.\n",
        "                self._logger.info(\"Circuit breaker transitioned to CLOSED state after successful HALF_OPEN attempt.\")\n",
        "            # Check if the circuit breaker is currently in the 'STATE_CLOSED'.\n",
        "            elif self._state == CircuitBreaker.STATE_CLOSED:\n",
        "                # If in CLOSED state and there were previous intermittent failures (failure_count > 0), reset the count.\n",
        "                if self._failure_count > 0:\n",
        "                    # Reset the consecutive failure count to 0 upon any successful call in CLOSED state.\n",
        "                    self._failure_count = 0\n",
        "                    # Log a debug message about resetting the failure count in CLOSED state.\n",
        "                    self._logger.debug(\"Circuit breaker failure count reset in CLOSED state due to success.\")\n",
        "\n",
        "    # Defines a method to record a failed call, potentially opening the circuit.\n",
        "    def record_failure(self) -> None:\n",
        "        \"\"\"Records a failed call, potentially opening the circuit.\"\"\"\n",
        "        # Acquire the internal thread lock to ensure atomic updates to the circuit breaker's state.\n",
        "        with self._lock:\n",
        "            # Update the timestamp of the last recorded failure to the current UTC time.\n",
        "            self._last_failure_time_utc = datetime.now(timezone.utc)\n",
        "            # Check if the circuit breaker is currently in the 'STATE_HALF_OPEN'.\n",
        "            if self._state == CircuitBreaker.STATE_HALF_OPEN:\n",
        "                # If a failure occurs in HALF_OPEN state, transition back to 'STATE_OPEN'.\n",
        "                self._state = CircuitBreaker.STATE_OPEN\n",
        "                # Set failure count to threshold to ensure it remains open for the recovery timeout period.\n",
        "                self._failure_count = self.failure_threshold\n",
        "                # Log a warning about the transition back to OPEN state due to failure in HALF_OPEN.\n",
        "                self._logger.warning(\"Circuit breaker transitioned back to OPEN state due to failure in HALF_OPEN.\")\n",
        "            # Check if the circuit breaker is currently in the 'STATE_CLOSED'.\n",
        "            elif self._state == CircuitBreaker.STATE_CLOSED:\n",
        "                # If in CLOSED state, increment the consecutive failure count.\n",
        "                self._failure_count += 1\n",
        "                # Log a debug message about the incremented failure count in CLOSED state.\n",
        "                self._logger.debug(f\"Circuit breaker failure count incremented to {self._failure_count} in CLOSED state.\")\n",
        "                # Check if the failure count has reached or exceeded the defined failure threshold.\n",
        "                if self._failure_count >= self.failure_threshold:\n",
        "                    # If the threshold is met, transition the state to 'STATE_OPEN'.\n",
        "                    self._state = CircuitBreaker.STATE_OPEN\n",
        "                    # Log a warning about the transition to OPEN state due to exceeding the failure threshold.\n",
        "                    self._logger.warning(f\"Circuit breaker transitioned to OPEN state after {self._failure_count} failures.\")\n"
      ],
      "metadata": {
        "id": "WY_hqOPmfQhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the 'AdaptiveRateLimiter' class, which implements an adaptive rate limiting mechanism\n",
        "# that adjusts request delays based on API response patterns.\n",
        "class AdaptiveRateLimiter:\n",
        "    \"\"\"\n",
        "    Implements an adaptive rate limiter that adjusts request delays based on API\n",
        "    response patterns (successes, failures, rate limit errors).\n",
        "\n",
        "    This helps in dynamically managing request rates to avoid hitting API quotas\n",
        "    and to adapt to changing server load conditions.\n",
        "    \"\"\"\n",
        "    # Defines the constructor for the AdaptiveRateLimiter class.\n",
        "    def __init__(self,\n",
        "                 initial_delay_seconds: float = 0.1,\n",
        "                 max_delay_seconds: float = 5.0,\n",
        "                 min_delay_seconds: float = 0.05, # Minimum possible delay\n",
        "                 success_streak_for_decrease: int = 10, # Consecutive successes to decrease delay\n",
        "                 decrease_factor: float = 0.9, # Factor to decrease delay by\n",
        "                 failure_increase_factor: float = 1.5, # Factor to increase delay on general failure\n",
        "                 rate_limit_error_increase_factor: float = 2.0): # Factor for rate limit errors\n",
        "        \"\"\"\n",
        "        Initializes the AdaptiveRateLimiter.\n",
        "\n",
        "        Args:\n",
        "            initial_delay_seconds: Starting delay between requests in seconds.\n",
        "            max_delay_seconds: Maximum possible delay between requests.\n",
        "            min_delay_seconds: Minimum possible delay, even after many successes.\n",
        "            success_streak_for_decrease: Number of consecutive successes before attempting to decrease delay.\n",
        "            decrease_factor: Multiplicative factor to decrease delay (e.g., 0.9 for 10% decrease).\n",
        "            failure_increase_factor: Multiplicative factor to increase delay on a general failure.\n",
        "            rate_limit_error_increase_factor: Multiplicative factor for more aggressive delay increase on rate limit errors.\n",
        "        \"\"\"\n",
        "        # Assign the 'min_delay_seconds' argument to the instance variable 'self.min_delay_seconds'.\n",
        "        self.min_delay_seconds = min_delay_seconds\n",
        "        # Assign the 'max_delay_seconds' argument to the instance variable 'self.max_delay_seconds'.\n",
        "        self.max_delay_seconds = max_delay_seconds\n",
        "        # Initialize 'self.current_delay_seconds', ensuring it's not below 'min_delay_seconds'.\n",
        "        self.current_delay_seconds = max(min_delay_seconds, initial_delay_seconds)\n",
        "\n",
        "        # Initialize the internal counter for consecutive successful requests to 0.\n",
        "        self._success_streak = 0\n",
        "        # Initialize the internal counter for consecutive failed requests to 0 (primarily for monitoring).\n",
        "        self._failure_streak = 0\n",
        "\n",
        "        # Assign 'success_streak_for_decrease' to an internal instance variable.\n",
        "        self._success_streak_for_decrease = success_streak_for_decrease\n",
        "        # Assign 'decrease_factor' to an internal instance variable.\n",
        "        self._decrease_factor = decrease_factor\n",
        "        # Assign 'failure_increase_factor' to an internal instance variable.\n",
        "        self._failure_increase_factor = failure_increase_factor\n",
        "        # Assign 'rate_limit_error_increase_factor' to an internal instance variable.\n",
        "        self._rate_limit_error_increase_factor = rate_limit_error_increase_factor\n",
        "\n",
        "        # Initialize the timestamp of the last request using a monotonic clock (nanoseconds) for accurate interval measurement.\n",
        "        self._last_request_time_ns = time.monotonic_ns()\n",
        "        # Initialize an asyncio.Lock object for ensuring thread-safe (in async context) updates to the limiter's state.\n",
        "        self._lock = asyncio.Lock()\n",
        "        # Obtain a logger instance specific to this AdaptiveRateLimiter class instance.\n",
        "        self._logger = logging.getLogger(f\"{__name__}.AdaptiveRateLimiter\")\n",
        "\n",
        "    # Defines an asynchronous method to acquire permission to proceed, waiting if necessary based on rate limits.\n",
        "    async def acquire(self) -> None:\n",
        "        \"\"\"\n",
        "        Asynchronously waits for the appropriate time based on the current rate limiting state.\n",
        "        This method should be awaited before making an external API call.\n",
        "        \"\"\"\n",
        "        # Declare 'wait_time_ns' to store the calculated wait duration in nanoseconds.\n",
        "        wait_time_ns: int # Type hint for clarity\n",
        "\n",
        "        # Asynchronously acquire the internal lock to ensure atomic calculation of wait time and update of last request time.\n",
        "        async with self._lock:\n",
        "            # Get the current time using a monotonic clock in nanoseconds for precise interval calculation.\n",
        "            current_time_ns = time.monotonic_ns()\n",
        "            # Calculate the time elapsed in nanoseconds since the last request was made.\n",
        "            time_since_last_request_ns = current_time_ns - self._last_request_time_ns\n",
        "            # Calculate the required delay in nanoseconds based on the current delay setting.\n",
        "            required_delay_ns = int(self.current_delay_seconds * 1_000_000_000)\n",
        "\n",
        "            # Initialize 'wait_time_ns' to 0, assuming no wait is needed initially.\n",
        "            wait_time_ns = 0\n",
        "            # Check if the time since the last request is less than the required delay.\n",
        "            if time_since_last_request_ns < required_delay_ns:\n",
        "                # If so, calculate the necessary wait time in nanoseconds.\n",
        "                wait_time_ns = required_delay_ns - time_since_last_request_ns\n",
        "\n",
        "            # Update the last request time to the current time plus any enforced wait time.\n",
        "            # This effectively reserves the slot for the current request and ensures subsequent calls respect this one.\n",
        "            self._last_request_time_ns = current_time_ns + wait_time_ns\n",
        "\n",
        "        # Perform the actual wait (sleep) outside the lock to avoid holding it during the sleep period.\n",
        "        # Check if a non-zero wait time was calculated.\n",
        "        if wait_time_ns > 0:\n",
        "            # Convert the wait time from nanoseconds to seconds for asyncio.sleep.\n",
        "            wait_time_seconds = wait_time_ns / 1_000_000_000\n",
        "            # Log a debug message indicating the rate limiter is waiting.\n",
        "            self._logger.debug(f\"Rate limiter waiting for {wait_time_seconds:.3f} seconds. Current delay: {self.current_delay_seconds:.3f}s.\")\n",
        "            # Asynchronously pause execution for the calculated 'wait_time_seconds'.\n",
        "            await asyncio.sleep(wait_time_seconds)\n",
        "\n",
        "    # Defines an asynchronous method to record a successful API request and adjust rate limiting delay.\n",
        "    async def record_success(self) -> None:\n",
        "        \"\"\"\n",
        "        Records a successful API request and potentially adjusts the rate limiting delay downwards.\n",
        "        \"\"\"\n",
        "        # Asynchronously acquire the internal lock to ensure atomic updates to rate limiter state.\n",
        "        async with self._lock:\n",
        "            # Increment the consecutive success streak counter.\n",
        "            self._success_streak += 1\n",
        "            # Reset the consecutive failure streak counter upon a successful request.\n",
        "            self._failure_streak = 0\n",
        "\n",
        "            # Check if the success streak has reached the threshold required for decreasing the delay.\n",
        "            if self._success_streak >= self._success_streak_for_decrease:\n",
        "                # Calculate the new delay by applying the decrease factor.\n",
        "                new_delay = self.current_delay_seconds * self._decrease_factor\n",
        "                # Update the current delay, ensuring it does not fall below the defined minimum delay.\n",
        "                self.current_delay_seconds = max(self.min_delay_seconds, new_delay)\n",
        "                # Reset the success streak counter after adjusting the delay.\n",
        "                self._success_streak = 0\n",
        "                # Log a debug message indicating the delay decrease.\n",
        "                self._logger.debug(f\"Rate limiter delay decreased to {self.current_delay_seconds:.3f}s after success streak.\")\n",
        "\n",
        "    # Defines an asynchronous method to record a failed API request and adjust rate limiting delay.\n",
        "    async def record_failure(self, is_rate_limit_error: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Records a failed API request and adjusts the rate limiting delay upwards.\n",
        "        More aggressive increase for specific rate limit errors.\n",
        "\n",
        "        Args:\n",
        "            is_rate_limit_error: True if the failure was due to an explicit rate limiting error from the API.\n",
        "        \"\"\"\n",
        "        # Asynchronously acquire the internal lock to ensure atomic updates to rate limiter state.\n",
        "        async with self._lock:\n",
        "            # Increment the consecutive failure streak counter.\n",
        "            self._failure_streak += 1\n",
        "            # Reset the consecutive success streak counter upon any failure.\n",
        "            self._success_streak = 0\n",
        "\n",
        "            # Determine the appropriate increase factor based on whether it's a rate limit error.\n",
        "            increase_factor = self._rate_limit_error_increase_factor if is_rate_limit_error else self._failure_increase_factor\n",
        "            # Calculate the new delay by applying the selected increase factor.\n",
        "            new_delay = self.current_delay_seconds * increase_factor\n",
        "            # Update the current delay, ensuring it does not exceed the defined maximum delay.\n",
        "            self.current_delay_seconds = min(self.max_delay_seconds, new_delay)\n",
        "            # Determine the logging level based on the error type (WARNING for rate limit errors, INFO otherwise).\n",
        "            log_level = logging.WARNING if is_rate_limit_error else logging.INFO\n",
        "            # Log a message indicating the delay increase and the reason.\n",
        "            self._logger.log(log_level, f\"Rate limiter delay increased to {self.current_delay_seconds:.3f}s due to failure (rate_limit_error={is_rate_limit_error}).\")\n"
      ],
      "metadata": {
        "id": "RX7wC_4Gf_3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the 'VertexTTSOrchestrator' class, providing a production-grade solution\n",
        "# for Text-to-Speech (TTS) synthesis using Google Cloud services.\n",
        "class VertexTTSOrchestrator:\n",
        "    \"\"\"\n",
        "    Production-grade Text-to-Speech (TTS) orchestrator for Google Cloud.\n",
        "\n",
        "    This class provides a robust and cost-efficient solution for TTS synthesis,\n",
        "    incorporating:\n",
        "    - Comprehensive input validation and error handling.\n",
        "    - Dynamic discovery and validation of available TTS voices.\n",
        "    - Intelligent voice selection based on quality preference and cost considerations.\n",
        "    - Thread-safe operations for shared resources (e.g., cost metrics, cache).\n",
        "    - True asynchronous API calls for non-blocking I/O.\n",
        "    - Caching mechanism with Time-To-Live (TTL) for audio responses.\n",
        "    - Reliability patterns: Circuit Breaker for API resilience, Adaptive Rate Limiter\n",
        "      for managing request quotas, and retry mechanisms for transient errors.\n",
        "    - Detailed cost tracking and optimization recommendations.\n",
        "    - Professional-grade logging for observability and debugging.\n",
        "    - Concurrency management for batch operations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Defines a class constant for the default Google Cloud region if not specified.\n",
        "    DEFAULT_REGION = \"us-central1\"\n",
        "    # Defines a class constant for the default monthly cost threshold in USD for triggering cost-saving measures.\n",
        "    DEFAULT_COST_THRESHOLD_USD = 100.0\n",
        "    # Defines a class constant for the default maximum number of concurrent synthesis requests.\n",
        "    DEFAULT_MAX_CONCURRENT_REQUESTS = 10\n",
        "    # Defines a class constant for the default TTL for cached audio items in seconds (e.g., 24 hours).\n",
        "    DEFAULT_CACHE_TTL_SECONDS = 24 * 60 * 60\n",
        "    # Defines a class constant for the factor of the cost threshold at which to consider downgrading voice quality.\n",
        "    COST_THRESHOLD_DOWNGRADE_FACTOR = 0.8\n",
        "    # Defines a class constant for the maximum number of items in the in-memory cache.\n",
        "    MAX_CACHE_SIZE = 1000\n",
        "\n",
        "    # Defines the constructor for the VertexTTSOrchestrator class.\n",
        "    def __init__(self,\n",
        "                 project_id: str,\n",
        "                 region: str = DEFAULT_REGION,\n",
        "                 enable_caching: bool = True,\n",
        "                 cache_ttl_seconds: int = DEFAULT_CACHE_TTL_SECONDS,\n",
        "                 cost_threshold_usd: float = DEFAULT_COST_THRESHOLD_USD,\n",
        "                 max_concurrent_requests: int = DEFAULT_MAX_CONCURRENT_REQUESTS,\n",
        "                 enable_circuit_breaker: bool = True,\n",
        "                 google_application_credentials_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initializes the VertexTTSOrchestrator with comprehensive validation and setup.\n",
        "\n",
        "        Args:\n",
        "            project_id: Google Cloud Project ID. Must be a valid format.\n",
        "            region: Vertex AI region (e.g., \"us-central1\"). Must be a supported region.\n",
        "            enable_caching: If True, enables caching of synthesized audio.\n",
        "            cache_ttl_seconds: Time-To-Live for cached items in seconds.\n",
        "            cost_threshold_usd: Monthly cost threshold in USD. If estimated costs approach this,\n",
        "                                 the orchestrator may switch to more cost-effective voice tiers.\n",
        "            max_concurrent_requests: Maximum number of concurrent TTS synthesis operations.\n",
        "            enable_circuit_breaker: If True, enables the circuit breaker pattern for API calls.\n",
        "            google_application_credentials_path: Optional path to Google Cloud service account JSON key file.\n",
        "                                                 If None, attempts to use default credentials.\n",
        "\n",
        "        Raises:\n",
        "            ConfigurationError: If project_id, region, or other parameters are invalid.\n",
        "            InitializationError: If client initialization or other setup fails.\n",
        "        \"\"\"\n",
        "        # Obtain a logger instance specific to this VertexTTSOrchestrator class instance.\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "        # Log an informational message indicating the start of orchestrator initialization.\n",
        "        self.logger.info(f\"Initializing VertexTTSOrchestrator for project '{project_id}' in region '{region}'.\")\n",
        "\n",
        "        # Validate the format of the provided 'project_id' using an internal helper method.\n",
        "        if not self._is_valid_project_id(project_id):\n",
        "            # If 'project_id' is invalid, raise a ConfigurationError.\n",
        "            raise ConfigurationError(f\"Invalid Google Cloud Project ID format: '{project_id}'.\")\n",
        "        # Assign the validated 'project_id' to an instance variable.\n",
        "        self.project_id = project_id\n",
        "\n",
        "        # Validate the provided 'region' using an internal helper method.\n",
        "        if not self._is_valid_vertex_ai_region(region):\n",
        "            # If 'region' is invalid or unsupported, raise a ConfigurationError.\n",
        "            raise ConfigurationError(f\"Invalid or unsupported Vertex AI region: '{region}'.\")\n",
        "        # Assign the validated 'region' to an instance variable.\n",
        "        self.region = region\n",
        "\n",
        "        # Validate that 'cost_threshold_usd' is a positive number (integer or float).\n",
        "        if not isinstance(cost_threshold_usd, (int, float)) or cost_threshold_usd <= 0:\n",
        "            # If 'cost_threshold_usd' is invalid, raise a ConfigurationError.\n",
        "            raise ConfigurationError(f\"Cost threshold (cost_threshold_usd) must be a positive number, got {cost_threshold_usd}.\")\n",
        "        # Assign the validated 'cost_threshold_usd' to an instance variable.\n",
        "        self.cost_threshold_usd = cost_threshold_usd\n",
        "\n",
        "        # Validate that 'max_concurrent_requests' is a positive integer.\n",
        "        if not isinstance(max_concurrent_requests, int) or max_concurrent_requests <= 0:\n",
        "            # If 'max_concurrent_requests' is invalid, raise a ConfigurationError.\n",
        "            raise ConfigurationError(f\"Max concurrent requests must be a positive integer, got {max_concurrent_requests}.\")\n",
        "        # Initialize an asyncio.Semaphore to limit concurrent asynchronous synthesis operations.\n",
        "        self._async_semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
        "\n",
        "        # Assign the 'enable_caching' boolean flag to an instance variable.\n",
        "        self.enable_caching = enable_caching\n",
        "        # Validate that 'cache_ttl_seconds' is a positive integer.\n",
        "        if not isinstance(cache_ttl_seconds, int) or cache_ttl_seconds <= 0:\n",
        "            # If 'cache_ttl_seconds' is invalid, raise a ConfigurationError.\n",
        "            raise ConfigurationError(f\"Cache TTL (cache_ttl_seconds) must be a positive integer, got {cache_ttl_seconds}.\")\n",
        "        # Convert 'cache_ttl_seconds' to a timedelta object and store it as 'self.cache_ttl'.\n",
        "        self.cache_ttl = timedelta(seconds=cache_ttl_seconds)\n",
        "        # Initialize an empty dictionary for in-memory cache storage.\n",
        "        self._cache_storage: Dict[str, Tuple[bytes, datetime]] = {}\n",
        "        # Initialize a threading.Lock for thread-safe access to the '_cache_storage'.\n",
        "        self._cache_lock = threading.Lock()\n",
        "        # Initialize an empty list to maintain cache access order for LRU eviction.\n",
        "        self._cache_access_order: List[str] = []\n",
        "\n",
        "        # Initialize an instance of CostMetrics for thread-safe cost tracking.\n",
        "        self.cost_metrics = CostMetrics()\n",
        "\n",
        "        # Initialize a CircuitBreaker instance if 'enable_circuit_breaker' is True, otherwise set to None.\n",
        "        self.circuit_breaker = CircuitBreaker() if enable_circuit_breaker else None\n",
        "        # Initialize an instance of AdaptiveRateLimiter for managing request rates.\n",
        "        self.rate_limiter = AdaptiveRateLimiter()\n",
        "\n",
        "        # Initialize Google Cloud clients using the internal '_init_clients' method.\n",
        "        self._init_clients(google_application_credentials_path)\n",
        "\n",
        "        # Initialize voice mappings: 'cost_optimized' and 'premium' tiers, initially empty.\n",
        "        self.voice_mapping: Dict[str, Dict[str, Dict[str, Any]]] = {\"cost_optimized\": {}, \"premium\": {}}\n",
        "        # Initialize an empty set to store all available voice names discovered from the API.\n",
        "        self.available_voices: Set[str] = set()\n",
        "        # Populate 'self.voice_mapping' and 'self.available_voices' by dynamically discovering voices from the TTS API.\n",
        "        self._initialize_voice_mapping_from_api()\n",
        "\n",
        "        # Log an informational message indicating successful orchestrator initialization.\n",
        "        self.logger.info(\"VertexTTSOrchestrator initialized successfully.\")\n",
        "\n",
        "    # Defines an internal helper method to validate the Google Cloud Project ID format.\n",
        "    def _is_valid_project_id(self, project_id: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validates the Google Cloud Project ID format.\n",
        "        Rules: 6-30 chars, lowercase letters, numbers, hyphens. Must start with a letter, cannot end with a hyphen.\n",
        "\n",
        "        Args:\n",
        "            project_id: The project ID string to validate.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the project_id format is valid, False otherwise.\n",
        "        \"\"\"\n",
        "        # Check if the provided 'project_id' is actually a string.\n",
        "        if not isinstance(project_id, str):\n",
        "            # If not a string, it's an invalid format, so return False.\n",
        "            return False\n",
        "        # Define the regular expression pattern for validating Google Cloud project ID constraints.\n",
        "        pattern = r'^[a-z][a-z0-9-]{4,28}[a-z0-9]$'\n",
        "        # Perform a full match of the 'project_id' against the defined 'pattern'.\n",
        "        # Convert the match object (or None) to a boolean value indicating validity.\n",
        "        return bool(re.fullmatch(pattern, project_id))\n",
        "\n",
        "    # Defines an internal helper method to validate the Google Cloud region format and check against known Vertex AI regions.\n",
        "    def _is_valid_vertex_ai_region(self, region: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validates the Google Cloud region format and checks if it's a known Vertex AI supported region.\n",
        "        Note: This list is not exhaustive and might need updates as Google expands regions.\n",
        "              For true dynamic validation, an API call to list available regions would be needed,\n",
        "              but that adds complexity and potential failure points to initialization.\n",
        "\n",
        "        Args:\n",
        "            region: The region string to validate (e.g., \"us-central1\").\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the region format is valid and in the known list, False otherwise.\n",
        "        \"\"\"\n",
        "        # Check if the provided 'region' is actually a string.\n",
        "        if not isinstance(region, str):\n",
        "            # If not a string, it's an invalid format, so return False.\n",
        "            return False\n",
        "        # Define a regular expression pattern for common Google Cloud region name formats.\n",
        "        pattern = r'^[a-z]+-[a-z0-9]+(?:-[a-z0-9]+)?$'\n",
        "        # Check if the 'region' string fully matches the defined 'pattern'.\n",
        "        if not re.fullmatch(pattern, region):\n",
        "            # If the format doesn't match, return False.\n",
        "            return False\n",
        "\n",
        "        # Define a set of known common Vertex AI supported regions for validation.\n",
        "        known_vertex_ai_regions = {\n",
        "            'us-central1', 'us-east1', 'us-east4', 'us-west1', 'us-west2', 'us-west4',\n",
        "            'europe-central2', 'europe-north1', 'europe-southwest1', 'europe-west1',\n",
        "            'europe-west2', 'europe-west3', 'europe-west4', 'europe-west6', 'europe-west8', 'europe-west9',\n",
        "            'asia-east1', 'asia-east2', 'asia-northeast1', 'asia-northeast2', 'asia-northeast3',\n",
        "            'asia-south1', 'asia-southeast1', 'asia-southeast2',\n",
        "            'australia-southeast1', 'australia-southeast2',\n",
        "            'northamerica-northeast1', 'northamerica-northeast2',\n",
        "            'southamerica-east1', 'southamerica-west1',\n",
        "            'me-central1', 'me-west1', 'africa-south1'\n",
        "        }\n",
        "        # Check if the provided 'region' is present in the set of 'known_vertex_ai_regions'.\n",
        "        return region in known_vertex_ai_regions\n",
        "\n",
        "    # Defines an internal method to initialize Google Cloud clients with error handling and authentication.\n",
        "    def _init_clients(self, credentials_path: Optional[str]) -> None:\n",
        "        \"\"\"\n",
        "        Initializes Google Cloud clients (TextToSpeech, Vertex AI, Storage)\n",
        "        with robust error handling, authentication, and connectivity checks.\n",
        "\n",
        "        Args:\n",
        "            credentials_path: Optional path to a service account key file.\n",
        "\n",
        "        Raises:\n",
        "            InitializationError: If client initialization or authentication fails.\n",
        "        \"\"\"\n",
        "        # Log an informational message indicating the start of Google Cloud client initialization.\n",
        "        self.logger.info(\"Initializing Google Cloud clients...\")\n",
        "        # Begin a try block to handle potential exceptions during client initialization.\n",
        "        try:\n",
        "            # Initialize 'creds' to None; it will hold credentials if explicitly found or default ones are used.\n",
        "            creds = None\n",
        "            # Check if a 'credentials_path' was provided.\n",
        "            if credentials_path:\n",
        "                # Check if the file at 'credentials_path' exists.\n",
        "                if not os.path.exists(credentials_path):\n",
        "                    # If the credentials file is not found, raise an InitializationError.\n",
        "                    raise InitializationError(f\"Credentials file not found at: {credentials_path}\")\n",
        "                # Set the 'GOOGLE_APPLICATION_CREDENTIALS' environment variable to the provided path.\n",
        "                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
        "                # Log that credentials from the specified path are being used.\n",
        "                self.logger.info(f\"Using credentials from specified path: {credentials_path}\")\n",
        "            # Else, check if the 'GOOGLE_APPLICATION_CREDENTIALS' environment variable is already set.\n",
        "            elif os.getenv('GOOGLE_APPLICATION_CREDENTIALS'):\n",
        "                 # Log that credentials from the environment variable are being used.\n",
        "                 self.logger.info(f\"Using credentials from GOOGLE_APPLICATION_CREDENTIALS environment variable: {os.getenv('GOOGLE_APPLICATION_CREDENTIALS')}\")\n",
        "            # Else, attempt to use default Google Cloud credentials.\n",
        "            else:\n",
        "                # Log that an attempt is being made to use default credentials.\n",
        "                self.logger.info(\"Attempting to use default Google Cloud credentials.\")\n",
        "                # Begin a try block to handle potential errors when fetching default credentials.\n",
        "                try:\n",
        "                    # Attempt to get default credentials and the associated project ID.\n",
        "                    default_creds, _ = get_default_credentials()\n",
        "                    # Check if default credentials were successfully obtained.\n",
        "                    if default_creds is None:\n",
        "                         # If no default credentials found, raise DefaultCredentialsError.\n",
        "                         raise DefaultCredentialsError(\"No default credentials found.\")\n",
        "                    # Assign the obtained default credentials to 'creds'.\n",
        "                    creds = default_creds\n",
        "                # Catch DefaultCredentialsError if default credentials are not found or invalid.\n",
        "                except DefaultCredentialsError as e:\n",
        "                    # Raise an InitializationError with a detailed message about the authentication failure.\n",
        "                    raise InitializationError(\n",
        "                        \"Google Cloud authentication failed: Default credentials not found or invalid. \"\n",
        "                        \"Ensure you are authenticated (e.g., `gcloud auth application-default login`) \"\n",
        "                        \"or provide a service account key file.\"\n",
        "                    ) from e\n",
        "\n",
        "            # Initialize the Text-to-Speech client.\n",
        "            # Pass 'creds' only if they were explicitly obtained via default mechanism and no path/env var was set.\n",
        "            # Client libraries typically pick up GOOGLE_APPLICATION_CREDENTIALS automatically if set.\n",
        "            self.tts_client = texttospeech.TextToSpeechClient(credentials=creds if creds and not credentials_path and not os.getenv('GOOGLE_APPLICATION_CREDENTIALS') else None)\n",
        "            # Perform a connectivity test for the initialized TTS client.\n",
        "            self._test_tts_client_connectivity()\n",
        "\n",
        "            # Initialize the Vertex AI SDK.\n",
        "            # Pass 'creds' under the same conditions as for the TTS client.\n",
        "            vertexai.init(project=self.project_id, location=self.region, credentials=creds if creds and not credentials_path and not os.getenv('GOOGLE_APPLICATION_CREDENTIALS') else None)\n",
        "            # Log successful initialization of the Vertex AI SDK.\n",
        "            self.logger.info(f\"Vertex AI SDK initialized for project '{self.project_id}' in location '{self.region}'.\")\n",
        "\n",
        "            # Note: Storage client initialization for GCS caching is commented out as in-memory cache is used.\n",
        "            # if self.enable_caching:\n",
        "            #     self.storage_client = storage.Client(project=self.project_id, credentials=creds)\n",
        "            #     self._test_storage_client_connectivity()\n",
        "\n",
        "            # Log successful initialization of all Google Cloud clients.\n",
        "            self.logger.info(\"All Google Cloud clients initialized successfully.\")\n",
        "\n",
        "        # Catch GoogleCloudError specifically, which can occur during client interactions.\n",
        "        except GoogleCloudError as e:\n",
        "            # Log the Google Cloud error that occurred during client initialization.\n",
        "            self.logger.error(f\"A Google Cloud error occurred during client initialization: {e}\", exc_info=True)\n",
        "            # Raise an InitializationError, wrapping the original GoogleCloudError.\n",
        "            raise InitializationError(f\"Failed to initialize Google Cloud clients due to GoogleCloudError: {e}\") from e\n",
        "        # Catch any other unexpected exceptions during client initialization.\n",
        "        except Exception as e:\n",
        "            # Log the unexpected error.\n",
        "            self.logger.error(f\"An unexpected error occurred during client initialization: {e}\", exc_info=True)\n",
        "            # Raise an InitializationError, wrapping the original unexpected error.\n",
        "            raise InitializationError(f\"Unexpected error during client initialization: {e}\") from e\n",
        "\n",
        "    # Defines an internal method to test connectivity to the Google Cloud Text-to-Speech API.\n",
        "    def _test_tts_client_connectivity(self) -> None:\n",
        "        \"\"\"\n",
        "        Tests connectivity to the Google Cloud Text-to-Speech API by listing available voices.\n",
        "\n",
        "        Raises:\n",
        "            InitializationError: If the connectivity test fails.\n",
        "        \"\"\"\n",
        "        # Check if the TTS client has been initialized before attempting a connectivity test.\n",
        "        if not self.tts_client:\n",
        "             # If the TTS client is not initialized, raise an InitializationError.\n",
        "             raise InitializationError(\"TTS client is not initialized before testing connectivity.\")\n",
        "        # Log an informational message indicating the start of the TTS client connectivity test.\n",
        "        self.logger.info(\"Testing Text-to-Speech client connectivity...\")\n",
        "        # Begin a try block to handle potential exceptions during the API call.\n",
        "        try:\n",
        "            # Make a lightweight API call (list_voices) to verify connectivity and authentication.\n",
        "            voices_response = self.tts_client.list_voices()\n",
        "            # Check if the API call returned any voices.\n",
        "            if not voices_response.voices:\n",
        "                # Log a warning if no voices were returned, though the call itself was successful.\n",
        "                self.logger.warning(\"TTS API connectivity test: list_voices returned no voices, but call was successful.\")\n",
        "            # Handle the case where voices were successfully listed.\n",
        "            else:\n",
        "                # Log an informational message confirming successful TTS client connectivity and the number of voices found.\n",
        "                self.logger.info(f\"TTS client connectivity verified: Successfully listed {len(voices_response.voices)} available voices.\")\n",
        "        # Catch GoogleCloudError specifically, which can occur during the API call.\n",
        "        except GoogleCloudError as e:\n",
        "            # Log the error that occurred during the connectivity test.\n",
        "            self.logger.error(f\"TTS client connectivity test failed: {e}\", exc_info=True)\n",
        "            # Raise an InitializationError, wrapping the original GoogleCloudError.\n",
        "            raise InitializationError(f\"TTS client connectivity test failed: {e}\") from e\n",
        "        # Catch any other unexpected exceptions during the connectivity test.\n",
        "        except Exception as e:\n",
        "            # Log the unexpected error.\n",
        "            self.logger.error(f\"Unexpected error during TTS client connectivity test: {e}\", exc_info=True)\n",
        "            # Raise an InitializationError, wrapping the original unexpected error.\n",
        "            raise InitializationError(f\"Unexpected error during TTS client connectivity test: {e}\") from e\n",
        "\n",
        "    # Defines an internal method to dynamically discover and categorize available TTS voices from the API.\n",
        "    def _initialize_voice_mapping_from_api(self) -> None:\n",
        "        \"\"\"\n",
        "        Dynamically discovers available voices from the Google Cloud TTS API and\n",
        "        categorizes them into 'cost_optimized' (Standard) and 'premium' (Neural2, WaveNet) tiers.\n",
        "        Populates `self.voice_mapping` and `self.available_voices`.\n",
        "\n",
        "        Falls back to a minimal hardcoded mapping if API discovery fails.\n",
        "        \"\"\"\n",
        "        # Check if the TTS client has been initialized before attempting voice discovery.\n",
        "        if not self.tts_client:\n",
        "            # Log an error if the TTS client is not available.\n",
        "            self.logger.error(\"TTS client not initialized. Cannot perform dynamic voice discovery.\")\n",
        "            # Initialize voice mapping using a fallback mechanism.\n",
        "            self._initialize_fallback_voice_mapping(\"TTS client not available for dynamic discovery.\")\n",
        "            # Return early as dynamic discovery cannot proceed.\n",
        "            return\n",
        "\n",
        "        # Log an informational message indicating the start of voice mapping initialization from the API.\n",
        "        self.logger.info(\"Initializing voice mapping from Google Cloud TTS API...\")\n",
        "        # Begin a try block to handle potential exceptions during API interaction.\n",
        "        try:\n",
        "            # Retrieve the list of all available voices from the Google Cloud TTS API.\n",
        "            voices_response = self.tts_client.list_voices()\n",
        "            # Check if the API response contains any voices.\n",
        "            if not voices_response.voices:\n",
        "                # Log a warning if no voices were returned by the API.\n",
        "                self.logger.warning(\"No voices returned from TTS API during dynamic discovery.\")\n",
        "                # Initialize voice mapping using a fallback mechanism.\n",
        "                self._initialize_fallback_voice_mapping(\"API returned no voices.\")\n",
        "                # Return early as there are no voices to process.\n",
        "                return\n",
        "\n",
        "            # Initialize temporary dictionaries to build the voice mapping for cost-optimized voices.\n",
        "            cost_optimized_map: Dict[str, Dict[str, Any]] = {}\n",
        "            # Initialize temporary dictionaries to build the voice mapping for premium voices.\n",
        "            premium_map: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "            # Iterate over each 'voice' object in the 'voices_response.voices' list.\n",
        "            for voice in voices_response.voices:\n",
        "                # Check if the current 'voice' has any associated language codes.\n",
        "                if not voice.language_codes:\n",
        "                    # Log a debug message and skip this voice if it has no language codes.\n",
        "                    self.logger.debug(f\"Skipping voice '{voice.name}' as it has no language codes specified.\")\n",
        "                    # Continue to the next voice in the list.\n",
        "                    continue\n",
        "\n",
        "                # Get the primary language code for the current voice (typically the first one listed).\n",
        "                primary_language_code = voice.language_codes[0]\n",
        "                # Determine the VoiceQuality tier (e.g., STANDARD, NEURAL2, WAVENET) from the voice name.\n",
        "                quality = VoiceQuality.from_voice_name(voice.name)\n",
        "\n",
        "                # Prepare a dictionary containing detailed information about the current voice.\n",
        "                voice_info = {\n",
        "                    \"name\": voice.name,\n",
        "                    \"gender\": voice.ssml_gender,\n",
        "                    \"quality_tier_enum\": quality,\n",
        "                    \"natural_sample_rate_hertz\": voice.natural_sample_rate_hertz,\n",
        "                    \"all_language_codes\": list(voice.language_codes)\n",
        "                }\n",
        "                # Add the name of the current voice to the set of all available voice names for quick validation.\n",
        "                self.available_voices.add(voice.name)\n",
        "\n",
        "                # Categorize and store the voice based on its quality tier.\n",
        "                # Check if the voice quality is STANDARD.\n",
        "                if quality == VoiceQuality.STANDARD:\n",
        "                    # If this language code is not yet in 'cost_optimized_map', add this voice.\n",
        "                    # This prefers the first standard voice encountered for a given language.\n",
        "                    if primary_language_code not in cost_optimized_map:\n",
        "                         # Add the 'voice_info' to the 'cost_optimized_map' for its primary language code.\n",
        "                         cost_optimized_map[primary_language_code] = voice_info\n",
        "                # Handle premium voices (Neural2 or WaveNet).\n",
        "                else:\n",
        "                    # Get the currently stored premium voice for this language code, if any.\n",
        "                    current_premium_voice = premium_map.get(primary_language_code)\n",
        "                    # Prefer Neural2 voices over WaveNet if both exist for a language, or add if no premium voice exists yet.\n",
        "                    if not current_premium_voice or \\\n",
        "                       (quality == VoiceQuality.NEURAL2 and current_premium_voice[\"quality_tier_enum\"] == VoiceQuality.WAVENET):\n",
        "                        # Add or update the 'voice_info' in the 'premium_map' for its primary language code.\n",
        "                        premium_map[primary_language_code] = voice_info\n",
        "\n",
        "            # Assign the populated 'cost_optimized_map' to the instance's voice mapping.\n",
        "            self.voice_mapping[\"cost_optimized\"] = cost_optimized_map\n",
        "            # Assign the populated 'premium_map' to the instance's voice mapping.\n",
        "            self.voice_mapping[\"premium\"] = premium_map\n",
        "\n",
        "            # Ensure that essential fallback voices (e.g., for 'en-US') are present in the mapping.\n",
        "            self._ensure_fallback_voices_present()\n",
        "\n",
        "            # Log a summary of the successful voice mapping initialization.\n",
        "            self.logger.info(f\"Voice mapping initialized successfully: \"\n",
        "                             f\"{len(self.available_voices)} total voices discovered. \"\n",
        "                             f\"{len(cost_optimized_map)} languages in cost-optimized tier, \"\n",
        "                             f\"{len(premium_map)} languages in premium tier.\")\n",
        "\n",
        "        # Catch GoogleCloudError specifically, which can occur during API interactions.\n",
        "        except GoogleCloudError as e:\n",
        "            # Log the error and indicate that a fallback mapping will be used.\n",
        "            self.logger.error(f\"GoogleCloudError during dynamic voice discovery: {e}. Using fallback mapping.\", exc_info=True)\n",
        "            # Initialize voice mapping using the fallback mechanism.\n",
        "            self._initialize_fallback_voice_mapping(f\"GoogleCloudError: {e}\")\n",
        "        # Catch any other unexpected exceptions during voice discovery.\n",
        "        except Exception as e:\n",
        "            # Log the unexpected error and indicate that a fallback mapping will be used.\n",
        "            self.logger.error(f\"Unexpected error during dynamic voice discovery: {e}. Using fallback mapping.\", exc_info=True)\n",
        "            # Initialize voice mapping using the fallback mechanism.\n",
        "            self._initialize_fallback_voice_mapping(f\"Unexpected error: {e}\")\n",
        "\n",
        "    # Defines an internal method to initialize a minimal, hardcoded voice mapping as a fallback.\n",
        "    def _initialize_fallback_voice_mapping(self, reason: str) -> None:\n",
        "        \"\"\"\n",
        "        Initializes a minimal, hardcoded voice mapping as a fallback\n",
        "        if dynamic discovery from the API fails.\n",
        "\n",
        "        Args:\n",
        "            reason: The reason why fallback mapping is being used.\n",
        "        \"\"\"\n",
        "        # Log a warning message indicating that fallback voice mapping is being used and the reason.\n",
        "        self.logger.warning(f\"Using fallback voice mapping due to: {reason}\")\n",
        "        # Define a basic, hardcoded voice mapping with common 'en-US' voices.\n",
        "        self.voice_mapping = {\n",
        "            \"cost_optimized\": {\n",
        "                \"en-US\": {\n",
        "                    \"name\": \"en-US-Standard-C\", \"gender\": texttospeech.SsmlVoiceGender.FEMALE,\n",
        "                    \"quality_tier_enum\": VoiceQuality.STANDARD, \"natural_sample_rate_hertz\": 24000,\n",
        "                    \"all_language_codes\": [\"en-US\"]\n",
        "                }\n",
        "            },\n",
        "            \"premium\": {\n",
        "                \"en-US\": {\n",
        "                    \"name\": \"en-US-Neural2-C\", \"gender\": texttospeech.SsmlVoiceGender.FEMALE,\n",
        "                    \"quality_tier_enum\": VoiceQuality.NEURAL2, \"natural_sample_rate_hertz\": 24000,\n",
        "                    \"all_language_codes\": [\"en-US\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        # Clear any existing entries in 'self.available_voices'.\n",
        "        self.available_voices.clear()\n",
        "        # Iterate through the tiers in the fallback 'self.voice_mapping'.\n",
        "        for tier_map in self.voice_mapping.values():\n",
        "            # Iterate through the voice data in each language within the tier.\n",
        "            for lang_voices_data in tier_map.values(): # Corrected variable name\n",
        "                # Add the voice name to the 'self.available_voices' set.\n",
        "                self.available_voices.add(lang_voices_data[\"name\"]) # Corrected variable name\n",
        "        # Log an informational message about the initialized fallback voice mapping.\n",
        "        self.logger.info(f\"Fallback voice mapping initialized with {len(self.available_voices)} voices.\")\n",
        "\n",
        "    # Defines an internal method to ensure essential fallback voices (e.g., for 'en-US') are present.\n",
        "    def _ensure_fallback_voices_present(self) -> None:\n",
        "        \"\"\"\n",
        "        Checks if essential fallback voices (e.g., for 'en-US') are present in the\n",
        "        dynamically generated voice mapping. Logs a warning if not found.\n",
        "        This helps ensure basic functionality even if specific regional voices are missing.\n",
        "        \"\"\"\n",
        "        # Define the default fallback language code (e.g., 'en-US').\n",
        "        default_fallback_lang = \"en-US\"\n",
        "        # Check if the 'default_fallback_lang' is present in the 'cost_optimized' tier of the voice mapping.\n",
        "        if default_fallback_lang not in self.voice_mapping[\"cost_optimized\"]:\n",
        "            # Log a warning if the default fallback language is not found in the cost-optimized tier.\n",
        "            self.logger.warning(f\"Default fallback language '{default_fallback_lang}' not found in 'cost_optimized' voice tier after dynamic discovery.\")\n",
        "            # Iterate through existing languages in the cost-optimized tier to find a suitable 'en-' voice.\n",
        "            for lang, voice_data in self.voice_mapping[\"cost_optimized\"].items():\n",
        "                # Check if the language code starts with \"en-\".\n",
        "                if lang.startswith(\"en-\"):\n",
        "                    # Assign this 'en-' voice data as the fallback for 'default_fallback_lang'.\n",
        "                    self.voice_mapping[\"cost_optimized\"][default_fallback_lang] = voice_data\n",
        "                    # Log that an alternative 'en-' voice has been assigned as fallback.\n",
        "                    self.logger.info(f\"Assigned '{voice_data['name']}' as fallback for '{default_fallback_lang}' in cost_optimized tier.\")\n",
        "                    # Break the loop as a suitable fallback has been found and assigned.\n",
        "                    break\n",
        "\n",
        "        # Check if the 'default_fallback_lang' is present in the 'premium' tier of the voice mapping.\n",
        "        if default_fallback_lang not in self.voice_mapping[\"premium\"]:\n",
        "            # Log a warning if the default fallback language is not found in the premium tier.\n",
        "            self.logger.warning(f\"Default fallback language '{default_fallback_lang}' not found in 'premium' voice tier after dynamic discovery.\")\n",
        "            # Iterate through existing languages in the premium tier to find a suitable 'en-' voice.\n",
        "            for lang, voice_data in self.voice_mapping[\"premium\"].items():\n",
        "                # Check if the language code starts with \"en-\".\n",
        "                if lang.startswith(\"en-\"):\n",
        "                    # Assign this 'en-' voice data as the fallback for 'default_fallback_lang'.\n",
        "                    self.voice_mapping[\"premium\"][default_fallback_lang] = voice_data\n",
        "                    # Log that an alternative 'en-' voice has been assigned as fallback.\n",
        "                    self.logger.info(f\"Assigned '{voice_data['name']}' as fallback for '{default_fallback_lang}' in premium tier.\")\n",
        "                    # Break the loop as a suitable fallback has been found and assigned.\n",
        "                    break\n",
        "\n",
        "    # Defines an internal helper method to get voice configuration from a tier map with simple language fallback.\n",
        "    def _get_voice_config_from_mapping(self, language_code: str, quality_tier_map: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Helper to get voice config from a tier map, with simple language fallback (e.g., 'en-GB' to 'en-US').\n",
        "        \"\"\"\n",
        "        # Attempt to get the voice configuration directly using the provided 'language_code'.\n",
        "        voice_config = quality_tier_map.get(language_code)\n",
        "        # Check if 'voice_config' was not found and if 'language_code' contains a region subtag (e.g., 'en-GB').\n",
        "        if not voice_config and '-' in language_code:\n",
        "            # Extract the generic language part (e.g., 'en' from 'en-GB').\n",
        "            generic_lang = language_code.split('-')[0]\n",
        "            # Iterate through the language codes and voice configurations in the 'quality_tier_map'.\n",
        "            for lc, vc in quality_tier_map.items():\n",
        "                # Check if the current language code 'lc' matches the generic language or starts with the generic language prefix.\n",
        "                if lc == generic_lang or lc.startswith(f\"{generic_lang}-\"):\n",
        "                    # Log a debug message indicating the fallback to a more generic language variant.\n",
        "                    self.logger.debug(f\"Language '{language_code}' not found in tier, falling back to '{lc}'.\")\n",
        "                    # Return the voice configuration 'vc' of the first suitable generic match.\n",
        "                    return vc\n",
        "        # Return the 'voice_config' (which could be None if no direct match or fallback was found).\n",
        "        return voice_config\n",
        "\n",
        "    # Defines an internal method to calculate the estimated cost for a TTSRequest.\n",
        "    def _calculate_estimated_cost(self, tts_request: TTSRequest, actual_quality_tier: VoiceQuality) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the estimated cost for a given TTSRequest based on its character count\n",
        "        and the actual voice quality tier to be used, considering Google Cloud free tiers.\n",
        "\n",
        "        Args:\n",
        "            tts_request: The TTSRequest object containing text and other details.\n",
        "            actual_quality_tier: The VoiceQuality enum member that will actually be used for synthesis.\n",
        "\n",
        "        Returns:\n",
        "            float: The estimated cost in USD for this synthesis request.\n",
        "        \"\"\"\n",
        "        # Get the billable character count from the 'tts_request' (handles SSML stripping).\n",
        "        char_count = tts_request.get_billable_character_count()\n",
        "        # If the character count is zero, there is no cost.\n",
        "        if char_count == 0:\n",
        "            # Return 0.0 as the estimated cost.\n",
        "            return 0.0\n",
        "\n",
        "        # Get a snapshot of the current cost metrics for estimation.\n",
        "        cost_metrics_snapshot = self.cost_metrics.get_current_metrics_snapshot()\n",
        "        # Get the free tier character limit for the 'actual_quality_tier'.\n",
        "        free_tier_limit = actual_quality_tier.get_free_tier_character_limit()\n",
        "        # Get the cost per million characters for the 'actual_quality_tier'.\n",
        "        cost_per_million_chars = actual_quality_tier.get_cost_per_million_characters()\n",
        "\n",
        "        # Initialize 'current_tier_usage' to 0.\n",
        "        current_tier_usage = 0\n",
        "        # Check if the 'actual_quality_tier' is STANDARD.\n",
        "        if actual_quality_tier == VoiceQuality.STANDARD:\n",
        "            # Assign the standard voice usage from the metrics snapshot.\n",
        "            current_tier_usage = cost_metrics_snapshot['standard_voice_usage']\n",
        "        # Handle premium quality tiers (WaveNet, Neural2).\n",
        "        else:\n",
        "            # Assign the premium voice usage from the metrics snapshot.\n",
        "            current_tier_usage = cost_metrics_snapshot['premium_voice_usage']\n",
        "\n",
        "        # Calculate the remaining characters in the free tier for this quality tier.\n",
        "        remaining_free_tier_chars = max(0, free_tier_limit - current_tier_usage)\n",
        "        # Determine how many characters of the current request fall into the free tier.\n",
        "        chars_in_free_tier_for_this_request = min(char_count, remaining_free_tier_chars)\n",
        "\n",
        "        # Calculate how many characters of the current request will be paid (billed).\n",
        "        paid_chars_in_this_request = char_count - chars_in_free_tier_for_this_request\n",
        "\n",
        "        # Initialize 'estimated_cost' to 0.0.\n",
        "        estimated_cost = 0.0\n",
        "        # Check if there are any characters to be paid for in this request.\n",
        "        if paid_chars_in_this_request > 0:\n",
        "            # Calculate the estimated cost based on the number of paid characters and cost per million.\n",
        "            estimated_cost = (paid_chars_in_this_request * cost_per_million_chars) / 1_000_000.0\n",
        "\n",
        "        # Log a debug message detailing the cost estimation parameters and result.\n",
        "        self.logger.debug(f\"Request {tts_request.request_id}: Cost estimation: \"\n",
        "                          f\"{char_count} chars, Tier: {actual_quality_tier.value}, \"\n",
        "                          f\"Current Tier Usage: {current_tier_usage}, Free Limit: {free_tier_limit}, \"\n",
        "                          f\"Paid Chars in this Req: {paid_chars_in_this_request}, Est. Cost: ${estimated_cost:.6f}\")\n",
        "        # Return the calculated 'estimated_cost'.\n",
        "        return estimated_cost\n",
        "\n",
        "    # Defines an internal method to generate a deterministic cache key for a TTSRequest using SHA-256.\n",
        "    def _generate_cache_key(self, tts_request: TTSRequest, final_voice_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Generates a deterministic cache key for a TTSRequest using SHA-256.\n",
        "        The key incorporates all parameters that affect the synthesized audio output,\n",
        "        including the final selected voice name. Unicode normalization is applied to text.\n",
        "\n",
        "        Args:\n",
        "            tts_request: The TTSRequest object.\n",
        "            final_voice_name: The actual voice name that will be used for synthesis.\n",
        "\n",
        "        Returns:\n",
        "            str: A SHA-256 hash string representing the cache key.\n",
        "        \"\"\"\n",
        "        # Initialize 'normalized_text' with the raw text from the request.\n",
        "        normalized_text: str\n",
        "        # Begin a try block to handle potential TypeErrors during Unicode normalization.\n",
        "        try:\n",
        "            # Normalize the Unicode text from 'tts_request.text' to NFKC form for canonical representation.\n",
        "            normalized_text = unicodedata.normalize('NFKC', tts_request.text)\n",
        "        # Catch TypeError, which might occur if 'tts_request.text' is not a string (though validated).\n",
        "        except TypeError as e:\n",
        "            # Log an error if Unicode normalization fails, and use the raw text as a fallback.\n",
        "            self.logger.error(f\"Error normalizing text for cache key (request {tts_request.request_id}): {e}. Using raw text.\")\n",
        "            # Assign the raw text to 'normalized_text'.\n",
        "            normalized_text = tts_request.text\n",
        "\n",
        "        # Construct a composite string from all relevant parameters that affect audio output for consistent hashing.\n",
        "        cache_data_string = (\n",
        "            f\"text:{normalized_text}|\"\n",
        "            f\"lang:{tts_request.language_code}|\"\n",
        "            f\"voice:{final_voice_name}|\"\n",
        "            f\"ssml:{tts_request.ssml_enabled}|\"\n",
        "            f\"rate:{tts_request.speaking_rate:.2f}|\"\n",
        "            f\"pitch:{tts_request.pitch:.2f}|\"\n",
        "            f\"encoding:{tts_request.audio_encoding.value}\"\n",
        "        )\n",
        "\n",
        "        # Encode the composite 'cache_data_string' to UTF-8 bytes before hashing.\n",
        "        encoded_data = cache_data_string.encode('utf-8')\n",
        "        # Generate a SHA-256 hash of the encoded data.\n",
        "        sha256_hash = hashlib.sha256(encoded_data)\n",
        "        # Return the hexadecimal representation of the SHA-256 hash digest.\n",
        "        return sha256_hash.hexdigest()\n",
        "\n",
        "    # Defines an internal method for intelligent voice selection based on request parameters and cost constraints.\n",
        "    def _intelligent_voice_selection(self, tts_request: TTSRequest) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Intelligently selects the most appropriate voice configuration for a given TTSRequest.\n",
        "        Considers requested voice name, quality preference, language, cost constraints,\n",
        "        and availability from the dynamically discovered voice mapping.\n",
        "\n",
        "        Args:\n",
        "            tts_request: The TTSRequest object.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary containing the selected voice configuration,\n",
        "                            including 'name', 'gender', 'quality_tier_enum', etc.\n",
        "\n",
        "        Raises:\n",
        "            VoiceSelectionError: If a suitable voice cannot be found or if the requested\n",
        "                                 voice is invalid or unavailable.\n",
        "        \"\"\"\n",
        "        # Log a debug message indicating the start of the intelligent voice selection process.\n",
        "        self.logger.debug(f\"Request {tts_request.request_id}: Starting intelligent voice selection for lang '{tts_request.language_code}', quality '{tts_request.quality_tier.value}'.\")\n",
        "\n",
        "        # Initialize 'selected_voice_config' to None.\n",
        "        selected_voice_config: Optional[Dict[str, Any]] = None\n",
        "\n",
        "        # --- Step 1: Handle explicitly requested voice_name ---\n",
        "        # Check if a specific 'voice_name' was provided in the 'tts_request'.\n",
        "        if tts_request.voice_name:\n",
        "            # Validate if the requested 'voice_name' is in the set of available voices.\n",
        "            if tts_request.voice_name not in self.available_voices:\n",
        "                # If the voice is not available, raise a VoiceSelectionError.\n",
        "                raise VoiceSelectionError(f\"Request {tts_request.request_id}: Requested voice '{tts_request.voice_name}' is not available or invalid.\")\n",
        "            # Iterate through the voice mapping tiers ('cost_optimized', 'premium') to find the requested voice.\n",
        "            for tier in [\"cost_optimized\", \"premium\"]:\n",
        "                # Iterate through language-to-voice_config mappings within the current tier.\n",
        "                for lang_map_voice_config in self.voice_mapping[tier].values(): # Corrected variable name\n",
        "                    # Check if the 'name' in the current 'lang_map_voice_config' matches the requested 'voice_name'.\n",
        "                    if lang_map_voice_config[\"name\"] == tts_request.voice_name:\n",
        "                        # If a match is found, assign it to 'selected_voice_config'.\n",
        "                        selected_voice_config = lang_map_voice_config\n",
        "                        # Break the inner loop as the voice has been found.\n",
        "                        break\n",
        "                # Check if 'selected_voice_config' has been found in the current tier.\n",
        "                if selected_voice_config:\n",
        "                    # Break the outer loop as the voice has been found.\n",
        "                    break\n",
        "            # This check is a safeguard; if voice_name is in available_voices, it should be found in voice_mapping.\n",
        "            if not selected_voice_config:\n",
        "                 # If configuration for an available voice is not found, raise a VoiceSelectionError indicating a mapping issue.\n",
        "                 raise VoiceSelectionError(f\"Request {tts_request.request_id}: Could not find configuration for available voice '{tts_request.voice_name}'. Mapping issue.\")\n",
        "\n",
        "            # --- Cost constraint check for explicitly requested premium voice ---\n",
        "            # Get a snapshot of the current cost metrics.\n",
        "            current_cost_snapshot = self.cost_metrics.get_current_metrics_snapshot()\n",
        "            # Get the current estimated monthly cost from the snapshot.\n",
        "            current_monthly_cost = current_cost_snapshot['estimated_monthly_cost']\n",
        "            # Check if the selected voice is premium and if the cost threshold is being approached or exceeded.\n",
        "            if selected_voice_config[\"quality_tier_enum\"] != VoiceQuality.STANDARD and \\\n",
        "               current_monthly_cost > self.cost_threshold_usd * self.COST_THRESHOLD_DOWNGRADE_FACTOR:\n",
        "                # Log a warning that a premium voice was requested but cost constraints suggest downgrading.\n",
        "                self.logger.warning(f\"Request {tts_request.request_id}: Requested premium voice '{tts_request.voice_name}' but cost threshold exceeded. \"\n",
        "                                    f\"Attempting to downgrade to standard voice for language '{tts_request.language_code}'.\")\n",
        "                # Do not return here; fall through to standard voice selection logic below.\n",
        "                # Reset selected_voice_config to None so that the logic below re-evaluates for a standard voice.\n",
        "                selected_voice_config = None\n",
        "            # If no cost constraint issue for the explicitly requested voice.\n",
        "            else:\n",
        "                # Log that the explicitly requested voice will be used.\n",
        "                self.logger.info(f\"Request {tts_request.request_id}: Using explicitly requested voice '{tts_request.voice_name}'.\")\n",
        "                # Return the configuration of the explicitly requested voice.\n",
        "                return selected_voice_config\n",
        "\n",
        "        # --- Step 2: Cost Threshold Check for default quality preference (if no explicit voice was chosen or if downgraded) ---\n",
        "        # Get the initially preferred quality tier from the request.\n",
        "        effective_quality_tier = tts_request.quality_tier\n",
        "        # Check if 'selected_voice_config' is still None (meaning no explicit valid voice was chosen or it was nullified by cost check).\n",
        "        if selected_voice_config is None: # This block executes if no explicit voice is used or if it was overridden by cost\n",
        "            # Get a snapshot of the current cost metrics if not already fetched.\n",
        "            # current_cost_snapshot = self.cost_metrics.get_current_metrics_snapshot() # Already fetched if explicit voice was premium\n",
        "            # current_monthly_cost = current_cost_snapshot['estimated_monthly_cost'] # Already fetched\n",
        "            # Re-fetch if not already available (e.g. if tts_request.voice_name was None)\n",
        "            if not tts_request.voice_name: # Only re-fetch if not done in the explicit voice section\n",
        "                current_cost_snapshot = self.cost_metrics.get_current_metrics_snapshot()\n",
        "                current_monthly_cost = current_cost_snapshot['estimated_monthly_cost']\n",
        "\n",
        "            # Check if the preferred quality is premium and if the cost threshold suggests downgrading.\n",
        "            if tts_request.quality_tier != VoiceQuality.STANDARD and \\\n",
        "               current_monthly_cost > self.cost_threshold_usd * self.COST_THRESHOLD_DOWNGRADE_FACTOR:\n",
        "                # Log a warning about switching to STANDARD quality due to cost constraints.\n",
        "                self.logger.warning(f\"Request {tts_request.request_id}: Approaching/exceeded cost threshold (${self.cost_threshold_usd:.2f}). \"\n",
        "                                    f\"Switching preferred quality from '{tts_request.quality_tier.value}' to '{VoiceQuality.STANDARD.value}' \"\n",
        "                                    f\"for language '{tts_request.language_code}'.\")\n",
        "                # Set the 'effective_quality_tier' to STANDARD.\n",
        "                effective_quality_tier = VoiceQuality.STANDARD\n",
        "\n",
        "        # --- Step 3: Select voice based on (potentially adjusted) quality tier and language ---\n",
        "        # Determine the target tier map name based on 'effective_quality_tier'.\n",
        "        target_tier_map_name = \"cost_optimized\" if effective_quality_tier == VoiceQuality.STANDARD else \"premium\"\n",
        "        # Get the actual dictionary for the target tier from 'self.voice_mapping'.\n",
        "        target_tier_map = self.voice_mapping[target_tier_map_name]\n",
        "        # Attempt to get the voice configuration using the 'tts_request.language_code' and the 'target_tier_map'.\n",
        "        selected_voice_config = self._get_voice_config_from_mapping(tts_request.language_code, target_tier_map)\n",
        "\n",
        "        # --- Step 4: Fallback logic if specific language/tier combination is not found ---\n",
        "        # Check if 'selected_voice_config' is still None after the initial attempt.\n",
        "        if not selected_voice_config:\n",
        "            # Log a warning that a voice was not found for the specific language and tier.\n",
        "            self.logger.warning(f\"Request {tts_request.request_id}: Voice not found for language '{tts_request.language_code}' in '{target_tier_map_name}' tier.\")\n",
        "            # --- Fallback a: If premium was desired, try standard for the same language. ---\n",
        "            # Check if the original target was 'premium'.\n",
        "            if target_tier_map_name == \"premium\":\n",
        "                # Log an attempt to fall back to the 'cost_optimized' tier for the same language.\n",
        "                self.logger.debug(f\"Request {tts_request.request_id}: Attempting fallback to 'cost_optimized' tier for language '{tts_request.language_code}'.\")\n",
        "                # Attempt to get voice config from the 'cost_optimized' tier.\n",
        "                selected_voice_config = self._get_voice_config_from_mapping(tts_request.language_code, self.voice_mapping[\"cost_optimized\"])\n",
        "                # If a voice is found in the fallback tier, update 'effective_quality_tier'.\n",
        "                if selected_voice_config:\n",
        "                    effective_quality_tier = VoiceQuality.STANDARD\n",
        "\n",
        "            # --- Fallback b: If still not found, try a default/generic language (e.g., 'en-US'). ---\n",
        "            # Check if 'selected_voice_config' is still None.\n",
        "            if not selected_voice_config:\n",
        "                # Define the default fallback language (e.g., 'en-US').\n",
        "                default_fallback_lang = \"en-US\"\n",
        "                # Log an attempt to fall back to the 'default_fallback_lang' in the 'target_tier_map_name' (original or standard if premium failed).\n",
        "                self.logger.debug(f\"Request {tts_request.request_id}: Attempting fallback to language '{default_fallback_lang}' in '{target_tier_map_name}' tier.\")\n",
        "                # Attempt to get voice config for the 'default_fallback_lang' in the 'target_tier_map'.\n",
        "                selected_voice_config = self._get_voice_config_from_mapping(default_fallback_lang, target_tier_map)\n",
        "                # If still not found and the target was premium, try 'default_fallback_lang' in 'cost_optimized' tier.\n",
        "                if not selected_voice_config and target_tier_map_name == \"premium\":\n",
        "                    # Log an attempt to fall back to 'default_fallback_lang' in the 'cost_optimized' tier.\n",
        "                    self.logger.debug(f\"Request {tts_request.request_id}: Attempting fallback to language '{default_fallback_lang}' in 'cost_optimized' tier.\")\n",
        "                    # Attempt to get voice config for 'default_fallback_lang' from 'cost_optimized' tier.\n",
        "                    selected_voice_config = self._get_voice_config_from_mapping(default_fallback_lang, self.voice_mapping[\"cost_optimized\"])\n",
        "                    # If a voice is found, update 'effective_quality_tier'.\n",
        "                    if selected_voice_config:\n",
        "                        effective_quality_tier = VoiceQuality.STANDARD\n",
        "\n",
        "        # --- Step 5: Final check and return ---\n",
        "        # Check if 'selected_voice_config' is still None after all selection and fallback attempts.\n",
        "        if not selected_voice_config:\n",
        "            # If no suitable voice could be found, raise a VoiceSelectionError.\n",
        "            raise VoiceSelectionError(f\"Request {tts_request.request_id}: Unable to find any suitable voice for language '{tts_request.language_code}' \"\n",
        "                                      f\"with quality preference '{tts_request.quality_tier.value}' after all fallbacks.\")\n",
        "\n",
        "        # Ensure the 'selected_voice_config' reflects the 'effective_quality_tier' if it was changed by fallback logic.\n",
        "        selected_voice_config[\"quality_tier_enum\"] = effective_quality_tier\n",
        "        # Log the details of the intelligently selected voice.\n",
        "        self.logger.info(f\"Request {tts_request.request_id}: Intelligently selected voice: '{selected_voice_config['name']}' \"\n",
        "                         f\"(Quality: {selected_voice_config['quality_tier_enum'].value}, Gender: {selected_voice_config['gender']}).\")\n",
        "        # Return the final 'selected_voice_config'.\n",
        "        return selected_voice_config\n",
        "\n",
        "    # Defines an asynchronous method to retrieve an item from the cache if it exists and is not expired.\n",
        "    async def _get_from_cache(self, cache_key: str) -> Optional[bytes]:\n",
        "        \"\"\"\n",
        "        Retrieves an item from the cache if it exists and is not expired.\n",
        "        Thread-safe.\n",
        "\n",
        "        Args:\n",
        "            cache_key: The key for the cached item.\n",
        "\n",
        "        Returns:\n",
        "            The cached audio content (bytes) if found and valid, else None.\n",
        "        \"\"\"\n",
        "        # Check if caching is disabled for the orchestrator.\n",
        "        if not self.enable_caching:\n",
        "            # If caching is disabled, return None immediately.\n",
        "            return None\n",
        "\n",
        "        # Acquire the cache lock to ensure thread-safe access to the cache storage and access order list.\n",
        "        with self._cache_lock:\n",
        "            # Attempt to retrieve the item associated with 'cache_key' from '_cache_storage'.\n",
        "            cached_item = self._cache_storage.get(cache_key)\n",
        "            # Check if the 'cached_item' was found.\n",
        "            if cached_item:\n",
        "                # Unpack the cached item into 'audio_content' and 'timestamp_utc'.\n",
        "                audio_content, timestamp_utc = cached_item\n",
        "                # Check if the cache item has expired by comparing its timestamp with the current time and TTL.\n",
        "                if datetime.now(timezone.utc) - timestamp_utc < self.cache_ttl:\n",
        "                    # Log a debug message indicating a cache hit.\n",
        "                    self.logger.debug(f\"Cache hit for key: {cache_key}\")\n",
        "                    # Attempt to remove the 'cache_key' from '_cache_access_order' if it exists (it should).\n",
        "                    if cache_key in self._cache_access_order:\n",
        "                        # Remove the key from its current position in the access order list.\n",
        "                        self._cache_access_order.remove(cache_key)\n",
        "                    # Append the 'cache_key' to the end of '_cache_access_order' to mark it as recently used (LRU).\n",
        "                    self._cache_access_order.append(cache_key)\n",
        "                    # Return the 'audio_content' from the cache.\n",
        "                    return audio_content\n",
        "                # Handle the case where the cache item has expired.\n",
        "                else:\n",
        "                    # Log a debug message indicating a stale cache item.\n",
        "                    self.logger.debug(f\"Cache stale for key: {cache_key}. Removing.\")\n",
        "                    # Delete the expired item from '_cache_storage'.\n",
        "                    del self._cache_storage[cache_key]\n",
        "                    # Attempt to remove the 'cache_key' from '_cache_access_order' if it exists.\n",
        "                    if cache_key in self._cache_access_order:\n",
        "                         # Remove the key from the access order list.\n",
        "                         self._cache_access_order.remove(cache_key)\n",
        "        # If the item was not found in cache or was expired and removed, return None.\n",
        "        return None\n",
        "\n",
        "    # Defines an asynchronous method to put an item into the cache with a creation timestamp.\n",
        "    async def _put_in_cache(self, cache_key: str, audio_content: bytes) -> None:\n",
        "        \"\"\"\n",
        "        Puts an item into the cache with a creation timestamp.\n",
        "        Handles cache eviction (simple LRU) if MAX_CACHE_SIZE is exceeded.\n",
        "        Thread-safe.\n",
        "\n",
        "        Args:\n",
        "            cache_key: The key for the item.\n",
        "            audio_content: The audio content (bytes) to cache.\n",
        "        \"\"\"\n",
        "        # Check if caching is disabled for the orchestrator.\n",
        "        if not self.enable_caching:\n",
        "            # If caching is disabled, return immediately without performing any cache operation.\n",
        "            return\n",
        "\n",
        "        # Acquire the cache lock to ensure thread-safe access to the cache storage and access order list.\n",
        "        with self._cache_lock:\n",
        "            # Check if the cache is full and the current 'cache_key' is not already in the cache (i.e., it's a new item).\n",
        "            if len(self._cache_storage) >= self.MAX_CACHE_SIZE and cache_key not in self._cache_storage:\n",
        "                # Check if there are items in the access order list to evict.\n",
        "                if self._cache_access_order:\n",
        "                    # Remove the least recently used key (at the beginning of the list).\n",
        "                    lru_key = self._cache_access_order.pop(0)\n",
        "                    # Check if the 'lru_key' is present in '_cache_storage' before attempting to delete.\n",
        "                    if lru_key in self._cache_storage:\n",
        "                        # Delete the LRU item from '_cache_storage'.\n",
        "                        del self._cache_storage[lru_key]\n",
        "                        # Log an informational message about the LRU eviction.\n",
        "                        self.logger.info(f\"Cache full. Evicted LRU item with key: {lru_key}\")\n",
        "\n",
        "            # Add or update the item in '_cache_storage' with the 'audio_content' and current UTC timestamp.\n",
        "            self._cache_storage[cache_key] = (audio_content, datetime.now(timezone.utc))\n",
        "            # Update the access order for LRU: remove if exists, then append to mark as recently used.\n",
        "            # Check if 'cache_key' is already in '_cache_access_order'.\n",
        "            if cache_key in self._cache_access_order:\n",
        "                # Remove the key from its current position.\n",
        "                self._cache_access_order.remove(cache_key)\n",
        "            # Append 'cache_key' to the end of the list.\n",
        "            self._cache_access_order.append(cache_key)\n",
        "            # Log a debug message indicating that the item has been cached.\n",
        "            self.logger.debug(f\"Cached item with key: {cache_key}\")\n",
        "\n",
        "    # Defines the core asynchronous method for performing Text-to-Speech synthesis for a single request.\n",
        "    async def synthesize_speech(self, tts_request: TTSRequest) -> Tuple[Optional[bytes], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Performs Text-to-Speech synthesis for a single TTSRequest.\n",
        "        This is the core method, incorporating caching, intelligent voice selection,\n",
        "        cost calculation, API call with reliability patterns (circuit breaker, rate limiter, retry),\n",
        "        and metrics updates. This method is truly asynchronous.\n",
        "\n",
        "        Args:\n",
        "            tts_request: The TTSRequest object to process.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - Optional[bytes]: The synthesized audio content as bytes if successful, else None.\n",
        "            - Dict[str, Any]: Metadata about the synthesis operation, including source (cache/API),\n",
        "                              voice used, cost, character count, and any errors.\n",
        "        \"\"\"\n",
        "        # Record the start time of the operation for performance measurement.\n",
        "        op_start_time = time.perf_counter()\n",
        "        # Log an informational message indicating the start of synthesis for the request.\n",
        "        self.logger.info(f\"Request {tts_request.request_id}: Starting synthesis for text (first 30 chars): '{tts_request.text[:30]}...'\")\n",
        "\n",
        "        # Initialize a dictionary to store metadata about the synthesis operation.\n",
        "        metadata: Dict[str, Any] = {\n",
        "            \"request_id\": tts_request.request_id,\n",
        "            \"source\": \"unknown\",\n",
        "            \"error\": None,\n",
        "            \"timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
        "        }\n",
        "\n",
        "        # Begin a try block to handle various exceptions that may occur during the synthesis process.\n",
        "        try:\n",
        "            # --- Step 1: Validate TTSRequest (already done in __post_init__, but can be a safeguard) ---\n",
        "            # (Validation is assumed to be handled by TTSRequest.__post_init__ upon object creation)\n",
        "\n",
        "            # --- Step 2: Intelligent Voice Selection ---\n",
        "            # Select the actual voice configuration (name, gender, quality tier) to be used.\n",
        "            selected_voice_config = self._intelligent_voice_selection(tts_request)\n",
        "            # Extract the final voice name from the selected configuration.\n",
        "            final_voice_name = selected_voice_config[\"name\"]\n",
        "            # Extract the actual VoiceQuality enum member to be used.\n",
        "            actual_quality_tier = selected_voice_config[\"quality_tier_enum\"]\n",
        "            # Extract the SSML gender from the selected configuration.\n",
        "            ssml_gender = selected_voice_config[\"gender\"]\n",
        "\n",
        "            # Update the 'metadata' dictionary with details of the selected voice.\n",
        "            metadata.update({\n",
        "                \"selected_voice_name\": final_voice_name,\n",
        "                \"selected_quality_tier\": actual_quality_tier.value,\n",
        "                \"selected_gender\": ssml_gender.name\n",
        "            })\n",
        "\n",
        "            # --- Step 3: Generate Cache Key ---\n",
        "            # Generate a cache key based on the final selected parameters and request details.\n",
        "            tts_request.cache_key = self._generate_cache_key(tts_request, final_voice_name)\n",
        "            # Add the generated cache key to the 'metadata'.\n",
        "            metadata[\"cache_key\"] = tts_request.cache_key\n",
        "\n",
        "            # --- Step 4: Check Cache ---\n",
        "            # Check if caching is enabled for the orchestrator.\n",
        "            if self.enable_caching:\n",
        "                # Attempt to retrieve audio content from the cache using the generated cache key.\n",
        "                cached_audio = await self._get_from_cache(tts_request.cache_key)\n",
        "                # Check if audio content was found in the cache.\n",
        "                if cached_audio:\n",
        "                    # Update cache metrics to record a cache hit.\n",
        "                    self.cost_metrics.update_cache_metrics(cache_hit=True)\n",
        "                    # Set the 'source' in metadata to \"cache\".\n",
        "                    metadata[\"source\"] = \"cache\"\n",
        "                    # Calculate the duration of the operation (cache retrieval).\n",
        "                    op_duration_ms = (time.perf_counter() - op_start_time) * 1000\n",
        "                    # Log a message indicating a cache hit and the operation duration.\n",
        "                    self.logger.info(f\"Request {tts_request.request_id}: Cache hit. Duration: {op_duration_ms:.2f}ms.\")\n",
        "                    # Return the cached audio content and metadata.\n",
        "                    return cached_audio, metadata\n",
        "            # If caching is disabled or if it's a cache miss:\n",
        "            # Update cache metrics to record a cache miss.\n",
        "            self.cost_metrics.update_cache_metrics(cache_hit=False)\n",
        "            # Set the 'source' in metadata to \"api_synthesized\".\n",
        "            metadata[\"source\"] = \"api_synthesized\"\n",
        "\n",
        "            # --- Step 5: Calculate Estimated Cost ---\n",
        "            # Calculate the estimated cost for the synthesis, now that the actual voice/quality is known.\n",
        "            tts_request.estimated_cost = self._calculate_estimated_cost(tts_request, actual_quality_tier)\n",
        "            # Add the estimated cost to the 'metadata'.\n",
        "            metadata[\"estimated_cost_usd\"] = tts_request.estimated_cost\n",
        "\n",
        "            # --- Step 6: Prepare for API Call (acquire semaphore, rate limit) ---\n",
        "            # Asynchronously acquire the semaphore to limit concurrency for I/O-bound API calls.\n",
        "            async with self._async_semaphore:\n",
        "                # Wait according to the adaptive rate limiter before making the API call.\n",
        "                await self.rate_limiter.acquire()\n",
        "\n",
        "                # Construct the SynthesisInput arguments based on whether SSML is enabled.\n",
        "                synthesis_input_args = {}\n",
        "                # Check if SSML is enabled for the request.\n",
        "                if tts_request.ssml_enabled:\n",
        "                    # If SSML is enabled, set the 'ssml' field in arguments.\n",
        "                    synthesis_input_args[\"ssml\"] = tts_request.text\n",
        "                # Handle plain text input.\n",
        "                else:\n",
        "                    # If SSML is not enabled, set the 'text' field in arguments.\n",
        "                    synthesis_input_args[\"text\"] = tts_request.text\n",
        "                # Create the SynthesisInput object using the prepared arguments.\n",
        "                synthesis_input = texttospeech.SynthesisInput(**synthesis_input_args)\n",
        "\n",
        "                # Construct the VoiceSelectionParams object.\n",
        "                voice_params = texttospeech.VoiceSelectionParams(\n",
        "                    language_code=tts_request.language_code,\n",
        "                    name=final_voice_name,\n",
        "                    ssml_gender=ssml_gender\n",
        "                )\n",
        "\n",
        "                # Construct the AudioConfig object.\n",
        "                audio_config = texttospeech.AudioConfig(\n",
        "                    audio_encoding=tts_request.audio_encoding,\n",
        "                    speaking_rate=tts_request.speaking_rate,\n",
        "                    pitch=tts_request.pitch,\n",
        "                )\n",
        "\n",
        "                # Define the actual API call function using functools.partial to pre-fill arguments.\n",
        "                # This creates a callable for self.tts_client.synthesize_speech with fixed input, voice, and audio_config.\n",
        "                api_call_func = functools.partial(\n",
        "                    self.tts_client.synthesize_speech, # type: ignore # google-cloud-texttospeech client methods are not perfectly typed for partial\n",
        "                    input=synthesis_input,\n",
        "                    voice=voice_params,\n",
        "                    audio_config=audio_config\n",
        "                )\n",
        "\n",
        "                # --- Step 7: Execute API Call (with Circuit Breaker and client-side retry) ---\n",
        "                # Log a debug message indicating that an API call is about to be made.\n",
        "                self.logger.debug(f\"Request {tts_request.request_id}: Making API call to tts_client.synthesize_speech.\")\n",
        "                # Initialize 'response' to None.\n",
        "                response: Optional[texttospeech.SynthesizeSpeechResponse] = None\n",
        "\n",
        "                # Check if the circuit breaker is enabled.\n",
        "                if self.circuit_breaker:\n",
        "                    # Execute the API call via the circuit breaker, running the (potentially blocking) Google client call in a separate thread.\n",
        "                    response = await self.circuit_breaker.execute_async(\n",
        "                        # Lambda function to execute the 'api_call_func' in a thread pool executor.\n",
        "                        lambda: asyncio.to_thread(api_call_func)\n",
        "                    )\n",
        "                # Handle direct execution if the circuit breaker is not enabled.\n",
        "                else:\n",
        "                    # Execute the (potentially blocking) Google client call directly in a separate thread.\n",
        "                    response = await asyncio.to_thread(api_call_func)\n",
        "\n",
        "                # Check if the API response is valid and contains audio content.\n",
        "                if not response or not response.audio_content:\n",
        "                    # If the response is invalid or lacks audio content, raise a SynthesisFailedError.\n",
        "                    raise SynthesisFailedError(f\"Request {tts_request.request_id}: API call returned empty response or no audio content.\")\n",
        "\n",
        "                # If the API call was successful, record success with the rate limiter.\n",
        "                await self.rate_limiter.record_success()\n",
        "                # Extract the audio content from the API response.\n",
        "                audio_content = response.audio_content\n",
        "                # Get the billable character count for the request and add it to metadata.\n",
        "                metadata[\"character_count\"] = tts_request.get_billable_character_count()\n",
        "\n",
        "                # --- Step 8: Update Cost Metrics (after successful synthesis) ---\n",
        "                # Calculate the duration of the synthesis operation.\n",
        "                op_duration_ms = (time.perf_counter() - op_start_time) * 1000\n",
        "                # Update cost metrics with details of the successful synthesis.\n",
        "                self.cost_metrics.update_on_synthesis_success(\n",
        "                    character_count=metadata[\"character_count\"],\n",
        "                    quality_tier=actual_quality_tier,\n",
        "                    cost=tts_request.estimated_cost,\n",
        "                    response_time_ms=op_duration_ms\n",
        "                )\n",
        "\n",
        "                # --- Step 9: Cache the Result (if caching enabled) ---\n",
        "                # Check if caching is enabled for the orchestrator.\n",
        "                if self.enable_caching:\n",
        "                    # Put the synthesized audio content into the cache.\n",
        "                    await self._put_in_cache(tts_request.cache_key, audio_content)\n",
        "\n",
        "                # Log a message indicating successful synthesis and relevant details.\n",
        "                self.logger.info(f\"Request {tts_request.request_id}: Synthesis successful. Duration: {op_duration_ms:.2f}ms. \"\n",
        "                                 f\"Chars: {metadata['character_count']}, Cost: ${tts_request.estimated_cost:.6f}.\")\n",
        "                # Return the synthesized audio content and metadata.\n",
        "                return audio_content, metadata\n",
        "\n",
        "        # Catch ValidationError if input validation for TTSRequest fails.\n",
        "        except ValidationError as e:\n",
        "            # Log the validation error.\n",
        "            self.logger.error(f\"Request {tts_request.request_id}: Validation error - {e}\", exc_info=True)\n",
        "            # Set the error message in metadata.\n",
        "            metadata[\"error\"] = f\"Validation Error: {e}\"\n",
        "            # Record a synthesis failure in cost metrics.\n",
        "            self.cost_metrics.record_synthesis_failure()\n",
        "            # Return None for audio content and the updated metadata.\n",
        "            return None, metadata\n",
        "        # Catch VoiceSelectionError if intelligent voice selection fails.\n",
        "        except VoiceSelectionError as e:\n",
        "            # Log the voice selection error.\n",
        "            self.logger.error(f\"Request {tts_request.request_id}: Voice selection error - {e}\", exc_info=True)\n",
        "            # Set the error message in metadata.\n",
        "            metadata[\"error\"] = f\"Voice Selection Error: {e}\"\n",
        "            # Record a synthesis failure in cost metrics.\n",
        "            self.cost_metrics.record_synthesis_failure()\n",
        "            # Return None for audio content and the updated metadata.\n",
        "            return None, metadata\n",
        "        # Catch APICallError, which can be raised by the circuit breaker or direct API call issues.\n",
        "        except APICallError as e:\n",
        "            # Log the API call error.\n",
        "            self.logger.error(f\"Request {tts_request.request_id}: API call error during synthesis - {e}\", exc_info=True)\n",
        "            # Set the error message in metadata.\n",
        "            metadata[\"error\"] = f\"API Call Error: {e}\"\n",
        "            # Check if the underlying cause of the APICallError is a Google API ResourceExhausted error (rate limiting).\n",
        "            is_rate_limit_err = isinstance(e.__cause__, gapi_exceptions.ResourceExhausted)\n",
        "            # Record failure with the rate limiter, indicating if it was a rate limit error.\n",
        "            await self.rate_limiter.record_failure(is_rate_limit_error=is_rate_limit_err)\n",
        "            # Record a synthesis failure in cost metrics.\n",
        "            self.cost_metrics.record_synthesis_failure()\n",
        "            # Return None for audio content and the updated metadata.\n",
        "            return None, metadata\n",
        "        # Catch GoogleCloudError for specific errors from Google Cloud client libraries.\n",
        "        except GoogleCloudError as e:\n",
        "            # Log the Google Cloud error.\n",
        "            self.logger.error(f\"Request {tts_request.request_id}: Google Cloud error during synthesis - {e}\", exc_info=True)\n",
        "            # Set the error message in metadata.\n",
        "            metadata[\"error\"] = f\"Google Cloud Error: {e}\"\n",
        "            # Check if the GoogleCloudError is a ResourceExhausted error (rate limiting).\n",
        "            is_rate_limit_err = isinstance(e, gapi_exceptions.ResourceExhausted)\n",
        "            # Record failure with the rate limiter, indicating if it was a rate limit error.\n",
        "            await self.rate_limiter.record_failure(is_rate_limit_error=is_rate_limit_err)\n",
        "            # Record a synthesis failure in cost metrics.\n",
        "            self.cost_metrics.record_synthesis_failure()\n",
        "            # Return None for audio content and the updated metadata.\n",
        "            return None, metadata\n",
        "        # Catch any other unexpected exceptions.\n",
        "        except Exception as e:\n",
        "            # Log the unexpected critical error.\n",
        "            self.logger.critical(f\"Request {tts_request.request_id}: Unexpected critical error during synthesis - {e}\", exc_info=True)\n",
        "            # Set the error message in metadata.\n",
        "            metadata[\"error\"] = f\"Unexpected Critical Error: {e}\"\n",
        "            # Record failure with the rate limiter, assuming a general failure.\n",
        "            await self.rate_limiter.record_failure()\n",
        "            # Record a synthesis failure in cost metrics.\n",
        "            self.cost_metrics.record_synthesis_failure()\n",
        "            # Return None for audio content and the updated metadata.\n",
        "            return None, metadata\n",
        "\n",
        "    # Defines a method to retrieve a comprehensive analysis of current TTS usage costs and recommendations.\n",
        "    def get_cost_analysis(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns a comprehensive analysis of current TTS usage costs, free tier status,\n",
        "        and actionable optimization recommendations.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary containing detailed cost and usage metrics,\n",
        "                            remaining free tier allocations, and optimization advice.\n",
        "        \"\"\"\n",
        "        # Log an informational message indicating the start of cost analysis report generation.\n",
        "        self.logger.info(\"Generating cost analysis report...\")\n",
        "        # Get a thread-safe snapshot of the current cost metrics.\n",
        "        current_metrics = self.cost_metrics.get_current_metrics_snapshot()\n",
        "\n",
        "        # Get the free tier character limit for STANDARD voice quality.\n",
        "        standard_free_limit = VoiceQuality.STANDARD.get_free_tier_character_limit()\n",
        "        # Get the free tier character limit for premium voice quality (using NEURAL2 as representative).\n",
        "        premium_free_limit = VoiceQuality.NEURAL2.get_free_tier_character_limit()\n",
        "\n",
        "        # Calculate the remaining free tier characters for standard voices.\n",
        "        remaining_standard_free_chars = max(0, standard_free_limit - current_metrics['standard_voice_usage'])\n",
        "        # Calculate the remaining free tier characters for premium voices.\n",
        "        remaining_premium_free_chars = max(0, premium_free_limit - current_metrics['premium_voice_usage'])\n",
        "\n",
        "        # Generate optimization recommendations based on the current metrics snapshot.\n",
        "        recommendations = self._generate_optimization_recommendations(current_metrics)\n",
        "\n",
        "        # Construct the comprehensive cost analysis report dictionary.\n",
        "        analysis_report = {\n",
        "            \"report_generated_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "            \"metrics_last_reset_utc\": current_metrics['last_reset_utc'],\n",
        "            \"current_usage_metrics\": {\n",
        "                \"total_characters_processed\": current_metrics['characters_processed'],\n",
        "                \"standard_voice_characters\": current_metrics['standard_voice_usage'],\n",
        "                \"premium_voice_characters\": current_metrics['premium_voice_usage'],\n",
        "                \"successful_requests\": current_metrics['successful_requests'],\n",
        "                \"failed_requests\": current_metrics['failed_requests'],\n",
        "            },\n",
        "            \"free_tier_status\": {\n",
        "                \"standard_voice_free_tier_limit\": standard_free_limit,\n",
        "                \"standard_voice_remaining_free_chars\": remaining_standard_free_chars,\n",
        "                \"premium_voice_free_tier_limit\": premium_free_limit,\n",
        "                \"premium_voice_remaining_free_chars\": remaining_premium_free_chars,\n",
        "            },\n",
        "            \"cost_information\": {\n",
        "                \"estimated_monthly_cost_usd\": current_metrics['estimated_monthly_cost'],\n",
        "                \"defined_cost_threshold_usd\": self.cost_threshold_usd,\n",
        "            },\n",
        "            \"performance_metrics\": {\n",
        "                \"cache_hit_rate\": f\"{current_metrics['cache_hit_rate']:.2%}\",\n",
        "                \"cache_hits\": current_metrics['cache_hits'],\n",
        "                \"cache_misses\": current_metrics['cache_misses'],\n",
        "                \"avg_api_response_time_ms\": f\"{current_metrics['avg_response_time_ms']:.2f}\",\n",
        "            },\n",
        "            \"optimization_recommendations\": recommendations\n",
        "        }\n",
        "        # Log an informational message indicating successful generation of the cost analysis report.\n",
        "        self.logger.info(\"Cost analysis report generated successfully.\")\n",
        "        # Return the constructed 'analysis_report'.\n",
        "        return analysis_report\n",
        "\n",
        "    # Defines an internal helper method to generate cost optimization recommendations based on current metrics.\n",
        "    def _generate_optimization_recommendations(self, current_metrics: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generates a list of cost optimization recommendations based on current usage metrics.\n",
        "        This is a helper method for `get_cost_analysis`.\n",
        "\n",
        "        Args:\n",
        "            current_metrics: A snapshot dictionary of current cost metrics.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: A list of string recommendations.\n",
        "        \"\"\"\n",
        "        # Log a debug message indicating the start of optimization recommendation generation.\n",
        "        self.logger.debug(\"Generating optimization recommendations based on current metrics...\")\n",
        "        # Initialize an empty list to store the generated recommendations.\n",
        "        recommendations: List[str] = []\n",
        "        # Define a warning factor for approaching the cost threshold (e.g., 90%).\n",
        "        cost_threshold_factor_warn = 0.9\n",
        "        # Define a warning factor for approaching free tier usage limits (e.g., 80%).\n",
        "        free_tier_usage_factor_warn = 0.8\n",
        "\n",
        "        # Get the free tier character limit for premium voice quality.\n",
        "        premium_free_limit = VoiceQuality.NEURAL2.get_free_tier_character_limit()\n",
        "        # Check if premium voice usage is approaching or has exceeded its free tier limit.\n",
        "        if current_metrics['premium_voice_usage'] > premium_free_limit * free_tier_usage_factor_warn:\n",
        "            # Construct a recommendation message for premium voice usage.\n",
        "            recommendation_msg = (\n",
        "                f\"Premium voice usage ({current_metrics['premium_voice_usage']:,} chars) is \"\n",
        "                f\"approaching/exceeded its monthly free tier limit ({premium_free_limit:,} chars). \"\n",
        "                \"Consider shifting non-critical requests to Standard voices to optimize costs.\"\n",
        "            )\n",
        "            # Add the recommendation message to the list.\n",
        "            recommendations.append(recommendation_msg)\n",
        "\n",
        "        # Get the free tier character limit for standard voice quality.\n",
        "        standard_free_limit = VoiceQuality.STANDARD.get_free_tier_character_limit()\n",
        "        # Check if standard voice usage is approaching the end of its free tier.\n",
        "        if standard_free_limit * free_tier_usage_factor_warn < current_metrics['standard_voice_usage'] <= standard_free_limit :\n",
        "             # Construct a recommendation message for standard voice usage.\n",
        "            recommendation_msg = (\n",
        "                f\"Standard voice usage ({current_metrics['standard_voice_usage']:,} chars) is \"\n",
        "                f\"approaching the end of its free tier ({standard_free_limit:,} chars). Monitor closely to anticipate costs.\"\n",
        "            )\n",
        "             # Add the recommendation message to the list.\n",
        "            recommendations.append(recommendation_msg)\n",
        "\n",
        "        # Check if the estimated monthly cost is approaching or has exceeded the defined cost threshold.\n",
        "        if current_metrics['estimated_monthly_cost'] > self.cost_threshold_usd * cost_threshold_factor_warn:\n",
        "            # Construct a recommendation message regarding the cost threshold.\n",
        "            recommendation_msg = (\n",
        "                f\"Estimated monthly cost (${current_metrics['estimated_monthly_cost']:.2f}) is \"\n",
        "                f\"approaching/exceeded the defined threshold (${self.cost_threshold_usd:.2f}). \"\n",
        "                \"Review usage patterns and consider stricter caching, batching, or quality tier adjustments.\"\n",
        "            )\n",
        "            # Add the recommendation message to the list.\n",
        "            recommendations.append(recommendation_msg)\n",
        "\n",
        "        # Calculate the total number of cache lookups.\n",
        "        total_cache_lookups = current_metrics['cache_hits'] + current_metrics['cache_misses']\n",
        "        # Check if caching is enabled and if there's enough data for a meaningful cache hit rate analysis.\n",
        "        if self.enable_caching and total_cache_lookups > 20:\n",
        "            # Check if the cache hit rate is below a certain threshold (e.g., 20%).\n",
        "            if current_metrics['cache_hit_rate'] < 0.20:\n",
        "                # Construct a recommendation message for low cache hit rate.\n",
        "                recommendation_msg = (\n",
        "                    f\"Cache hit rate is low ({current_metrics['cache_hit_rate']:.2%}). \"\n",
        "                    \"Review request patterns for frequently repeated synthesis tasks. \"\n",
        "                    \"Ensure cache TTL is appropriate for your use case.\"\n",
        "                )\n",
        "                # Add the recommendation message to the list.\n",
        "                recommendations.append(recommendation_msg)\n",
        "        # Else, if caching is disabled but there has been significant successful request activity.\n",
        "        elif not self.enable_caching and current_metrics['successful_requests'] > 50 :\n",
        "             # Construct a recommendation message suggesting enabling caching.\n",
        "             recommendation_msg = (\n",
        "                \"Caching is currently disabled. Enabling caching could significantly reduce costs \"\n",
        "                \"and improve response times for repeated synthesis requests.\"\n",
        "            )\n",
        "             # Add the recommendation message to the list.\n",
        "            recommendations.append(recommendation_msg)\n",
        "\n",
        "        # Calculate the total number of API attempts (successful + failed requests).\n",
        "        total_api_attempts = current_metrics['successful_requests'] + current_metrics['failed_requests']\n",
        "        # Check if there's enough data and if there have been any failed requests.\n",
        "        if total_api_attempts > 20 and current_metrics['failed_requests'] > 0:\n",
        "            # Calculate the API request failure rate.\n",
        "            failure_rate = current_metrics['failed_requests'] / total_api_attempts\n",
        "            # Check if the failure rate is above a certain threshold (e.g., 10%).\n",
        "            if failure_rate > 0.10:\n",
        "                # Construct a recommendation message for high API failure rate.\n",
        "                recommendation_msg = (\n",
        "                    f\"High API request failure rate detected ({failure_rate:.2%}). \"\n",
        "                    \"Investigate logs for common errors (e.g., invalid input, API issues, network problems, quota limits).\"\n",
        "                )\n",
        "                # Add the recommendation message to the list.\n",
        "                recommendations.append(recommendation_msg)\n",
        "\n",
        "        # Define a threshold for considering average API response time as high (e.g., 1500 ms).\n",
        "        avg_response_time_threshold_ms = 1500\n",
        "        # Check if average response time is high and there's enough data from successful requests.\n",
        "        if current_metrics['avg_response_time_ms'] > avg_response_time_threshold_ms and current_metrics['successful_requests'] > 10 :\n",
        "            # Construct a recommendation message for high average API response time.\n",
        "            recommendation_msg = (\n",
        "                f\"Average API response time is high ({current_metrics['avg_response_time_ms']:.2f}ms). \"\n",
        "                \"Consider optimizing request text length, checking network latency, or ensuring \"\n",
        "                \"sufficient concurrency settings if using batch processing.\"\n",
        "            )\n",
        "            # Add the recommendation message to the list.\n",
        "            recommendations.append(recommendation_msg)\n",
        "\n",
        "        # Check if no specific recommendations were generated.\n",
        "        if not recommendations:\n",
        "            # Add a default message indicating that usage appears to be within parameters.\n",
        "            recommendations.append(\"Current TTS usage appears to be within defined operational parameters. No immediate optimization actions flagged.\")\n",
        "\n",
        "        # Log a debug message indicating the number of generated optimization recommendations.\n",
        "        self.logger.debug(f\"Generated {len(recommendations)} optimization recommendations.\")\n",
        "        # Return the list of recommendation strings.\n",
        "        return recommendations\n",
        "\n",
        "    # Defines an asynchronous method to process a batch of TTSRequest objects concurrently.\n",
        "    async def batch_synthesize(self, requests_list: List[TTSRequest]) -> List[Tuple[Optional[bytes], Dict[str, Any]]]:\n",
        "        \"\"\"\n",
        "        Processes a batch of TTSRequest objects concurrently with cost optimization,\n",
        "        adaptive rate limiting, and circuit breaker patterns applied to each individual request.\n",
        "\n",
        "        Args:\n",
        "            requests_list: A list of TTSRequest objects to be synthesized.\n",
        "\n",
        "        Returns:\n",
        "            A list of tuples, where each tuple corresponds to an input TTSRequest and contains:\n",
        "            - Optional[bytes]: Synthesized audio content (bytes) if successful, else None.\n",
        "            - Dict[str, Any]: Metadata about the synthesis operation for that request.\n",
        "            The order of results in the list matches the order of requests in the input list.\n",
        "        \"\"\"\n",
        "        # Check if the input 'requests_list' is empty.\n",
        "        if not requests_list:\n",
        "            # Log an informational message if an empty list is provided.\n",
        "            self.logger.info(\"Batch synthesize called with an empty list of requests.\")\n",
        "            # Return an empty list as there are no requests to process.\n",
        "            return []\n",
        "\n",
        "        # Log an informational message indicating the start of batch synthesis and the number of requests.\n",
        "        self.logger.info(f\"Starting batch synthesis for {len(requests_list)} requests.\")\n",
        "\n",
        "        # Define a helper function 'sort_key' to determine the sorting order for requests.\n",
        "        # Requests are sorted by priority (ascending), then by preliminary estimated cost (ascending).\n",
        "        def sort_key(req: TTSRequest):\n",
        "            # Get the preferred quality tier from the request for preliminary cost estimation.\n",
        "            prelim_quality = req.quality_tier\n",
        "            # Get the billable character count from the request.\n",
        "            prelim_char_count = req.get_billable_character_count()\n",
        "            # Calculate a preliminary estimated cost for sorting purposes.\n",
        "            prelim_cost = (prelim_char_count * prelim_quality.get_cost_per_million_characters()) / 1_000_000.0\n",
        "            # Return a tuple of (priority, preliminary_cost) for sorting.\n",
        "            return (req.priority, prelim_cost)\n",
        "\n",
        "        # Initialize 'sorted_requests' with the original 'requests_list'.\n",
        "        sorted_requests: List[TTSRequest]\n",
        "        # Begin a try block to handle potential exceptions during request sorting.\n",
        "        try:\n",
        "            # Sort the 'requests_list' using the defined 'sort_key'.\n",
        "            sorted_requests = sorted(requests_list, key=sort_key)\n",
        "        # Catch any exception that occurs during sorting.\n",
        "        except Exception as e:\n",
        "            # Log an error message if sorting fails, and process requests in their original order.\n",
        "            self.logger.error(f\"Error during sorting of batch requests: {e}. Processing in original order.\", exc_info=True)\n",
        "            # Assign the original 'requests_list' to 'sorted_requests' as a fallback.\n",
        "            sorted_requests = requests_list\n",
        "\n",
        "        # Create a list of asyncio tasks, where each task is a call to 'self.synthesize_speech' for a request.\n",
        "        tasks = [self.synthesize_speech(req) for req in sorted_requests]\n",
        "\n",
        "        # Execute all tasks concurrently using asyncio.gather.\n",
        "        # 'return_exceptions=True' ensures that exceptions in tasks are returned as results rather than stopping 'gather'.\n",
        "        self.logger.info(f\"Executing {len(tasks)} synthesis tasks concurrently for batch.\")\n",
        "        # Await the completion of all tasks and get their results (or exceptions).\n",
        "        results_from_gather = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "        # Initialize an empty list to store the final results in the processed order.\n",
        "        final_results: List[Tuple[Optional[bytes], Dict[str, Any]]] = []\n",
        "        # Iterate through the results obtained from 'asyncio.gather', along with their original index.\n",
        "        for i, result_item in enumerate(results_from_gather):\n",
        "            # Get the request ID from the corresponding request in the 'sorted_requests' list for correlation.\n",
        "            original_request_id = sorted_requests[i].request_id\n",
        "            # Check if the 'result_item' is an instance of Exception (meaning the task failed).\n",
        "            if isinstance(result_item, Exception):\n",
        "                # Log an error message for the failed batch request, including the exception.\n",
        "                self.logger.error(f\"Batch request {original_request_id} (index {i}) failed with exception: {result_item}\", exc_info=result_item)\n",
        "                # Create an error metadata dictionary for the failed request.\n",
        "                error_metadata = {\n",
        "                    \"request_id\": original_request_id,\n",
        "                    \"error\": f\"Unhandled Batch Exception: {type(result_item).__name__} - {str(result_item)}\",\n",
        "                    \"source\": \"batch_processor_error\",\n",
        "                    \"timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
        "                }\n",
        "                # Append a tuple (None for audio, error_metadata) to 'final_results'.\n",
        "                final_results.append((None, error_metadata))\n",
        "            # Check if the 'result_item' is a tuple of length 2 (expected format for successful/handled synthesis).\n",
        "            elif isinstance(result_item, tuple) and len(result_item) == 2:\n",
        "                # Unpack the tuple into 'audio_data' and 'metadata'.\n",
        "                audio_data, metadata = result_item # type: ignore # mypy struggles with heterogeneous list from gather\n",
        "                # Ensure 'metadata' always has 'request_id' for correlation, using 'original_request_id' as fallback.\n",
        "                metadata[\"request_id\"] = metadata.get(\"request_id\", original_request_id)\n",
        "                # Append the 'audio_data' and 'metadata' to 'final_results'.\n",
        "                final_results.append((audio_data, metadata))\n",
        "            # Handle cases where the result format from 'asyncio.gather' is unexpected.\n",
        "            else:\n",
        "                 # Log an error message for the unexpected result format.\n",
        "                 self.logger.error(f\"Batch request {original_request_id} (index {i}) returned unexpected result format: {result_item}\")\n",
        "                 # Create an error metadata dictionary for the unexpected result.\n",
        "                 unexpected_error_metadata = {\n",
        "                    \"request_id\": original_request_id,\n",
        "                    \"error\": f\"Unexpected result format from synthesis task: {type(result_item).__name__}\",\n",
        "                    \"source\": \"batch_processor_unexpected_result\",\n",
        "                    \"timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
        "                }\n",
        "                 # Append a tuple (None for audio, unexpected_error_metadata) to 'final_results'.\n",
        "                 final_results.append((None, unexpected_error_metadata))\n",
        "\n",
        "        # Log an informational message indicating the completion of batch synthesis.\n",
        "        self.logger.info(f\"Batch synthesis completed for {len(requests_list)} requests.\")\n",
        "        # Return the 'final_results' list, which contains results in the order of 'sorted_requests'.\n",
        "        return final_results\n"
      ],
      "metadata": {
        "id": "9Jr8ZgF5iTTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines an asynchronous function to demonstrate production-like usage of the VertexTTSOrchestrator.\n",
        "\n",
        "async def main_usage_example():\n",
        "    \"\"\"\n",
        "    Demonstrates production-like usage of the VertexTTSOrchestrator,\n",
        "    showcasing single synthesis, batch synthesis, error handling, and cost analysis.\n",
        "    Configuration is partially driven by environment variables.\n",
        "    \"\"\"\n",
        "    # Obtain a logger instance specific to this \"MainUsageExample\".\n",
        "    main_logger = logging.getLogger(\"MainUsageExample\")\n",
        "    # Log an informational message indicating the start of the usage example.\n",
        "    main_logger.info(\"Starting VertexTTSOrchestrator usage example...\")\n",
        "\n",
        "    # Retrieve the Google Cloud Project ID from the \"GOOGLE_CLOUD_PROJECT\" environment variable.\n",
        "    project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
        "    # Check if the 'project_id' was successfully retrieved.\n",
        "    if not project_id:\n",
        "        # Log an error message if the environment variable is not set.\n",
        "        main_logger.error(\"GOOGLE_CLOUD_PROJECT environment variable not set. Cannot proceed.\")\n",
        "        # Print an error message to the console.\n",
        "        print(\"Error: GOOGLE_CLOUD_PROJECT environment variable is required.\")\n",
        "        # Return from the function as the required configuration is missing.\n",
        "        return\n",
        "\n",
        "    # Retrieve the Google Cloud Region from the \"GOOGLE_CLOUD_REGION\" environment variable,\n",
        "    # defaulting to 'VertexTTSOrchestrator.DEFAULT_REGION' if not set.\n",
        "    region = os.getenv(\"GOOGLE_CLOUD_REGION\", VertexTTSOrchestrator.DEFAULT_REGION)\n",
        "    # Retrieve the path to Google Application Credentials from the \"GOOGLE_APPLICATION_CREDENTIALS\" environment variable.\n",
        "    credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\") # This will be None if not set.\n",
        "\n",
        "    # Log the Project ID and Region being used.\n",
        "    main_logger.info(f\"Using Project ID: {project_id}, Region: {region}\")\n",
        "    # Check if a 'credentials_path' was provided.\n",
        "    if credentials_path:\n",
        "        # Log the path to the credentials file if it's being used.\n",
        "        main_logger.info(f\"Using Credentials Path: {credentials_path}\")\n",
        "    # Handle the case where no specific credentials path is provided.\n",
        "    else:\n",
        "        # Log that Application Default Credentials will be used.\n",
        "        main_logger.info(\"Using Application Default Credentials.\")\n",
        "\n",
        "    # Initialize the 'orchestrator' variable to None. It will hold the VertexTTSOrchestrator instance.\n",
        "    orchestrator: Optional[VertexTTSOrchestrator] = None\n",
        "    # Begin a try block to handle potential errors during orchestrator initialization.\n",
        "    try:\n",
        "        # Instantiate the VertexTTSOrchestrator with specified and default configurations.\n",
        "        orchestrator = VertexTTSOrchestrator(\n",
        "            project_id=project_id,\n",
        "            region=region,\n",
        "            enable_caching=True,\n",
        "            cost_threshold_usd=50.0,\n",
        "            max_concurrent_requests=5,\n",
        "            google_application_credentials_path=credentials_path\n",
        "        )\n",
        "    # Catch ConfigurationError or InitializationError specifically if they occur during instantiation.\n",
        "    except (ConfigurationError, InitializationError) as e:\n",
        "        # Log a critical error message if orchestrator initialization fails.\n",
        "        main_logger.critical(f\"Failed to initialize TTS Orchestrator: {e}\", exc_info=True)\n",
        "        # Print a critical error message to the console.\n",
        "        print(f\"Critical Error: Could not initialize TTS Orchestrator. {e}\")\n",
        "        # Return from the function as the orchestrator could not be initialized.\n",
        "        return\n",
        "    # Catch any other unexpected Exception during orchestrator initialization.\n",
        "    except Exception as e:\n",
        "        # Log a critical error message for any unexpected initialization error.\n",
        "        main_logger.critical(f\"Unexpected critical error during orchestrator initialization: {e}\", exc_info=True)\n",
        "        # Print an unexpected critical error message to the console.\n",
        "        print(f\"Unexpected Critical Error during initialization. {e}\")\n",
        "        # Return from the function.\n",
        "        return\n",
        "\n",
        "    # --- Example 1: Single TTS Requests ---\n",
        "    # Log a section header for demonstrating single TTS requests.\n",
        "    main_logger.info(\"\\n--- Demonstrating Single TTS Requests ---\")\n",
        "    # Define a list of dictionaries, each representing parameters for a single TTS request.\n",
        "    single_requests_data = [\n",
        "        {\"text\": \"Hello from the Text-to-Speech Orchestrator! This is a standard voice.\", \"lang\": \"en-US\", \"quality\": VoiceQuality.STANDARD},\n",
        "        {\"text\": \"This is a premium quality Neural2 voice, for critical applications.\", \"lang\": \"en-US\", \"quality\": VoiceQuality.NEURAL2},\n",
        "        {\"text\": \"Bonjour le monde, ceci est une voix franaise.\", \"lang\": \"fr-FR\", \"quality\": VoiceQuality.STANDARD},\n",
        "        {\"text\": \"<speak>This is <emphasis level='strong'>SSML</emphasis> text with a <break time='500ms'/> pause.</speak>\", \"lang\": \"en-GB\", \"quality\": VoiceQuality.NEURAL2, \"ssml\": True},\n",
        "        {\"text\": \"A short request for caching.\", \"lang\": \"en-US\", \"quality\": VoiceQuality.STANDARD},\n",
        "        {\"text\": \"A short request for caching.\", \"lang\": \"en-US\", \"quality\": VoiceQuality.STANDARD}, # Expected cache hit\n",
        "        {\"text\": \"\", \"lang\": \"ja-JP\", \"quality\": VoiceQuality.NEURAL2},\n",
        "        {\"text\": \"Invalid request with very long text\" * 1000, \"lang\": \"en-US\", \"quality\": VoiceQuality.STANDARD}, # Expected to fail TTSRequest validation\n",
        "    ]\n",
        "\n",
        "    # Iterate through the 'single_requests_data' list with an index 'i' and request data 'req_data'.\n",
        "    for i, req_data in enumerate(single_requests_data):\n",
        "        # Construct an output filename for the synthesized audio.\n",
        "        output_filename = f\"output_single_{i}_{req_data['lang']}.mp3\"\n",
        "        # Begin a try block to handle potential errors during the processing of a single request.\n",
        "        try:\n",
        "            # Attempt to create a TTSRequest object using data from the current 'req_data' dictionary.\n",
        "            tts_req = TTSRequest(\n",
        "                text=req_data[\"text\"],\n",
        "                language_code=req_data[\"lang\"],\n",
        "                quality_tier=req_data[\"quality\"],\n",
        "                ssml_enabled=req_data.get(\"ssml\", False)\n",
        "            )\n",
        "            # Log an informational message about processing the current single request.\n",
        "            main_logger.info(f\"Processing single request {tts_req.request_id} ({output_filename})...\")\n",
        "            # Call the orchestrator's synthesize_speech method with the created TTSRequest object.\n",
        "            # This returns a tuple: (audio_data_bytes_or_None, metadata_dictionary).\n",
        "            audio_data, metadata = await orchestrator.synthesize_speech(tts_req)\n",
        "\n",
        "            # Check if 'audio_data' was successfully returned (i.e., not None).\n",
        "            if audio_data:\n",
        "                # Open the specified 'output_filename' in binary write mode ('wb').\n",
        "                with open(output_filename, \"wb\") as f:\n",
        "                    # Write the 'audio_data' bytes to the opened file.\n",
        "                    f.write(audio_data)\n",
        "                # Log a success message including the request ID, output filename, and metadata.\n",
        "                main_logger.info(f\"SUCCESS: Request {tts_req.request_id} -> Saved to {output_filename}. Metadata: {metadata}\")\n",
        "            # Handle cases where 'audio_data' is None, indicating a failure in synthesis.\n",
        "            else:\n",
        "                # Log an error message for the failed request, including request ID and metadata.\n",
        "                main_logger.error(f\"FAILURE: Request {tts_req.request_id} -> No audio data returned. Metadata: {metadata}\")\n",
        "\n",
        "        # Catch ValidationError if TTSRequest creation fails due to invalid input data.\n",
        "        except ValidationError as e:\n",
        "            # Log an error message indicating that the request data failed validation and was skipped.\n",
        "            main_logger.error(f\"SKIPPED: Request data for '{output_filename}' failed validation: {e}\")\n",
        "        # Catch any other unexpected Exception during the processing of a single request.\n",
        "        except Exception as e:\n",
        "            # Log a critical error message for unexpected errors during single request processing.\n",
        "            main_logger.critical(f\"CRITICAL ERROR during single request for '{output_filename}': {e}\", exc_info=True)\n",
        "\n",
        "    # --- Example 2: Batch TTS Requests ---\n",
        "    # Log a section header for demonstrating batch TTS requests.\n",
        "    main_logger.info(\"\\n--- Demonstrating Batch TTS Requests ---\")\n",
        "    # Define a list of TTSRequest objects for batch processing.\n",
        "    batch_requests_data: List[TTSRequest] = [\n",
        "        TTSRequest(text=\"Batch item one, standard quality.\", language_code=\"en-US\", quality_tier=VoiceQuality.STANDARD, priority=1),\n",
        "        TTSRequest(text=\"Batch item two, premium quality.\", language_code=\"en-GB\", quality_tier=VoiceQuality.NEURAL2, priority=5),\n",
        "        TTSRequest(text=\"Batch item three, another standard.\", language_code=\"es-ES\", quality_tier=VoiceQuality.STANDARD, priority=2),\n",
        "        TTSRequest(text=\"<speak>Batch SSML <prosody rate='slow'>slowly spoken</prosody>.</speak>\", language_code=\"en-US\", quality_tier=VoiceQuality.NEURAL2, ssml_enabled=True, priority=3),\n",
        "        TTSRequest(text=\"Batch item five, likely to be cached.\", language_code=\"en-US\", quality_tier=VoiceQuality.STANDARD, priority=1),\n",
        "    ]\n",
        "    # Begin a try block to handle potential errors during batch synthesis.\n",
        "    try:\n",
        "        # Call the orchestrator's batch_synthesize method with the list of TTSRequest objects.\n",
        "        # This returns a list of tuples, each containing (audio_data_or_None, metadata_dictionary).\n",
        "        batch_results = await orchestrator.batch_synthesize(batch_requests_data)\n",
        "        # Iterate through the 'batch_results' list with an index 'i' and result tuple '(audio_data, metadata)'.\n",
        "        for i, (audio_data, metadata) in enumerate(batch_results):\n",
        "            # Get the request ID from metadata, defaulting if not present.\n",
        "            req_id = metadata.get(\"request_id\", f\"batch_unknown_{i}\")\n",
        "            # Construct an output filename for the synthesized audio from the batch request.\n",
        "            output_filename = f\"output_batch_{i}_{req_id}.mp3\"\n",
        "            # Check if 'audio_data' was successfully returned for this batch item.\n",
        "            if audio_data:\n",
        "                # Open the specified 'output_filename' in binary write mode ('wb').\n",
        "                with open(output_filename, \"wb\") as f:\n",
        "                    # Write the 'audio_data' bytes to the opened file.\n",
        "                    f.write(audio_data)\n",
        "                # Log a success message for the batch request item.\n",
        "                main_logger.info(f\"SUCCESS: Batch request {req_id} -> Saved to {output_filename}. Metadata: {metadata}\")\n",
        "            # Handle cases where 'audio_data' is None for a batch item.\n",
        "            else:\n",
        "                # Log an error message for the failed batch request item.\n",
        "                main_logger.error(f\"FAILURE: Batch request {req_id} -> Synthesis failed. Metadata: {metadata}\")\n",
        "    # Catch any unexpected Exception during the batch synthesis process.\n",
        "    except Exception as e:\n",
        "        # Log a critical error message for unexpected errors during batch synthesis.\n",
        "        main_logger.critical(f\"CRITICAL ERROR during batch synthesis: {e}\", exc_info=True)\n",
        "\n",
        "    # --- Example 3: Get Cost Analysis ---\n",
        "    # Log a section header for demonstrating cost analysis retrieval.\n",
        "    main_logger.info(\"\\n--- Generating Cost Analysis Report ---\")\n",
        "    # Begin a try block to handle potential errors during cost analysis generation.\n",
        "    try:\n",
        "        # Call the orchestrator's get_cost_analysis method to retrieve the cost report.\n",
        "        cost_analysis = orchestrator.get_cost_analysis()\n",
        "        # Log the cost analysis report as a pretty-printed JSON string.\n",
        "        main_logger.info(\"Cost Analysis Report:\\n\" + json.dumps(cost_analysis, indent=2))\n",
        "        # Print a header for the cost analysis report to the console.\n",
        "        print(\"\\n=== Cost Analysis Report ===\")\n",
        "        # Print the cost analysis report as a pretty-printed JSON string to the console.\n",
        "        print(json.dumps(cost_analysis, indent=2))\n",
        "        # Print a footer for the cost analysis report to the console.\n",
        "        print(\"==========================\")\n",
        "    # Catch any unexpected Exception during cost analysis generation.\n",
        "    except Exception as e:\n",
        "        # Log an error message if cost analysis generation fails.\n",
        "        main_logger.error(f\"Error generating cost analysis: {e}\", exc_info=True)\n",
        "\n",
        "    # Log an informational message indicating the end of the usage example.\n",
        "    main_logger.info(\"VertexTTSOrchestrator usage example finished.\")\n",
        "\n",
        "# Check if the script is being run as the main module.\n",
        "if __name__ == \"__main__\":\n",
        "    # Begin a try block to handle top-level exceptions during script execution.\n",
        "    try:\n",
        "        # Run the 'main_usage_example' asynchronous function using asyncio.run().\n",
        "        asyncio.run(main_usage_example())\n",
        "    # Catch KeyboardInterrupt (e.g., Ctrl+C) to allow graceful exit.\n",
        "    except KeyboardInterrupt:\n",
        "        # Log an informational message if the script is interrupted by the user.\n",
        "        logger.info(\"Main example interrupted by user (KeyboardInterrupt). Exiting.\")\n",
        "    # Catch any other unexpected Exception at the top level of script execution.\n",
        "    except Exception as e:\n",
        "        # Log a critical error message for unhandled exceptions in the main execution block.\n",
        "        logger.critical(f\"Critical unhandled exception in main: {e}\", exc_info=True)\n"
      ],
      "metadata": {
        "id": "AKmTb9EtkbEI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}