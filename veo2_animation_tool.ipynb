{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKRwvnR8xOzw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n",
        "# VEO Video Pipeline: Sequential Animation Tool for Google Vertex AI\n",
        "\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code Style: PEP-8](https://img.shields.io/badge/code%20style-PEP--8-orange.svg)](https://www.python.org/dev/peps/pep-0008/)\n",
        "\n",
        "**Repository:** [https://github.com/chirindaopensource/veo-video-pipeline](https://github.com/chirindaopensource/veo-video-pipeline)  \n",
        "**Author:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "## Overview\n",
        "\n",
        "The VEO Video Pipeline is a Python-based tool designed to orchestrate the generation of sequential video animations using Google's state-of-the-art Video Generation Model (VEO) via the Vertex AI API. This project provides a robust, cost-efficient, and extensible framework for transforming a series of text prompts, optionally seeded by an initial reference image, into a cohesive video narrative. The entire pipeline is encapsulated within a single iPython Notebook (`veo2_animation_tool.ipynb`) for ease of use, experimentation, and modification.\n",
        "\n",
        "This tool is particularly useful for creators, researchers, and developers looking to:\n",
        "*   Generate short video clips from text prompts.\n",
        "*   Create longer animated sequences by chaining multiple VEO generations.\n",
        "*   Leverage an initial image to guide the style and content of the first video segment.\n",
        "*   Automate the process of using the last frame of a generated segment as the reference for the next, ensuring visual continuity.\n",
        "\n",
        "The implementation prioritizes robustness through comprehensive error handling, retry mechanisms for API calls, and efficient resource management, including memory-efficient image processing and video stitching.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "*   **Sequential Video Generation:** Generates multiple video segments based on an ordered list of prompts.\n",
        "*   **Image-to-Video & Video-to-Video:**\n",
        "    *   Initiates the sequence with a user-provided reference image.\n",
        "    *   Automatically uses the last frame of the previously generated segment as the reference for the next, enabling chained animations.\n",
        "*   **Google VEO Model Integration:** Utilizes the `video-generation-001` model via the synchronous Vertex AI API.\n",
        "*   **Robust API Interaction:** Implements `tenacity`-based exponential backoff and retry logic for transient network or API errors.\n",
        "*   **Cost-Conscious Design:**\n",
        "    *   Optimized image encoding (JPEG) to reduce payload sizes.\n",
        "    *   Recommends appropriate video dimensions.\n",
        "    *   Handles GCS downloads efficiently.\n",
        "*   **Efficient Video Stitching:** Uses `ffmpeg` (via `subprocess`) for fast, lossless concatenation of video segments.\n",
        "*   **Configuration Management:** Centralized configuration (`VEOConfig`) for model parameters, API endpoints, and retry settings.\n",
        "*   **Error Handling:** Custom exceptions (`VEOVideoGenerationError`) for pipeline-specific issues.\n",
        "*   **Standardized Logging:** Clear and informative logging for monitoring and debugging.\n",
        "*   **Self-Contained Notebook:** All logic, including helper functions and the main pipeline class, is within `veo2_animation_tool.ipynb`.\n",
        "\n",
        "## How It Works (High-Level)\n",
        "\n",
        "1.  **Initialization & Authentication:** Sets up Google Cloud credentials and project details.\n",
        "2.  **Input Validation:** Checks the validity of the reference image path, scene prompts, and output folder name.\n",
        "3.  **Initial Reference:** Loads the user-provided reference image, resizes it to VEO-compatible dimensions, and encodes it to Base64.\n",
        "4.  **Iterative Segment Generation:** For each prompt in the sequence:\n",
        "    *   Constructs a request payload including the prompt and the current reference image data (either the initial image or the last frame of the previous segment).\n",
        "    *   Calls the Vertex AI VEO API to generate a video segment.\n",
        "    *   Handles the API response, which may contain video bytes directly or a GCS URI for download.\n",
        "    *   Saves the generated segment locally.\n",
        "    *   If it's not the last prompt, extracts the last frame of the newly generated segment, resizes, and encodes it to Base64 to serve as the reference for the next iteration.\n",
        "5.  **Video Stitching:** Once all segments are generated, they are concatenated into a single MP4 file using `ffmpeg`.\n",
        "6.  **Output:** The final stitched video and individual segments are saved to a specified output directory in the user's home folder.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running the notebook, ensure you have the following:\n",
        "\n",
        "1.  **Python:** Version 3.8 or higher.\n",
        "2.  **Google Cloud Platform (GCP) Account:**\n",
        "    *   A GCP Project with billing enabled.\n",
        "    *   The **Vertex AI API** enabled for your project.\n",
        "3.  **`gcloud` CLI:** Google Cloud Command Line Interface installed and configured.\n",
        "    *   Authenticate by running: `gcloud auth application-default login`\n",
        "4.  **`ffmpeg`:** The `ffmpeg` command-line tool must be installed and accessible in your system's PATH.\n",
        "    *   Download from [ffmpeg.org](https://ffmpeg.org/download.html).\n",
        "5.  **Jupyter Notebook or JupyterLab:** To run the `.ipynb` file.\n",
        "\n",
        "## Installation & Setup\n",
        "\n",
        "1.  **Clone the Repository:**\n",
        "    ```bash\n",
        "    git clone https://github.com/chirindaopensource/veo-video-pipeline.git\n",
        "    cd veo-video-pipeline\n",
        "    ```\n",
        "\n",
        "2.  **Create a Virtual Environment (Recommended):**\n",
        "    ```bash\n",
        "    python -m venv veo_env\n",
        "    source veo_env/bin/activate  # On Windows: veo_env\\Scripts\\activate\n",
        "    ```\n",
        "\n",
        "3.  **Install Python Dependencies:**\n",
        "    The necessary Python packages are listed in the import section of the notebook. You can install them using pip:\n",
        "    ```bash\n",
        "    pip install google-cloud-aiplatform google-cloud-storage opencv-python Pillow requests tenacity numpy jupyterlab\n",
        "    ```\n",
        "    (Consider creating a `requirements.txt` file for easier dependency management in future project iterations).\n",
        "\n",
        "4.  **Verify `ffmpeg` Installation:**\n",
        "    Open your terminal and type:\n",
        "    ```bash\n",
        "    ffmpeg -version\n",
        "    ```\n",
        "    If it's installed correctly, you'll see version information. If not, please install it and ensure it's in your system PATH.\n",
        "\n",
        "5.  **Set Google Cloud Project ID:**\n",
        "    The script will attempt to auto-detect your GCP Project ID. However, it's best practice to set it explicitly via an environment variable:\n",
        "    ```bash\n",
        "    export GOOGLE_CLOUD_PROJECT=\"your-gcp-project-id\"\n",
        "    ```\n",
        "    Alternatively, you can pass it as an argument when calling `create_veo_video_pipeline` within the notebook.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "The primary configuration happens within the `veo2_animation_tool.ipynb` notebook, specifically in the example usage block (`if __name__ == \"__main__\":` or a similar cell designed for execution).\n",
        "\n",
        "Key parameters to configure:\n",
        "\n",
        "*   `reference_image_path` (str): Absolute or relative path to your initial reference image (e.g., `.jpg`, `.png`). A dummy image will be created if not found, but using your own is recommended.\n",
        "*   `scene_prompts` (List[str]): A list of text prompts, where each prompt describes a scene. The videos will be generated in the order of these prompts.\n",
        "*   `output_folder_name` (str): Name of the folder to be created in your user's home directory to store generated segments and the final video.\n",
        "*   `project_id` (Optional[str]): Your Google Cloud Project ID. If `None`, the pipeline attempts to auto-detect it or use the `GOOGLE_CLOUD_PROJECT` environment variable.\n",
        "\n",
        "Advanced configuration options (e.g., video length, FPS, API endpoints) are managed within the `VEOConfig` class and can be modified directly in the notebook if needed, though the defaults are generally sensible.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1.  **Launch JupyterLab or Jupyter Notebook:**\n",
        "    Navigate to the cloned repository directory in your terminal and run:\n",
        "    ```bash\n",
        "    jupyter lab\n",
        "    # or\n",
        "    # jupyter notebook\n",
        "    ```\n",
        "\n",
        "2.  **Open the Notebook:**\n",
        "    In the Jupyter interface, open `veo2_animation_tool.ipynb`.\n",
        "\n",
        "3.  **Configure Parameters:**\n",
        "    Locate the main execution cell (typically at the end of the notebook, often guarded by an `if __name__ == \"__main__\":` equivalent for notebooks or a clearly marked \"Run\" cell).\n",
        "    Modify the `example_params` dictionary with your desired `reference_image_path`, `scene_prompts`, and `output_folder_name`.\n",
        "\n",
        "    ```python\n",
        "    # Example configuration within the notebook:\n",
        "    example_params = {\n",
        "        \"reference_image_path\": \"path/to/your/image.jpg\",\n",
        "        \"scene_prompts\": [\n",
        "            \"A futuristic cityscape at dusk, neon lights reflecting on wet streets.\",\n",
        "            \"A sleek flying car zips through the city canyons.\",\n",
        "            \"The car lands on a skyscraper rooftop, overlooking the sprawling metropolis.\"\n",
        "        ],\n",
        "        \"output_folder_name\": \"my_veo_animation\",\n",
        "        # \"project_id\": \"your-gcp-project-id\"  # Optional: uncomment and set if needed\n",
        "    }\n",
        "    ```\n",
        "\n",
        "4.  **Run the Notebook:**\n",
        "    Execute the cells in the notebook sequentially, or use \"Run All Cells.\" The video generation process can take several minutes per segment, depending on the VEO model's load and video length. Monitor the logging output for progress.\n",
        "\n",
        "5.  **Locate Output:**\n",
        "    Upon successful completion, the individual video segments and the final stitched video (`final_stitched_video.mp4`) will be saved in `~/output_folder_name/` (e.g., `~/my_veo_animation/`).\n",
        "\n",
        "## Core Components (within `veo2_animation_tool.ipynb`)\n",
        "\n",
        "*   **`VEOConfig` (class):** A centralized configuration class holding constants like API model ID, default video parameters, retryable status codes, etc.\n",
        "*   **`VEOVideoGenerationError` (class):** Custom exception for pipeline-specific errors.\n",
        "*   **`VEOVideoPipeline` (class):** The main class orchestrating the video generation workflow.\n",
        "    *   `__init__(...)`: Initializes authentication and GCP settings.\n",
        "    *   `_initialize_authentication()`: Handles Google Cloud ADC.\n",
        "    *   `_validate_inputs(...)`: Validates user-provided parameters.\n",
        "    *   `_load_and_encode_image(...)`: Loads, resizes (preserving aspect ratio for VEO's recommended dimensions), and Base64 encodes images.\n",
        "    *   `_generate_video_segment(...)`: Makes the API call to Vertex AI VEO, with retries.\n",
        "    *   `_download_video_from_gcs(...)`: Downloads video if API returns a GCS URI.\n",
        "    *   `_extract_last_frame(...)`: Uses OpenCV to get the last frame of a video.\n",
        "    *   `_frame_to_base64(...)`: Converts an OpenCV frame to a Base64 encoded string for the next API call.\n",
        "    *   `_stitch_videos_ffmpeg(...)`: Concatenates video segments using `ffmpeg`.\n",
        "    *   `generate_sequential_videos(...)`: The main public method that executes the entire pipeline.\n",
        "*   **`create_veo_video_pipeline(...)` (function):** A factory function providing a simpler interface to instantiate and run the `VEOVideoPipeline`.\n",
        "*   **Helper Functions:** Such as `_is_retryable_http_error` for custom retry logic.\n",
        "\n",
        "## Cost Considerations\n",
        "\n",
        "Generating videos with Google's VEO model via Vertex AI incurs costs. Please refer to the [Vertex AI pricing page](https://cloud.google.com/vertex-ai/pricing) for the most up-to-date information on \"Generative AI models\" or \"Multimodal models.\"\n",
        "\n",
        "This pipeline uses the synchronous API, which might have different pricing characteristics than batch prediction. Costs are typically based on the duration of the video generated.\n",
        "\n",
        "This tool attempts to be cost-efficient by:\n",
        "*   Using recommended image dimensions to avoid unnecessary upscaling/downscaling by the model.\n",
        "*   Encoding images to JPEG with reasonable quality to reduce request payload sizes.\n",
        "*   Performing frame extraction and video stitching locally, avoiding additional cloud service costs for these tasks.\n",
        "\n",
        "**Monitor your GCP billing dashboard regularly when using this tool.**\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "*   **Authentication Errors:**\n",
        "    *   Ensure you've run `gcloud auth application-default login`.\n",
        "    *   Verify the `GOOGLE_CLOUD_PROJECT` environment variable is set correctly or passed to the pipeline.\n",
        "    *   Check if the Vertex AI API is enabled in your GCP project.\n",
        "*   **`ffmpeg: command not found`:**\n",
        "    *   `ffmpeg` is not installed or not in your system's PATH. Revisit the installation steps.\n",
        "*   **API Quota Errors (e.g., HTTP 429):**\n",
        "    *   You might be hitting API rate limits or project quotas. The retry logic will attempt to handle transient 429s, but persistent issues may require requesting a quota increase from Google Cloud.\n",
        "*   **`FileNotFoundError` for reference image:**\n",
        "    *   Double-check the `reference_image_path` provided.\n",
        "*   **Long Generation Times:**\n",
        "    *   Video generation is computationally intensive. Longer videos or higher FPS settings will take more time. The default is 4 seconds at 24 FPS.\n",
        "*   **Permission Denied (Output Directory):**\n",
        "    *   Ensure your user has write permissions to their home directory, where the output folder is created.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome! If you have improvements, bug fixes, or new features you'd like to add, please follow these steps:\n",
        "\n",
        "1.  **Fork the repository.**\n",
        "2.  **Create a new branch** for your feature or fix: `git checkout -b feature/your-feature-name` or `git checkout -b fix/your-bug-fix`.\n",
        "3.  **Make your changes** within the `veo2_animation_tool.ipynb` notebook. Ensure your code adheres to PEP-8 standards where applicable and is well-commented.\n",
        "4.  **Test your changes thoroughly.**\n",
        "5.  **Commit your changes:** `git commit -m \"Add concise description of your changes\"`.\n",
        "6.  **Push to your forked repository:** `git push origin feature/your-feature-name`.\n",
        "7.  **Open a Pull Request** to the `main` branch of `chirindaopensource/veo-video-pipeline`. Provide a clear description of your changes and why they are needed.\n",
        "\n",
        "Please also feel free to open an issue for bug reports or feature requests.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details."
      ],
      "metadata": {
        "id": "4A2twyw9znZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "tI1MrwOq34ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import base64\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "import io\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# Third-party Imports, Grouped for Clarity\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "from PIL import Image\n",
        "import requests\n",
        "from google.auth import default, exceptions as auth_exceptions\n",
        "from google.auth.transport.requests import Request\n",
        "from google.cloud import storage\n",
        "from google.api_core import exceptions as api_core_exceptions\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n"
      ],
      "metadata": {
        "id": "TRvBcTlq38Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "xh4-t2Kq4RFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging Configuration\n",
        "# Configure a logger for clear, standardized output for debugging and monitoring.\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "# Instantiate the logger using the standard __name__ convention.\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Centralized Configuration Class\n",
        "class VEOConfig:\n",
        "    \"\"\"\n",
        "    A centralized, immutable configuration class for VEO API constants.\n",
        "\n",
        "    This provides a single source of truth for static parameters, improving\n",
        "    maintainability and preventing the use of \"magic\" strings or numbers\n",
        "    in the pipeline logic.\n",
        "    \"\"\"\n",
        "    # The required OAuth scope for authenticating with the Vertex AI API.\n",
        "    AUTH_SCOPE: List[str] = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "    # The official, versioned model identifier for Google's VEO model.\n",
        "    MODEL_ID: str = \"video-generation-001\"\n",
        "    # The standard Vertex AI API method for synchronous (blocking) predictions.\n",
        "    API_METHOD: str = \"predict\"\n",
        "    # A generous timeout for the synchronous API call to allow for video generation.\n",
        "    API_TIMEOUT_SECONDS: int = 900\n",
        "    # The set of HTTP status codes that indicate transient server-side issues safe to retry.\n",
        "    RETRYABLE_STATUS_CODES: set[int] = {429, 500, 502, 503, 504}\n",
        "    # Recommended VEO dimensions for landscape videos (width, height).\n",
        "    LANDSCAPE_DIMS: Tuple[int, int] = (1024, 576)\n",
        "    # Recommended VEO dimensions for portrait videos (width, height).\n",
        "    PORTRAIT_DIMS: Tuple[int, int] = (576, 1024)\n",
        "    # Default video generation parameters.\n",
        "    DEFAULT_VIDEO_LENGTH: str = \"4s\"\n",
        "    DEFAULT_FPS: int = 24\n",
        "    # JPEG quality for encoding reference frames. 95 is a high-quality setting.\n",
        "    JPEG_ENCODING_QUALITY: int = 95\n",
        "\n",
        "\n",
        "# --- Custom Exception for Pipeline-Specific Errors ---\n",
        "class VEOVideoGenerationError(Exception):\n",
        "    \"\"\"Custom exception for specific, identifiable errors within the VEO video generation pipeline.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# --- Robust Retry Logic for API Calls ---\n",
        "def _is_retryable_http_error(exception: BaseException) -> bool:\n",
        "    \"\"\"\n",
        "    Custom tenacity retry condition to check for specific HTTP status codes.\n",
        "\n",
        "    This function determines if a caught exception is a requests.HTTPError with a\n",
        "    status code that is defined as retryable in VEOConfig.\n",
        "\n",
        "    Args:\n",
        "        exception (BaseException): The exception caught by tenacity.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the exception is an HTTPError with a retryable status code, False otherwise.\n",
        "    \"\"\"\n",
        "    # Return True only if the exception is an HTTPError and its status code is in our retryable set.\n",
        "    return (\n",
        "        isinstance(exception, requests.exceptions.HTTPError) and\n",
        "        hasattr(exception, 'response') and\n",
        "        exception.response.status_code in VEOConfig.RETRYABLE_STATUS_CODES\n",
        "    )\n",
        "\n",
        "# Define a reusable, robust retry strategy for API calls using the tenacity library.\n",
        "retry_on_api_error = retry(\n",
        "    # Use exponential backoff, starting at 4s and maxing out at 60s between retries.\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
        "    # Stop retrying after 10 attempts to prevent indefinite loops and fail gracefully.\n",
        "    stop=stop_after_attempt(10),\n",
        "    # Define the conditions for retrying: network errors or specific HTTP server errors.\n",
        "    retry=retry_if_exception_type((\n",
        "        requests.exceptions.ConnectionError,\n",
        "        requests.exceptions.Timeout,\n",
        "    )) | _is_retryable_http_error,\n",
        "    # Log a warning before each retry attempt for visibility into transient failures.\n",
        "    before_sleep=lambda retry_state: logger.warning(\n",
        "        f\"Retrying API call due to {retry_state.outcome.exception()}. \"\n",
        "        f\"Attempt #{retry_state.attempt_number}, waiting {retry_state.next_action.sleep:.2f}s...\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "lQ2mmj-U4VtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n"
      ],
      "metadata": {
        "id": "wb0sE-2u4tu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Class\n",
        "\n",
        "class VEOVideoPipeline:\n",
        "    \"\"\"\n",
        "    A professional, implementation-grade class for Google's VEO video generation.\n",
        "\n",
        "    This class orchestrates the complete workflow of generating sequential videos using\n",
        "    Google's VEO model via its synchronous API. It handles authentication, request\n",
        "    building, response parsing, frame extraction, and memory-efficient video stitching.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, project_id: Optional[str] = None, location: str = \"us-central1\"):\n",
        "        \"\"\"\n",
        "        Initializes the VEO video generation pipeline.\n",
        "\n",
        "        Args:\n",
        "            project_id (Optional[str]): Your Google Cloud project ID. If None, it will be\n",
        "                                        auto-detected from the environment.\n",
        "            location (str): The Google Cloud region for Vertex AI. Defaults to us-central1.\n",
        "\n",
        "        Raises:\n",
        "            VEOVideoGenerationError: If initialization or authentication fails.\n",
        "            ValueError: If input parameters are invalid.\n",
        "        \"\"\"\n",
        "        # Validate that the location parameter is a non-empty string.\n",
        "        if not isinstance(location, str) or not location:\n",
        "            raise ValueError(\"location must be a non-empty string\")\n",
        "\n",
        "        # Store the project ID and location as instance-specific attributes.\n",
        "        self.project_id: Optional[str] = project_id\n",
        "        self.location: str = location\n",
        "        # Construct the base API endpoint URL for Vertex AI.\n",
        "        self.api_endpoint: str = f\"https://{location}-aiplatform.googleapis.com\"\n",
        "\n",
        "        # Perform authentication immediately upon initialization to fail fast.\n",
        "        self._initialize_authentication()\n",
        "\n",
        "        # After attempting auto-detection, confirm that a project_id is set.\n",
        "        if not self.project_id:\n",
        "            raise VEOVideoGenerationError(\n",
        "                \"Google Cloud project_id could not be determined. \"\n",
        "                \"Please provide it explicitly or configure the environment.\"\n",
        "            )\n",
        "\n",
        "        # Log successful initialization for monitoring purposes.\n",
        "        logger.info(f\"VEOVideoPipeline initialized for project: {self.project_id}\")\n",
        "\n",
        "    def _initialize_authentication(self) -> None:\n",
        "        \"\"\"\n",
        "        Initializes Google Cloud authentication using Application Default Credentials (ADC).\n",
        "\n",
        "        This method explicitly requests the necessary scope for Vertex AI, ensuring\n",
        "        the credentials obtained are valid for making API calls.\n",
        "\n",
        "        Raises:\n",
        "            VEOVideoGenerationError: If authentication fails for any reason.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Use google.auth.default to find credentials, requesting the correct scope from VEOConfig.\n",
        "            self.credentials, detected_project_id = default(scopes=VEOConfig.AUTH_SCOPE)\n",
        "\n",
        "            # If the project_id was not provided during initialization, use the one detected by ADC.\n",
        "            if self.project_id is None:\n",
        "                self.project_id = detected_project_id\n",
        "\n",
        "        except auth_exceptions.DefaultCredentialsError as e:\n",
        "            # Provide a user-friendly error if credentials are not found in the environment.\n",
        "            raise VEOVideoGenerationError(\n",
        "                \"Failed to find default credentials. Please run \"\n",
        "                \"'gcloud auth application-default login' or configure the environment.\"\n",
        "            ) from e\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected authentication errors and wrap them in our custom exception.\n",
        "            raise VEOVideoGenerationError(f\"An unexpected error occurred during authentication: {e}\") from e\n",
        "\n",
        "    def _validate_inputs(\n",
        "        self,\n",
        "        reference_image_path: str,\n",
        "        scene_prompts: List[str],\n",
        "        output_folder_name: str\n",
        "    ) -> None:\n",
        "        \"\"\"Validates all user-provided inputs for the video generation pipeline.\"\"\"\n",
        "        # Validate the path to the reference image is a non-empty string.\n",
        "        if not isinstance(reference_image_path, str) or not reference_image_path:\n",
        "            raise ValueError(\"reference_image_path must be a non-empty string.\")\n",
        "        # Validate the reference image file actually exists.\n",
        "        if not os.path.exists(reference_image_path):\n",
        "            raise FileNotFoundError(f\"Reference image file not found: {reference_image_path}\")\n",
        "\n",
        "        # Validate that scene_prompts is a non-empty list.\n",
        "        if not isinstance(scene_prompts, list) or not scene_prompts:\n",
        "            raise ValueError(\"scene_prompts must be a non-empty list of strings.\")\n",
        "        # Validate that every item in the list is a non-empty, non-whitespace string.\n",
        "        if not all(isinstance(prompt, str) and prompt.strip() for prompt in scene_prompts):\n",
        "            raise ValueError(\"All items in scene_prompts must be non-empty strings.\")\n",
        "\n",
        "        # Validate the name for the output folder is a non-empty string.\n",
        "        if not isinstance(output_folder_name, str) or not output_folder_name:\n",
        "            raise ValueError(\"output_folder_name must be a non-empty string.\")\n",
        "\n",
        "    def _create_output_directory(self, folder_name: str) -> Path:\n",
        "        \"\"\"Creates a subdirectory in the user's home directory for storing outputs.\"\"\"\n",
        "        try:\n",
        "            # Use Path.home() for robust, cross-platform path resolution to the user's home directory.\n",
        "            home_dir = Path.home()\n",
        "            # Define the full path for the output directory.\n",
        "            output_dir = home_dir / folder_name\n",
        "            # Create the directory, including parent directories if needed, and set standard permissions.\n",
        "            output_dir.mkdir(mode=0o755, parents=True, exist_ok=True)\n",
        "            # Log the successful creation or confirmation of the directory.\n",
        "            logger.info(f\"Output directory ensured at: {output_dir}\")\n",
        "            return output_dir\n",
        "        except OSError as e:\n",
        "            # Raise a custom error if directory creation fails due to OS-level issues.\n",
        "            raise VEOVideoGenerationError(f\"Failed to create output directory '{folder_name}': {e}\") from e\n",
        "\n",
        "    def _load_and_encode_image(self, image_path: str) -> str:\n",
        "        \"\"\"Loads an image, resizes it for VEO, and encodes it to a base64 string.\"\"\"\n",
        "        try:\n",
        "            # Open the image file using a context manager to ensure it's properly closed.\n",
        "            with Image.open(image_path) as img:\n",
        "                # Convert the image to RGB to remove any alpha channel (e.g., from PNGs), ensuring compatibility.\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "                # Resize the image to VEO's recommended dimensions while preserving aspect ratio.\n",
        "                img = self._resize_image_for_veo(img)\n",
        "\n",
        "                # Use an in-memory buffer to avoid writing a temporary file to disk, which is more efficient.\n",
        "                buffer = io.BytesIO()\n",
        "\n",
        "                # Save the processed image to the buffer in JPEG format for efficiency and smaller payload size.\n",
        "                img.save(buffer, format='JPEG', quality=VEOConfig.JPEG_ENCODING_QUALITY)\n",
        "\n",
        "                # Get the raw byte value from the buffer.\n",
        "                image_bytes = buffer.getvalue()\n",
        "\n",
        "                # Encode the image bytes to a base64 string and decode to utf-8 for JSON serialization.\n",
        "                base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
        "\n",
        "                # Log the successful operation.\n",
        "                logger.info(f\"Successfully loaded and encoded image: {image_path}\")\n",
        "                return base64_encoded\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            # Raise a specific error if the file does not exist.\n",
        "            raise VEOVideoGenerationError(f\"Image file not found at path: {image_path}\")\n",
        "        except Exception as e:\n",
        "            # Wrap any other image processing errors in our custom exception.\n",
        "            raise VEOVideoGenerationError(f\"Failed to load and encode image '{image_path}': {e}\") from e\n",
        "\n",
        "    def _resize_image_for_veo(self, img: Image.Image) -> Image.Image:\n",
        "        \"\"\"Resizes a PIL Image to VEO recommended dimensions, preserving aspect ratio.\"\"\"\n",
        "        # Get the current width and height of the image.\n",
        "        width, height = img.size\n",
        "        # Calculate the aspect ratio to determine if the image is landscape, portrait, or square.\n",
        "        aspect_ratio = width / height\n",
        "\n",
        "        # Set target dimensions from VEOConfig based on whether the image is landscape (or square) or portrait.\n",
        "        target_dims = VEOConfig.LANDSCAPE_DIMS if aspect_ratio >= 1 else VEOConfig.PORTRAIT_DIMS\n",
        "\n",
        "        # Resize the image using Lanczos resampling, which provides high-quality downscaling results.\n",
        "        resized_img = img.resize(target_dims, Image.Resampling.LANCZOS)\n",
        "\n",
        "        return resized_img\n",
        "\n",
        "    def _build_request_body(self, prompt: str, image_data: Optional[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Constructs the request body according to the official VEO API specification.\"\"\"\n",
        "        # Define the instance payload, which contains the core prompt.\n",
        "        instance_payload = {\"prompt\": prompt}\n",
        "\n",
        "        # If reference image data is provided (for the first or subsequent segments), add it to the payload.\n",
        "        if image_data:\n",
        "            instance_payload[\"reference_image\"] = {\"image_bytes\": image_data}\n",
        "\n",
        "        # Construct the full request body with instances and parameters from VEOConfig.\n",
        "        request_body = {\n",
        "            \"instances\": [instance_payload],\n",
        "            \"parameters\": {\n",
        "                \"video_length\": VEOConfig.DEFAULT_VIDEO_LENGTH,\n",
        "                \"fps\": VEOConfig.DEFAULT_FPS,\n",
        "                \"seed\": np.random.randint(0, 2**32 - 1) # Use a random seed for creative variety.\n",
        "            }\n",
        "        }\n",
        "        return request_body\n",
        "\n",
        "    @retry_on_api_error\n",
        "    def _generate_video_segment(self, prompt: str, image_data: Optional[str]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generates a single video segment using the synchronous VEO API with robust retries.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The text prompt for the video segment.\n",
        "            image_data (Optional[str]): Base64 encoded image data for initialization.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: The prediction dictionary from the API response.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Refresh credentials before the API call to ensure the auth token is not expired.\n",
        "            self.credentials.refresh(Request())\n",
        "\n",
        "            # Construct the full API endpoint URL using constants from VEOConfig for accuracy.\n",
        "            url = (f\"{self.api_endpoint}/v1/projects/{self.project_id}/locations/{self.location}\"\n",
        "                   f\"/publishers/google/models/{VEOConfig.MODEL_ID}:{VEOConfig.API_METHOD}\")\n",
        "\n",
        "            # Prepare the authorization and content-type headers for the request.\n",
        "            headers = {\n",
        "                'Authorization': f'Bearer {self.credentials.token}',\n",
        "                'Content-Type': 'application/json; charset=utf-8'\n",
        "            }\n",
        "\n",
        "            # Construct the request body using the dedicated helper method.\n",
        "            request_body = self._build_request_body(prompt, image_data)\n",
        "\n",
        "            # Log the request initiation. This is a long-running operation.\n",
        "            logger.info(\"Sending request to VEO API. This may take several minutes...\")\n",
        "            # Send the POST request to the VEO API with a configured timeout.\n",
        "            response = requests.post(url, headers=headers, json=request_body, timeout=VEOConfig.API_TIMEOUT_SECONDS)\n",
        "            # Raise an exception for HTTP error codes (4xx or 5xx), which tenacity will catch for retries.\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse the JSON response from the API.\n",
        "            response_data = response.json()\n",
        "\n",
        "            # Extract the list of predictions from the response body.\n",
        "            predictions = response_data.get(\"predictions\")\n",
        "            # Validate that the predictions list exists and is not empty.\n",
        "            if not predictions or not isinstance(predictions, list):\n",
        "                raise VEOVideoGenerationError(f\"API response is missing 'predictions'. Response: {response_data}\")\n",
        "\n",
        "            # Log the successful receipt of the response.\n",
        "            logger.info(\"Successfully received response from VEO API.\")\n",
        "            # Return the first prediction object, as we send one instance at a time.\n",
        "            return predictions[0]\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            # Log the detailed error from the API response body if available for easier debugging.\n",
        "            logger.error(f\"HTTP Error during video generation request: {e.response.text}\")\n",
        "            raise  # Re-raise the exception to be handled by the tenacity retry decorator or the caller.\n",
        "        except Exception as e:\n",
        "            # Wrap any other exceptions in our custom error type for consistent error handling.\n",
        "            raise VEOVideoGenerationError(f\"Video generation failed: {e}\") from e\n",
        "\n",
        "    def _download_video_from_gcs(self, gcs_uri: str, local_path: Path) -> None:\n",
        "        \"\"\"Downloads a video from a Google Cloud Storage URI to a local path.\"\"\"\n",
        "        try:\n",
        "            # Validate that the GCS URI has the correct 'gs://' prefix.\n",
        "            if not gcs_uri.startswith('gs://'):\n",
        "                raise ValueError(f\"Invalid GCS URI format: {gcs_uri}\")\n",
        "\n",
        "            # Parse the bucket name and blob (object) name from the URI string.\n",
        "            bucket_name, blob_name = gcs_uri[5:].split('/', 1)\n",
        "\n",
        "            # Initialize the Google Cloud Storage client with the correct project and credentials.\n",
        "            storage_client = storage.Client(project=self.project_id, credentials=self.credentials)\n",
        "\n",
        "            # Get a reference to the bucket and blob objects.\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "\n",
        "            # Download the blob's contents to the specified local file path.\n",
        "            blob.download_to_filename(str(local_path))\n",
        "\n",
        "            # Log the successful download.\n",
        "            logger.info(f\"Successfully downloaded video from {gcs_uri} to {local_path}\")\n",
        "\n",
        "        except (api_core_exceptions.NotFound, FileNotFoundError):\n",
        "            # Handle cases where the GCS object does not exist.\n",
        "            raise VEOVideoGenerationError(f\"GCS object not found at URI: {gcs_uri}\")\n",
        "        except Exception as e:\n",
        "            # Wrap any other GCS-related errors.\n",
        "            raise VEOVideoGenerationError(f\"Failed to download video from GCS: {e}\") from e\n",
        "\n",
        "    def _extract_last_frame(self, video_path: Path) -> NDArray[np.uint8]:\n",
        "        \"\"\"Extracts the last frame from a video file using OpenCV.\"\"\"\n",
        "        # Initialize a video capture object with the path to the video file.\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        # Check if the video file was opened successfully.\n",
        "        if not cap.isOpened():\n",
        "            raise VEOVideoGenerationError(f\"Cannot open video file for frame extraction: {video_path}\")\n",
        "\n",
        "        try:\n",
        "            # Get the total number of frames in the video.\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            # Ensure the video is not empty.\n",
        "            if total_frames == 0:\n",
        "                raise VEOVideoGenerationError(f\"Video appears to be empty (0 frames): {video_path}\")\n",
        "\n",
        "            # Set the capture position to the very last frame (index is total_frames - 1).\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)\n",
        "\n",
        "            # Read the frame at the new position.\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # Check if the frame was read successfully. 'ret' will be False if it fails.\n",
        "            if not ret or frame is None:\n",
        "                raise VEOVideoGenerationError(f\"Failed to read the last frame from: {video_path}\")\n",
        "\n",
        "            # Log the successful extraction.\n",
        "            logger.info(f\"Successfully extracted last frame from video: {video_path}\")\n",
        "            # Return the frame as a NumPy array.\n",
        "            return frame\n",
        "\n",
        "        except Exception as e:\n",
        "            # Wrap any OpenCV or other errors in our custom exception.\n",
        "            raise VEOVideoGenerationError(f\"Frame extraction failed for '{video_path}': {e}\") from e\n",
        "        finally:\n",
        "            # Crucially, release the video capture object to free up system resources.\n",
        "            cap.release()\n",
        "\n",
        "    def _frame_to_base64(self, frame: NDArray[np.uint8]) -> str:\n",
        "        \"\"\"Converts an OpenCV frame (NumPy array) to a base64 encoded string.\"\"\"\n",
        "        try:\n",
        "            # Convert the frame from OpenCV's default BGR color space to PIL's expected RGB color space.\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Create a PIL Image object from the NumPy array for easier processing.\n",
        "            pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "            # Resize the frame to ensure it's compatible with VEO's input requirements, using the same logic.\n",
        "            pil_image = self._resize_image_for_veo(pil_image)\n",
        "\n",
        "            # Use an in-memory buffer to save the image without writing to disk.\n",
        "            buffer = io.BytesIO()\n",
        "            # Save the image to the buffer as a high-quality JPEG.\n",
        "            pil_image.save(buffer, format='JPEG', quality=VEOConfig.JPEG_ENCODING_QUALITY)\n",
        "\n",
        "            # Get the raw byte value and encode it to a utf-8 decoded base64 string.\n",
        "            image_bytes = buffer.getvalue()\n",
        "            base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
        "\n",
        "            return base64_encoded\n",
        "\n",
        "        except Exception as e:\n",
        "            # Wrap any conversion errors.\n",
        "            raise VEOVideoGenerationError(f\"Frame to base64 conversion failed: {e}\") from e\n",
        "\n",
        "    def _stitch_videos_ffmpeg(self, video_paths: List[Path], output_path: Path) -> None:\n",
        "        \"\"\"Stitches multiple video files into one using ffmpeg's concat demuxer for efficiency.\"\"\"\n",
        "        # Check if the ffmpeg executable is available in the system's PATH before proceeding.\n",
        "        if not shutil.which(\"ffmpeg\"):\n",
        "            raise FileNotFoundError(\n",
        "                \"ffmpeg not found. Please install ffmpeg and ensure it is in your system's PATH.\"\n",
        "            )\n",
        "\n",
        "        # Ensure there is at least one video to process.\n",
        "        if not video_paths:\n",
        "            raise VEOVideoGenerationError(\"No video clips were provided to stitch.\")\n",
        "\n",
        "        # Create a temporary manifest file for ffmpeg in the same directory as the output.\n",
        "        manifest_path = output_path.with_suffix('.txt')\n",
        "\n",
        "        try:\n",
        "            # Write the list of video files to the manifest in the format required by ffmpeg's concat demuxer.\n",
        "            with open(manifest_path, 'w') as f:\n",
        "                for video_path in video_paths:\n",
        "                    # Verify each video file exists before adding it to the manifest.\n",
        "                    if not video_path.exists():\n",
        "                        raise FileNotFoundError(f\"Video file for stitching not found: {video_path}\")\n",
        "                    # Use resolved absolute paths and quotes to handle spaces or special characters safely.\n",
        "                    f.write(f\"file '{video_path.resolve()}'\\n\")\n",
        "\n",
        "            # Log the creation of the manifest file.\n",
        "            logger.info(f\"Created ffmpeg manifest at: {manifest_path}\")\n",
        "\n",
        "            # Construct the ffmpeg command for efficient, lossless concatenation.\n",
        "            command = [\n",
        "                \"ffmpeg\",\n",
        "                \"-y\",                   # Overwrite output file if it exists.\n",
        "                \"-f\", \"concat\",         # Use the concat demuxer.\n",
        "                \"-safe\", \"0\",           # Allow absolute paths in the manifest file (required for resolved paths).\n",
        "                \"-i\", str(manifest_path), # Specify the input manifest file.\n",
        "                \"-c\", \"copy\",           # Copy codecs without re-encoding for maximum speed and quality preservation.\n",
        "                str(output_path)        # Specify the final output file path.\n",
        "            ]\n",
        "\n",
        "            # Log the command being executed for debugging purposes.\n",
        "            logger.info(f\"Executing ffmpeg command: {' '.join(command)}\")\n",
        "\n",
        "            # Execute the command, capturing stdout and stderr for detailed error reporting.\n",
        "            result = subprocess.run(\n",
        "                command,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=False # We check the return code manually for better error logging.\n",
        "            )\n",
        "\n",
        "            # Check if the ffmpeg command executed successfully.\n",
        "            if result.returncode != 0:\n",
        "                # If it failed, raise an error with the detailed stderr from ffmpeg.\n",
        "                raise VEOVideoGenerationError(\n",
        "                    f\"ffmpeg failed with exit code {result.returncode}.\\n\"\n",
        "                    f\"Stderr: {result.stderr}\"\n",
        "                )\n",
        "\n",
        "            # Log the successful completion of the stitching process.\n",
        "            logger.info(f\"Successfully stitched {len(video_paths)} videos into: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Wrap any exception in our custom error type.\n",
        "            raise VEOVideoGenerationError(f\"Video stitching failed: {e}\") from e\n",
        "        finally:\n",
        "            # Ensure the temporary manifest file is always cleaned up, even if errors occur.\n",
        "            if manifest_path.exists():\n",
        "                manifest_path.unlink()\n",
        "                logger.info(f\"Cleaned up manifest file: {manifest_path}\")\n",
        "\n",
        "    def generate_sequential_videos(self,\n",
        "        reference_image_path: str,\n",
        "        scene_prompts: List[str],\n",
        "        output_folder_name: str\n",
        "    ) -> Path:\n",
        "        \"\"\"Executes the full pipeline to generate a sequence of chained videos.\"\"\"\n",
        "        try:\n",
        "            # Step 0: Validate all inputs before starting any expensive processing.\n",
        "            self._validate_inputs(reference_image_path, scene_prompts, output_folder_name)\n",
        "\n",
        "            # Step 1: Create the output directory to store all generated artifacts.\n",
        "            output_dir = self._create_output_directory(output_folder_name)\n",
        "\n",
        "            # Step 2: Load and encode the initial reference image to start the sequence.\n",
        "            current_image_data: Optional[str] = self._load_and_encode_image(reference_image_path)\n",
        "\n",
        "            # Initialize a list to store the paths of the generated video segments for later stitching.\n",
        "            video_paths: List[Path] = []\n",
        "\n",
        "            # Step 3: Iterate through each scene prompt to generate a corresponding video segment.\n",
        "            for i, prompt in enumerate(scene_prompts):\n",
        "                logger.info(f\"--- Processing Scene {i+1}/{len(scene_prompts)} ---\")\n",
        "                logger.info(f\"Prompt: '{prompt[:100]}...'\")\n",
        "\n",
        "                # Step 4: Generate a video segment using the current image data and prompt.\n",
        "                prediction = self._generate_video_segment(\n",
        "                    prompt=prompt,\n",
        "                    image_data=current_image_data\n",
        "                )\n",
        "\n",
        "                # Define a unique, ordered filename for this video segment.\n",
        "                video_filename = f\"segment_{i+1:03d}.mp4\"\n",
        "                video_path = output_dir / video_filename\n",
        "\n",
        "                # Step 5: Process the API response, which could contain bytes or a GCS URI.\n",
        "                if 'video_bytes' in prediction:\n",
        "                    # If video bytes are returned directly, decode them from base64 and save to a file.\n",
        "                    logger.info(f\"Received video as base64 bytes. Saving to {video_path}\")\n",
        "                    video_bytes = base64.b64decode(prediction['video_bytes'])\n",
        "                    with open(video_path, 'wb') as f:\n",
        "                        f.write(video_bytes)\n",
        "                elif 'gcs_uri' in prediction:\n",
        "                    # If a GCS URI is returned, download the video from the bucket.\n",
        "                    logger.info(f\"Received GCS URI. Downloading from {prediction['gcs_uri']}\")\n",
        "                    self._download_video_from_gcs(prediction['gcs_uri'], video_path)\n",
        "                else:\n",
        "                    # If neither key is present, the API response is invalid.\n",
        "                    raise VEOVideoGenerationError(f\"API prediction did not contain 'video_bytes' or 'gcs_uri': {prediction}\")\n",
        "\n",
        "                # Add the path of the newly created segment to our list for the final stitching step.\n",
        "                video_paths.append(video_path)\n",
        "\n",
        "                # Step 6: For all but the last prompt, extract the last frame to seed the next segment.\n",
        "                if i < len(scene_prompts) - 1:\n",
        "                    logger.info(\"Extracting last frame to use as reference for the next segment...\")\n",
        "                    last_frame = self._extract_last_frame(video_path)\n",
        "                    current_image_data = self._frame_to_base64(last_frame)\n",
        "\n",
        "            # Step 7: Stitch all generated video segments into a single final video file.\n",
        "            final_video_path = output_dir / \"final_stitched_video.mp4\"\n",
        "            self._stitch_videos_ffmpeg(video_paths, final_video_path)\n",
        "\n",
        "            # Log the successful completion of the entire pipeline.\n",
        "            logger.info(\"--- Sequential video generation pipeline completed successfully. ---\")\n",
        "            logger.info(f\"Final video saved to: {final_video_path}\")\n",
        "\n",
        "            # Return the path to the final product.\n",
        "            return final_video_path\n",
        "\n",
        "        except (VEOVideoGenerationError, ValueError, FileNotFoundError) as e:\n",
        "            # Catch known, specific errors and log them clearly before re-raising.\n",
        "            logger.error(f\"A predictable error occurred in the pipeline: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected errors for robust failure handling.\n",
        "            logger.error(f\"An unexpected error occurred in the main pipeline: {e}\", exc_info=True)\n",
        "            raise VEOVideoGenerationError(f\"An unexpected error occurred in the main pipeline: {e}\") from e\n",
        "\n",
        "\n",
        "def create_veo_video_pipeline(\n",
        "    reference_image_path: str,\n",
        "    scene_prompts: List[str],\n",
        "    output_folder_name: str,\n",
        "    project_id: Optional[str] = None\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    A high-level factory function to create and run the VEO sequential video pipeline.\n",
        "\n",
        "    This function abstracts away the class instantiation and execution, providing a\n",
        "    simple, clean interface to generate a complete video from an image and prompts.\n",
        "\n",
        "    Args:\n",
        "        reference_image_path (str): Path to the initial reference image file.\n",
        "        scene_prompts (List[str]): An ordered list of scene description prompts.\n",
        "        output_folder_name (str): The name for the output folder in the home directory.\n",
        "        project_id (Optional[str]): Your Google Cloud project ID. If None, it will be\n",
        "                                    auto-detected from ADC or environment variables.\n",
        "\n",
        "    Returns:\n",
        "        Path: The path to the final, stitched video file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # If project_id is not provided, attempt to get it from the environment as a fallback.\n",
        "        if project_id is None:\n",
        "            project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')\n",
        "            if project_id:\n",
        "                logger.info(f\"Using project ID from GOOGLE_CLOUD_PROJECT env var: {project_id}\")\n",
        "\n",
        "        # Initialize the VEO pipeline class with the determined project ID.\n",
        "        pipeline = VEOVideoPipeline(project_id=project_id)\n",
        "\n",
        "        # Execute the complete video generation pipeline with the provided parameters.\n",
        "        final_video_path = pipeline.generate_sequential_videos(\n",
        "            reference_image_path=reference_image_path,\n",
        "            scene_prompts=scene_prompts,\n",
        "            output_folder_name=output_folder_name\n",
        "        )\n",
        "\n",
        "        return final_video_path\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch and log any exceptions from the pipeline for clear top-level error reporting.\n",
        "        logger.error(f\"VEO video pipeline execution failed: {e}\")\n",
        "        # Re-raise the exception to allow the calling script to handle it as needed.\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "J_8LAWbw4vt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage Example"
      ],
      "metadata": {
        "id": "J3TBeSdC5BO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage and Testing Block\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Example usage of the corrected and robust VEO video generation pipeline.\n",
        "\n",
        "    *** IMPORTANT: CONFIGURATION REQUIRED ***\n",
        "    1.  **AUTHENTICATION**: Run `gcloud auth application-default login` in your terminal.\n",
        "    2.  **PROJECT ID**: Set the `GOOGLE_CLOUD_PROJECT` environment variable to your GCP project ID,\n",
        "        or pass the `project_id` argument to `create_veo_video_pipeline`.\n",
        "    3.  **APIs**: Ensure the Vertex AI API is enabled in your Google Cloud project.\n",
        "    4.  **DEPENDENCIES**: Install required packages:\n",
        "        pip install google-cloud-aiplatform google-cloud-storage opencv-python Pillow requests tenacity numpy\n",
        "    5.  **FFMPEG**: Install ffmpeg (https://ffmpeg.org/download.html) and ensure it is in your system's PATH.\n",
        "    6.  **REFERENCE IMAGE**: Replace the dummy image path with a path to your own image.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- CONFIGURE THESE PARAMETERS FOR YOUR RUN ---\n",
        "    try:\n",
        "        # Get the directory of the current script to create a sample image nearby.\n",
        "        script_dir = Path(__file__).parent\n",
        "    except NameError:\n",
        "        # Fallback for interactive environments (like Jupyter) where __file__ is not defined.\n",
        "        script_dir = Path.cwd()\n",
        "\n",
        "    # Create a dummy reference image for testing if one doesn't exist.\n",
        "    dummy_image_path = script_dir / \"veo_reference_image.jpg\"\n",
        "    if not dummy_image_path.exists():\n",
        "        print(f\"Creating a dummy reference image at: {dummy_image_path}\")\n",
        "        # A blue 16:9 image matching VEO's landscape dimensions.\n",
        "        dummy_img = Image.new('RGB', VEOConfig.LANDSCAPE_DIMS, color='darkblue')\n",
        "        dummy_img.save(dummy_image_path)\n",
        "\n",
        "    # Define the parameters for the video generation task.\n",
        "    example_params = {\n",
        "        \"reference_image_path\": str(dummy_image_path),\n",
        "        \"scene_prompts\": [\n",
        "            \"A majestic eagle perched on a rocky cliff overlooking a vast canyon, cinematic lighting.\",\n",
        "            \"The eagle spreads its wings and takes flight into the golden sunset, slow motion.\",\n",
        "            \"Soaring high above snow-capped mountains with clouds below, drone shot.\",\n",
        "            \"The eagle gracefully lands near a crystal clear alpine lake, its reflection in the water.\"\n",
        "        ],\n",
        "        \"output_folder_name\": \"veo_eagle_flight_sequence\",\n",
        "        # \"project_id\": \"your-gcp-project-id\"  # Optional: uncomment and set if needed.\n",
        "    }\n",
        "\n",
        "    print(\"--- Starting VEO Video Generation Pipeline ---\")\n",
        "    print(f\"Reference Image: {example_params['reference_image_path']}\")\n",
        "    print(f\"Output Folder Name: {example_params['output_folder_name']}\")\n",
        "    print(f\"Number of Scenes: {len(example_params['scene_prompts'])}\")\n",
        "\n",
        "    try:\n",
        "        # Execute the pipeline and time its execution.\n",
        "        start_time = time.time()\n",
        "        final_video = create_veo_video_pipeline(**example_params)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Print a clear success message with the final output path and total time.\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\" Video generation pipeline completed successfully!\")\n",
        "        print(f\" Final video saved to: {final_video}\")\n",
        "        print(f\" Total execution time: {end_time - start_time:.2f} seconds.\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    except (VEOVideoGenerationError, ValueError, FileNotFoundError) as e:\n",
        "        # Handle known, controlled pipeline failures gracefully with a clear error message.\n",
        "        print(f\"\\n PIPELINE FAILED: {e}\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        # Handle any other unexpected errors to prevent unhandled stack traces.\n",
        "        print(f\"\\n AN UNEXPECTED ERROR OCCURRED: {e}\")\n",
        "        sys.exit(1)\n"
      ],
      "metadata": {
        "id": "8uQBJgZJ4_L6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}